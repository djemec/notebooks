{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c62da09-2239-49f9-9365-17a73670cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d645d69a-a21b-48b4-8b07-53111e8902bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/ubuntu/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a047a12-b5a3-44d1-8de6-aaf32c83d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the tokenizer\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "eot = enc._special_tokens['<|endoftext|>'] # end of text token\n",
    "\n",
    "def tokenize(doc):\n",
    "    # tokenizes a single document and returns a numpy array of uint32 tokens\n",
    "    tokens = [eot] # the special <|endoftext|> token delimits all documents\n",
    "    tokens.extend(enc.encode_ordinary(doc['text']))\n",
    "    tokens_np = np.array(tokens)\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**32).all(), \"token dictionary too large for uint32\"\n",
    "    tokens_np_uint32 = tokens_np.astype(np.uint32)\n",
    "    return tokens_np_uint32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7cd3061-377f-4b4e-9eb3-e204b2f34e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_datafile(dataset, split, tokens_np, shard_index):\n",
    "    filename = path / f'{split}' / f'{split}_{dataset}_{shard_index:06d}'\n",
    "    np.save(filename, tokens_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8652badd-abc2-4b40-9832-e77d894eb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(data_loader, dataset_name, split, shard_size=int(1e6),max_shards=-1):\n",
    "    shard_index = 0\n",
    "    # preallocate buffer to hold current shard\n",
    "    all_tokens_np = np.empty((shard_size,), dtype=np.uint32)\n",
    "    token_count = 0\n",
    "    progress_bar = None\n",
    "    \n",
    "    for example in data_loader:\n",
    "        tokens = tokenize(example)\n",
    "        if token_count + len(tokens) < shard_size:\n",
    "            # simply append tokens to current shard\n",
    "            all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
    "            token_count += len(tokens)\n",
    "            # update progress bar\n",
    "            if progress_bar is None:\n",
    "                progress_bar = tqdm(total=shard_size, unit='tokens', desc=f'{dataset_name}_{split} | Shard {shard_index}')\n",
    "            progress_bar.update(len(tokens))\n",
    "        else:\n",
    "            # write the current shard and start a new one\n",
    "            # split the document into whatever fits in this shard; the remainder goes to next one\n",
    "            remainder = shard_size - token_count\n",
    "            progress_bar.update(remainder)\n",
    "            all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n",
    "            # write the file\n",
    "            write_datafile(dataset_name, split, all_tokens_np, shard_index)\n",
    "            # reset \n",
    "            shard_index += 1\n",
    "            # populate the next shard with the leftovers of the current doc\n",
    "            all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
    "            token_count = len(tokens)-remainder\n",
    "            #reset progress bar \n",
    "            progress_bar.close()\n",
    "            progress_bar = tqdm(total=shard_size, unit='tokens', desc=f'{dataset_name}_{split} | Shard {shard_index}')\n",
    "            progress_bar.update(token_count)\n",
    "            # break on max_shards\n",
    "            if (max_shards>0 and shard_index > max_shards):\n",
    "                break\n",
    "    \n",
    "    # write any remaining tokens as the last shard\n",
    "    if token_count != 0:\n",
    "        all_tokens_np = all_tokens_np[0:token_count]\n",
    "        write_datafile(dataset_name, split, all_tokens_np, shard_index)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1827a169-2da7-40f9-9962-095b454a4c78",
   "metadata": {},
   "source": [
    "## Fineweb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c75a5a-850f-4a9a-89ac-fa041eb6473b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79fc64ee3ee486e881a070755e9200d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fw_tr = load_dataset('HuggingFaceFW/fineweb-edu', name='sample-10BT', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f5187ca-b059-4a68-b9ff-46efa254e80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fineweb-edu_train | Shard 0: 100%|███████████████████████████| 1000000/1000000 [00:01<00:00, 809350.74tokens/s]\n",
      "fineweb-edu_train | Shard 1: 100%|███████████████████████████| 1000000/1000000 [00:01<00:00, 851218.79tokens/s]\n",
      "fineweb-edu_train | Shard 2: 100%|███████████████████████████| 1000000/1000000 [00:01<00:00, 816027.37tokens/s]\n",
      "fineweb-edu_train | Shard 3: 100%|███████████████████████████| 1000000/1000000 [00:01<00:00, 862546.85tokens/s]\n",
      "fineweb-edu_train | Shard 4: 100%|███████████████████████████| 1000000/1000000 [00:01<00:00, 835805.93tokens/s]\n",
      "fineweb-edu_train | Shard 5: 100%|███████████████████████████| 1000000/1000000 [00:01<00:00, 815815.96tokens/s]\n",
      "fineweb-edu_train | Shard 6: 100%|███████████████████████████| 1000000/1000000 [00:01<00:00, 839137.04tokens/s]\n",
      "fineweb-edu_train | Shard 7: 100%|███████████████████████████| 1000000/1000000 [00:01<00:00, 741277.02tokens/s]\n",
      "fineweb-edu_train | Shard 8: 100%|███████████████████████████| 1000000/1000000 [00:01<00:00, 944660.51tokens/s]\n",
      "fineweb-edu_train | Shard 9: 100%|██████████████████████████| 1000000/1000000 [00:00<00:00, 1051366.06tokens/s]\n",
      "fineweb-edu_train | Shard 10: 100%|█████████████████████████| 1000000/1000000 [00:00<00:00, 1034399.30tokens/s]\n",
      "fineweb-edu_train | Shard 11:   0%|                            | 2754/1000000 [00:00<00:00, 2903019.15tokens/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_dataset(fw_tr, 'fineweb-edu', 'train', max_shards=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a39b6-8004-4ba3-be71-624a1dd9a922",
   "metadata": {},
   "source": [
    "## Wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf442db-6976-4799-ad55-8fb836c9e275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wikitext-103_train | Shard 0: 100%|█████████████████████████| 1000000/1000000 [00:00<00:00, 1586942.16tokens/s]\n",
      "wikitext-103_train | Shard 1: 100%|█████████████████████████| 1000000/1000000 [00:00<00:00, 1647187.36tokens/s]\n",
      "wikitext-103_train | Shard 2: 100%|█████████████████████████| 1000000/1000000 [00:00<00:00, 1674751.93tokens/s]\n",
      "wikitext-103_train | Shard 3: 100%|█████████████████████████| 1000000/1000000 [00:00<00:00, 1596866.50tokens/s]\n",
      "wikitext-103_train | Shard 4: 100%|█████████████████████████| 1000000/1000000 [00:00<00:00, 1621049.08tokens/s]\n",
      "wikitext-103_train | Shard 5: 100%|█████████████████████████| 1000000/1000000 [00:00<00:00, 1626793.21tokens/s]\n",
      "wikitext-103_train | Shard 6: 100%|█████████████████████████| 1000000/1000000 [00:00<00:00, 1589975.92tokens/s]\n",
      "wikitext-103_train | Shard 7: 100%|█████████████████████████| 1000000/1000000 [00:00<00:00, 1658283.86tokens/s]\n",
      "wikitext-103_train | Shard 8: 100%|█████████████████████████| 1000000/1000000 [00:00<00:00, 1637016.85tokens/s]\n",
      "wikitext-103_train | Shard 9: 100%|█████████████████████████| 1000000/1000000 [00:00<00:00, 1613794.67tokens/s]\n",
      "wikitext-103_train | Shard 10: 100%|████████████████████████| 1000000/1000000 [00:00<00:00, 1608919.69tokens/s]\n",
      "wikitext-103_train | Shard 11:   0%|                             | 725/1000000 [00:00<00:08, 120635.95tokens/s]\n"
     ]
    }
   ],
   "source": [
    "wt_tr = load_dataset('iohadrubin/wikitext-103-raw-v1', split='train', streaming=True)\n",
    "tokenize_dataset(wt_tr, 'wikitext-103', 'train', max_shards=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3baba79e-e8f6-413b-b02a-3af04973bb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wikitext-103_test | Shard 0:  28%|███████▍                   | 276095/1000000 [00:00<00:00, 1634496.93tokens/s]\n"
     ]
    }
   ],
   "source": [
    "wt_ts = load_dataset('iohadrubin/wikitext-103-raw-v1', split='test', streaming=True)\n",
    "tokenize_dataset(wt_ts, 'wikitext-103', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a8e0aa-f697-49e9-afc0-48235e38bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data = Path('/home/ubuntu/data/train')\n",
    "shards = sorted(list(train_data.iterdir()))\n",
    "shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4dd50-af10-4d04-a30a-ace5a3308be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
