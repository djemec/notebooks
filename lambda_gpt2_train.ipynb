{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1ccc17-34f1-4ce0-b52d-2ce6b226f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9337790f-85c2-4caa-86c0-a69913e2ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# distributed training\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06cac088-434c-4041-a81c-0cea116513f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  5 20:03:45 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:06:00.0 Off |                    0 |\n",
      "| N/A   42C    P0             48W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f31765f3-99d5-479a-a4f0-48b122248a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)\n",
    "data_path = Path('/home/ubuntu/data')\n",
    "log_path = Path('/home/ubuntu/log')\n",
    "model_path = Path('/home/ubuntu/model')\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    # elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    #     device = 'mps'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9f2c2-b676-48dc-ab3f-868c94f0999f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a337dc4-8af3-44c2-8757-4e2493ab9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # ensures that you can split embeddings across the heads\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projection for all heads in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, n_embd embedding dimensionality \n",
    "        # calculate query, key, value for all heads in batch, then move head forward\n",
    "        # nh - num heads, hs - head size, C  nh*hs aka channels\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # attention (materializes the large (T,T) matrix for all the queries and keys)\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v # (B, nh, T, T) X (B, nh, T, hs) - > (B, nh, T, hs)\n",
    "        # replace attention with flash attention \n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # allows for pathway to pass through gradients instead of going through each \"box\"\n",
    "        # this is a feed forward network\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length/context\n",
    "    vocab_size: int = 100276 # switched to GP4 tokenizer \n",
    "    n_layer: int = 12 \n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # weight tokenizer element\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # weight position element\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # hidden layers aka Transformers\n",
    "            ln_f = nn.LayerNorm(config.n_embd), #log normalization \n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # language model head going from embeddings to vocab\n",
    "\n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        mean = 0.0\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=mean, std=std)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f'Cannot forward sequence, out of context'\n",
    "        # forward the token and positions\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the block\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and head\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        if master_process:\n",
    "            print(f'num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters')\n",
    "            print(f'num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters')\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        if master_process:\n",
    "            print(f'using fused AdamW: {use_fused}')\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f6e099-15cf-4b2f-8a12-471bf4cc8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "    print(f'loading {filename}')\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, process_rank, num_processes, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in {'train','test','val'}\n",
    "\n",
    "        # get the shard filenames\n",
    "        data_root = data_path / f'{split}'\n",
    "        shards = list(data_root.iterdir())\n",
    "        self.shards = sorted(shards)\n",
    "        assert len(shards) > 0, f'no shards found for split {split}'\n",
    "        if master_process:\n",
    "            print(f'found {len(shards)} shards for split {split}')\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # state, init at shard zero\n",
    "        self.remaining_shards = self.shards\n",
    "        random.shuffle(self.remaining_shards)\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.remaining_shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        if len(buf) < B * T + 1:\n",
    "            self.current_shard += 1\n",
    "            if self.current_shard >= len(self.remaining_shards):\n",
    "                self.reset()\n",
    "            self.tokens = load_tokens(self.remaining_shards[self.current_shard])\n",
    "            self.current_position = B * T * self.process_rank\n",
    "            return self.next_batch()\n",
    "        x = (buf[:-1]).view(B, T)\n",
    "        y = (buf[1:]).view(B, T)\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353045c-7a7f-4af8-a400-8cd39f3fc183",
   "metadata": {},
   "source": [
    "## Setup distributed run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e5dc5c-fc7b-419f-9674-63e32a6983f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not using ddp, using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set up DDP (distributed data parallel).\n",
    "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
    "device_type = device # override device if using ddp do device_type acts as backup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
    "    assert torch.cuda.is_available(), 'for now i think we need CUDA for DDP'\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "else:\n",
    "    # vanilla, non-DDP run\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    print(f'not using ddp, using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd7d3e7-ddee-4cae-87eb-761365712204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 524288\n",
      "=> calculated gradient accumulation steps: 16\n",
      "found 153 shards for split train\n",
      "loading /home/ubuntu/data/train/train_fineweb_edu_000015.npy\n",
      "found 1 shards for split test\n",
      "loading /home/ubuntu/data/test/test_fineweb_edu_000000.npy\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens, made smaller for testing \n",
    "B = 32 # micro batch size\n",
    "T = 1024 # sequence length\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, 'make sure total_batch_size is divisible by B * T * ddp_world_size'\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "if master_process:\n",
    "    print(f'total desired batch size: {total_batch_size}')\n",
    "    print(f'=> calculated gradient accumulation steps: {grad_accum_steps}')\n",
    "\n",
    "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split='train')\n",
    "test_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split='test')\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=100608)) # make divisible by power of 2 was 50304\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "    \n",
    "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "750e6e53-0eab-414e-8d39-7c50d9a24136",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 1e-3\n",
    "min_lr = max_lr * 0.1\n",
    "max_steps = 30000 \n",
    "warmup_steps = 0.03 * max_steps # 5% warmup\n",
    "weight_decay = 0.1\n",
    "def get_lr(it):\n",
    "    # 1/ linear warmup \n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "    # 2/ if iterations > lr_decay_iters, return min learning rate\n",
    "    if it > max_steps: \n",
    "        return min_lr\n",
    "\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e9b12e6-f47e-4a72-a9a0-7dbd05b760f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 162,988,032 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=min_lr, betas=(0.9, 0.95), eps=1e-8)\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=max_lr, device_type=device_type)\n",
    "enc = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ce62d9-49ce-4517-9b26-02bcb450f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = log_path / 'log.txt'\n",
    "with open(log_file, 'w') as f: # open for writing to clear the file\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b61b328-12a7-46e9-bd2e-e26de69601c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /home/ubuntu/data/test/test_fineweb_edu_000000.npy\n",
      "test loss: 11.6788\n",
      "step     0 | loss: 11.680244 | lr 1.1111e-06| dt: 8532.67ms | tok/sec: 61444.78\n",
      "step     1 | loss: 11.615857 | lr 2.2222e-06| dt: 3327.48ms | tok/sec: 157563.13\n",
      "step     2 | loss: 11.487227 | lr 3.3333e-06| dt: 3331.74ms | tok/sec: 157361.41\n",
      "step     3 | loss: 11.334348 | lr 4.4444e-06| dt: 3334.18ms | tok/sec: 157246.34\n",
      "step     4 | loss: 11.158936 | lr 5.5556e-06| dt: 3337.24ms | tok/sec: 157102.14\n",
      "step     5 | loss: 11.032065 | lr 6.6667e-06| dt: 3340.01ms | tok/sec: 156971.98\n",
      "step     6 | loss: 10.896227 | lr 7.7778e-06| dt: 3338.51ms | tok/sec: 157042.57\n",
      "step     7 | loss: 10.793282 | lr 8.8889e-06| dt: 3339.89ms | tok/sec: 156977.74\n",
      "step     8 | loss: 10.707595 | lr 1.0000e-05| dt: 3341.78ms | tok/sec: 156889.05\n",
      "step     9 | loss: 10.639880 | lr 1.1111e-05| dt: 3340.73ms | tok/sec: 156937.94\n",
      "step    10 | loss: 10.565011 | lr 1.2222e-05| dt: 3339.39ms | tok/sec: 157001.16\n",
      "step    11 | loss: 10.522324 | lr 1.3333e-05| dt: 3337.05ms | tok/sec: 157111.03\n",
      "step    12 | loss: 10.461610 | lr 1.4444e-05| dt: 3338.08ms | tok/sec: 157062.58\n",
      "step    13 | loss: 10.414379 | lr 1.5556e-05| dt: 3337.95ms | tok/sec: 157068.94\n",
      "step    14 | loss: 10.399239 | lr 1.6667e-05| dt: 3338.47ms | tok/sec: 157044.28\n",
      "step    15 | loss: 10.377752 | lr 1.7778e-05| dt: 3338.05ms | tok/sec: 157064.00\n",
      "step    16 | loss: 10.327511 | lr 1.8889e-05| dt: 3335.60ms | tok/sec: 157179.29\n",
      "step    17 | loss: 10.318966 | lr 2.0000e-05| dt: 3338.07ms | tok/sec: 157063.02\n",
      "step    18 | loss: 10.336303 | lr 2.1111e-05| dt: 3335.63ms | tok/sec: 157177.93\n",
      "step    19 | loss: 10.283069 | lr 2.2222e-05| dt: 3332.56ms | tok/sec: 157323.12\n",
      "step    20 | loss: 10.256595 | lr 2.3333e-05| dt: 3334.56ms | tok/sec: 157228.60\n",
      "step    21 | loss: 10.257790 | lr 2.4444e-05| dt: 3335.76ms | tok/sec: 157172.18\n",
      "step    22 | loss: 10.223226 | lr 2.5556e-05| dt: 3337.87ms | tok/sec: 157072.45\n",
      "step    23 | loss: 10.196814 | lr 2.6667e-05| dt: 3336.76ms | tok/sec: 157124.81\n",
      "step    24 | loss: 10.171528 | lr 2.7778e-05| dt: 3335.56ms | tok/sec: 157181.61\n",
      "step    25 | loss: 10.169464 | lr 2.8889e-05| dt: 3333.56ms | tok/sec: 157275.53\n",
      "step    26 | loss: 10.135842 | lr 3.0000e-05| dt: 3334.43ms | tok/sec: 157234.66\n",
      "step    27 | loss: 10.085836 | lr 3.1111e-05| dt: 3334.32ms | tok/sec: 157239.70\n",
      "step    28 | loss: 10.056847 | lr 3.2222e-05| dt: 3331.04ms | tok/sec: 157394.80\n",
      "step    29 | loss: 10.067853 | lr 3.3333e-05| dt: 3329.45ms | tok/sec: 157470.02\n",
      "step    30 | loss: 10.010831 | lr 3.4444e-05| dt: 3332.28ms | tok/sec: 157336.31\n",
      "step    31 | loss: 9.990713 | lr 3.5556e-05| dt: 3331.07ms | tok/sec: 157393.46\n",
      "step    32 | loss: 9.927260 | lr 3.6667e-05| dt: 3332.24ms | tok/sec: 157337.84\n",
      "step    33 | loss: 9.912204 | lr 3.7778e-05| dt: 3331.47ms | tok/sec: 157374.19\n",
      "step    34 | loss: 9.857505 | lr 3.8889e-05| dt: 3332.77ms | tok/sec: 157313.15\n",
      "step    35 | loss: 9.811710 | lr 4.0000e-05| dt: 3331.65ms | tok/sec: 157366.09\n",
      "step    36 | loss: 9.775909 | lr 4.1111e-05| dt: 3331.10ms | tok/sec: 157391.66\n",
      "step    37 | loss: 9.747080 | lr 4.2222e-05| dt: 3333.36ms | tok/sec: 157285.20\n",
      "step    38 | loss: 9.711117 | lr 4.3333e-05| dt: 3332.05ms | tok/sec: 157346.82\n",
      "step    39 | loss: 9.702740 | lr 4.4444e-05| dt: 3333.03ms | tok/sec: 157300.59\n",
      "step    40 | loss: 9.623472 | lr 4.5556e-05| dt: 3334.16ms | tok/sec: 157247.36\n",
      "step    41 | loss: 9.568754 | lr 4.6667e-05| dt: 3336.77ms | tok/sec: 157124.54\n",
      "step    42 | loss: 9.544861 | lr 4.7778e-05| dt: 3336.01ms | tok/sec: 157160.31\n",
      "step    43 | loss: 9.482154 | lr 4.8889e-05| dt: 3335.81ms | tok/sec: 157169.66\n",
      "step    44 | loss: 9.459738 | lr 5.0000e-05| dt: 3336.47ms | tok/sec: 157138.46\n",
      "step    45 | loss: 9.429956 | lr 5.1111e-05| dt: 3335.65ms | tok/sec: 157177.20\n",
      "step    46 | loss: 9.347822 | lr 5.2222e-05| dt: 3336.89ms | tok/sec: 157118.77\n",
      "step    47 | loss: 9.307401 | lr 5.3333e-05| dt: 3338.45ms | tok/sec: 157045.23\n",
      "step    48 | loss: 9.260021 | lr 5.4444e-05| dt: 3335.44ms | tok/sec: 157187.16\n",
      "step    49 | loss: 9.189786 | lr 5.5556e-05| dt: 3337.94ms | tok/sec: 157069.51\n",
      "loading /home/ubuntu/data/test/test_fineweb_edu_000000.npy\n",
      "test loss: 9.1632\n",
      "step    50 | loss: 9.162560 | lr 5.6667e-05| dt: 5871.54ms | tok/sec: 89293.07\n",
      "step    51 | loss: 9.099123 | lr 5.7778e-05| dt: 3338.96ms | tok/sec: 157021.55\n",
      "step    52 | loss: 9.066711 | lr 5.8889e-05| dt: 3338.21ms | tok/sec: 157056.59\n",
      "step    53 | loss: 9.021205 | lr 6.0000e-05| dt: 3339.55ms | tok/sec: 156993.54\n",
      "step    54 | loss: 8.967000 | lr 6.1111e-05| dt: 3342.19ms | tok/sec: 156869.46\n",
      "step    55 | loss: 8.883826 | lr 6.2222e-05| dt: 3340.04ms | tok/sec: 156970.54\n",
      "step    56 | loss: 8.907516 | lr 6.3333e-05| dt: 3340.80ms | tok/sec: 156935.01\n",
      "step    57 | loss: 8.861392 | lr 6.4444e-05| dt: 3340.69ms | tok/sec: 156940.15\n",
      "step    58 | loss: 8.775156 | lr 6.5556e-05| dt: 3337.44ms | tok/sec: 157093.07\n",
      "step    59 | loss: 8.722844 | lr 6.6667e-05| dt: 3340.00ms | tok/sec: 156972.34\n",
      "step    60 | loss: 8.671771 | lr 6.7778e-05| dt: 3340.46ms | tok/sec: 156950.85\n",
      "step    61 | loss: 8.624255 | lr 6.8889e-05| dt: 3342.41ms | tok/sec: 156859.41\n",
      "step    62 | loss: 8.602681 | lr 7.0000e-05| dt: 3342.19ms | tok/sec: 156869.64\n",
      "step    63 | loss: 8.546608 | lr 7.1111e-05| dt: 3339.65ms | tok/sec: 156988.75\n",
      "step    64 | loss: 8.537984 | lr 7.2222e-05| dt: 3341.39ms | tok/sec: 156907.33\n",
      "step    65 | loss: 8.474247 | lr 7.3333e-05| dt: 3340.55ms | tok/sec: 156946.79\n",
      "step    66 | loss: 8.384536 | lr 7.4444e-05| dt: 3340.35ms | tok/sec: 156956.17\n",
      "step    67 | loss: 8.365441 | lr 7.5556e-05| dt: 3341.49ms | tok/sec: 156902.44\n",
      "step    68 | loss: 8.323149 | lr 7.6667e-05| dt: 3343.27ms | tok/sec: 156818.85\n",
      "step    69 | loss: 8.313155 | lr 7.7778e-05| dt: 3341.42ms | tok/sec: 156905.84\n",
      "step    70 | loss: 8.253600 | lr 7.8889e-05| dt: 3341.48ms | tok/sec: 156902.93\n",
      "step    71 | loss: 8.240832 | lr 8.0000e-05| dt: 3343.62ms | tok/sec: 156802.59\n",
      "step    72 | loss: 8.175714 | lr 8.1111e-05| dt: 3339.80ms | tok/sec: 156981.73\n",
      "step    73 | loss: 8.132392 | lr 8.2222e-05| dt: 3345.04ms | tok/sec: 156735.86\n",
      "step    74 | loss: 8.105103 | lr 8.3333e-05| dt: 3343.13ms | tok/sec: 156825.33\n",
      "step    75 | loss: 8.075610 | lr 8.4444e-05| dt: 3343.27ms | tok/sec: 156818.72\n",
      "step    76 | loss: 8.028434 | lr 8.5556e-05| dt: 3344.69ms | tok/sec: 156752.12\n",
      "step    77 | loss: 7.980367 | lr 8.6667e-05| dt: 3343.28ms | tok/sec: 156818.47\n",
      "step    78 | loss: 7.900038 | lr 8.7778e-05| dt: 3345.84ms | tok/sec: 156698.25\n",
      "step    79 | loss: 7.862428 | lr 8.8889e-05| dt: 3342.70ms | tok/sec: 156845.89\n",
      "step    80 | loss: 7.873017 | lr 9.0000e-05| dt: 3343.42ms | tok/sec: 156811.91\n",
      "step    81 | loss: 7.853995 | lr 9.1111e-05| dt: 3345.32ms | tok/sec: 156722.62\n",
      "step    82 | loss: 7.780142 | lr 9.2222e-05| dt: 3344.70ms | tok/sec: 156751.86\n",
      "step    83 | loss: 7.778057 | lr 9.3333e-05| dt: 3345.30ms | tok/sec: 156723.61\n",
      "step    84 | loss: 7.759328 | lr 9.4444e-05| dt: 3343.32ms | tok/sec: 156816.39\n",
      "step    85 | loss: 7.731306 | lr 9.5556e-05| dt: 3350.37ms | tok/sec: 156486.60\n",
      "step    86 | loss: 7.695786 | lr 9.6667e-05| dt: 3346.95ms | tok/sec: 156646.38\n",
      "step    87 | loss: 7.622550 | lr 9.7778e-05| dt: 3346.89ms | tok/sec: 156649.34\n",
      "step    88 | loss: 7.613003 | lr 9.8889e-05| dt: 3347.67ms | tok/sec: 156612.82\n",
      "step    89 | loss: 7.557775 | lr 1.0000e-04| dt: 3350.08ms | tok/sec: 156500.05\n",
      "step    90 | loss: 7.559483 | lr 1.0111e-04| dt: 3346.47ms | tok/sec: 156669.00\n",
      "step    91 | loss: 7.526459 | lr 1.0222e-04| dt: 3348.19ms | tok/sec: 156588.39\n",
      "step    92 | loss: 7.535336 | lr 1.0333e-04| dt: 3348.26ms | tok/sec: 156585.28\n",
      "step    93 | loss: 7.501894 | lr 1.0444e-04| dt: 3349.65ms | tok/sec: 156520.15\n",
      "step    94 | loss: 7.453341 | lr 1.0556e-04| dt: 3349.66ms | tok/sec: 156519.75\n",
      "step    95 | loss: 7.538297 | lr 1.0667e-04| dt: 3349.11ms | tok/sec: 156545.30\n",
      "step    96 | loss: 7.459115 | lr 1.0778e-04| dt: 3351.23ms | tok/sec: 156446.63\n",
      "step    97 | loss: 7.473802 | lr 1.0889e-04| dt: 3348.56ms | tok/sec: 156571.00\n",
      "step    98 | loss: 7.470281 | lr 1.1000e-04| dt: 3349.30ms | tok/sec: 156536.47\n",
      "step    99 | loss: 7.406713 | lr 1.1111e-04| dt: 3347.02ms | tok/sec: 156643.13\n",
      "loading /home/ubuntu/data/test/test_fineweb_edu_000000.npy\n",
      "test loss: 7.3903\n",
      "rank 0 sample 0: Hello, I'm a language model, the the most of a that are from the I and the. He at the the of the most, or the first\n",
      "rank 0 sample 1: Hello, I'm a language model, are one of the, the body would be to about its 201 by the 2. In in the body.\n",
      "step   100 | loss: 7.423803 | lr 1.1222e-04| dt: 8807.75ms | tok/sec: 59525.72\n",
      "step   101 | loss: 7.390124 | lr 1.1333e-04| dt: 3341.12ms | tok/sec: 156919.95\n",
      "step   102 | loss: 7.390665 | lr 1.1444e-04| dt: 3348.82ms | tok/sec: 156559.08\n",
      "step   103 | loss: 7.375504 | lr 1.1556e-04| dt: 3347.76ms | tok/sec: 156608.73\n",
      "step   104 | loss: 7.359015 | lr 1.1667e-04| dt: 3348.31ms | tok/sec: 156582.81\n",
      "step   105 | loss: 7.269827 | lr 1.1778e-04| dt: 3349.57ms | tok/sec: 156523.95\n",
      "step   106 | loss: 7.239723 | lr 1.1889e-04| dt: 3349.61ms | tok/sec: 156522.22\n",
      "step   107 | loss: 7.265174 | lr 1.2000e-04| dt: 3349.73ms | tok/sec: 156516.38\n",
      "step   108 | loss: 7.269264 | lr 1.2111e-04| dt: 3351.70ms | tok/sec: 156424.63\n",
      "step   109 | loss: 7.203442 | lr 1.2222e-04| dt: 3354.37ms | tok/sec: 156300.10\n",
      "step   110 | loss: 7.216761 | lr 1.2333e-04| dt: 3350.64ms | tok/sec: 156473.92\n",
      "step   111 | loss: 7.194648 | lr 1.2444e-04| dt: 3352.15ms | tok/sec: 156403.73\n",
      "step   112 | loss: 7.157559 | lr 1.2556e-04| dt: 3350.93ms | tok/sec: 156460.65\n",
      "step   113 | loss: 7.187396 | lr 1.2667e-04| dt: 3351.31ms | tok/sec: 156442.49\n",
      "step   114 | loss: 7.151817 | lr 1.2778e-04| dt: 3352.76ms | tok/sec: 156375.10\n",
      "step   115 | loss: 7.212904 | lr 1.2889e-04| dt: 3350.21ms | tok/sec: 156494.13\n",
      "step   116 | loss: 7.107139 | lr 1.3000e-04| dt: 3353.34ms | tok/sec: 156347.79\n",
      "step   117 | loss: 7.106823 | lr 1.3111e-04| dt: 3352.29ms | tok/sec: 156396.80\n",
      "step   118 | loss: 7.086449 | lr 1.3222e-04| dt: 3353.12ms | tok/sec: 156358.33\n",
      "step   119 | loss: 7.151420 | lr 1.3333e-04| dt: 3347.46ms | tok/sec: 156622.74\n",
      "step   120 | loss: 7.094643 | lr 1.3444e-04| dt: 3353.72ms | tok/sec: 156330.22\n",
      "step   121 | loss: 7.008717 | lr 1.3556e-04| dt: 3353.92ms | tok/sec: 156320.86\n",
      "step   122 | loss: 7.032061 | lr 1.3667e-04| dt: 3354.72ms | tok/sec: 156283.46\n",
      "step   123 | loss: 7.008278 | lr 1.3778e-04| dt: 3350.53ms | tok/sec: 156479.31\n",
      "step   124 | loss: 6.977047 | lr 1.3889e-04| dt: 3353.73ms | tok/sec: 156329.89\n",
      "step   125 | loss: 6.975805 | lr 1.4000e-04| dt: 3352.63ms | tok/sec: 156381.01\n",
      "step   126 | loss: 6.964976 | lr 1.4111e-04| dt: 3351.92ms | tok/sec: 156414.36\n",
      "step   127 | loss: 6.975086 | lr 1.4222e-04| dt: 3350.05ms | tok/sec: 156501.32\n",
      "step   128 | loss: 6.927440 | lr 1.4333e-04| dt: 3353.33ms | tok/sec: 156348.67\n",
      "step   129 | loss: 6.912778 | lr 1.4444e-04| dt: 3353.22ms | tok/sec: 156353.63\n",
      "step   130 | loss: 6.891499 | lr 1.4556e-04| dt: 3355.04ms | tok/sec: 156268.75\n",
      "step   131 | loss: 6.862215 | lr 1.4667e-04| dt: 3353.51ms | tok/sec: 156340.30\n",
      "step   132 | loss: 6.898763 | lr 1.4778e-04| dt: 3353.89ms | tok/sec: 156322.40\n",
      "step   133 | loss: 6.889197 | lr 1.4889e-04| dt: 3353.02ms | tok/sec: 156363.07\n",
      "step   134 | loss: 6.916406 | lr 1.5000e-04| dt: 3355.06ms | tok/sec: 156267.63\n",
      "step   135 | loss: 6.879323 | lr 1.5111e-04| dt: 3353.39ms | tok/sec: 156345.46\n",
      "step   136 | loss: 6.823357 | lr 1.5222e-04| dt: 3353.34ms | tok/sec: 156347.91\n",
      "step   137 | loss: 6.789776 | lr 1.5333e-04| dt: 3354.66ms | tok/sec: 156286.67\n",
      "step   138 | loss: 6.761707 | lr 1.5444e-04| dt: 3353.28ms | tok/sec: 156350.85\n",
      "step   139 | loss: 6.806386 | lr 1.5556e-04| dt: 3355.92ms | tok/sec: 156227.62\n",
      "step   140 | loss: 6.834849 | lr 1.5667e-04| dt: 3353.18ms | tok/sec: 156355.28\n",
      "step   141 | loss: 6.909733 | lr 1.5778e-04| dt: 3354.43ms | tok/sec: 156297.32\n",
      "step   142 | loss: 6.861695 | lr 1.5889e-04| dt: 3353.00ms | tok/sec: 156364.05\n",
      "step   143 | loss: 6.875760 | lr 1.6000e-04| dt: 3352.25ms | tok/sec: 156399.04\n",
      "step   144 | loss: 6.807095 | lr 1.6111e-04| dt: 3352.86ms | tok/sec: 156370.28\n",
      "step   145 | loss: 6.824952 | lr 1.6222e-04| dt: 3356.61ms | tok/sec: 156195.82\n",
      "step   146 | loss: 6.867228 | lr 1.6333e-04| dt: 3353.71ms | tok/sec: 156330.54\n",
      "step   147 | loss: 6.791525 | lr 1.6444e-04| dt: 3353.74ms | tok/sec: 156329.31\n",
      "step   148 | loss: 6.778088 | lr 1.6556e-04| dt: 3352.44ms | tok/sec: 156390.16\n",
      "step   149 | loss: 6.821303 | lr 1.6667e-04| dt: 3353.75ms | tok/sec: 156328.70\n",
      "loading /home/ubuntu/data/test/test_fineweb_edu_000000.npy\n",
      "test loss: 6.7476\n",
      "step   150 | loss: 6.813348 | lr 1.6778e-04| dt: 5893.55ms | tok/sec: 88959.66\n",
      "step   151 | loss: 6.742813 | lr 1.6889e-04| dt: 3351.65ms | tok/sec: 156426.89\n",
      "step   152 | loss: 6.711405 | lr 1.7000e-04| dt: 3353.64ms | tok/sec: 156334.22\n",
      "step   153 | loss: 6.675225 | lr 1.7111e-04| dt: 3357.88ms | tok/sec: 156136.63\n",
      "step   154 | loss: 6.692687 | lr 1.7222e-04| dt: 3356.01ms | tok/sec: 156223.40\n",
      "step   155 | loss: 6.748739 | lr 1.7333e-04| dt: 3356.97ms | tok/sec: 156179.14\n",
      "step   156 | loss: 6.681868 | lr 1.7444e-04| dt: 3355.66ms | tok/sec: 156240.08\n",
      "step   157 | loss: 6.706686 | lr 1.7556e-04| dt: 3356.49ms | tok/sec: 156201.34\n",
      "step   158 | loss: 6.688473 | lr 1.7667e-04| dt: 3355.53ms | tok/sec: 156246.16\n",
      "step   159 | loss: 6.641917 | lr 1.7778e-04| dt: 3352.77ms | tok/sec: 156374.55\n",
      "step   160 | loss: 6.645452 | lr 1.7889e-04| dt: 3357.33ms | tok/sec: 156162.02\n",
      "step   161 | loss: 6.590063 | lr 1.8000e-04| dt: 3357.72ms | tok/sec: 156144.10\n",
      "step   162 | loss: 6.633235 | lr 1.8111e-04| dt: 3356.75ms | tok/sec: 156189.24\n",
      "step   163 | loss: 6.600060 | lr 1.8222e-04| dt: 3356.11ms | tok/sec: 156218.78\n",
      "step   164 | loss: 6.610431 | lr 1.8333e-04| dt: 3358.40ms | tok/sec: 156112.21\n",
      "step   165 | loss: 6.614897 | lr 1.8444e-04| dt: 3355.74ms | tok/sec: 156236.19\n",
      "step   166 | loss: 6.637346 | lr 1.8556e-04| dt: 3356.87ms | tok/sec: 156183.68\n",
      "step   167 | loss: 6.597307 | lr 1.8667e-04| dt: 3354.49ms | tok/sec: 156294.35\n",
      "step   168 | loss: 6.549844 | lr 1.8778e-04| dt: 3355.71ms | tok/sec: 156237.59\n",
      "step   169 | loss: 6.619194 | lr 1.8889e-04| dt: 3355.70ms | tok/sec: 156238.24\n",
      "step   170 | loss: 6.584312 | lr 1.9000e-04| dt: 3352.50ms | tok/sec: 156387.28\n",
      "step   171 | loss: 6.617741 | lr 1.9111e-04| dt: 3357.74ms | tok/sec: 156143.03\n",
      "step   172 | loss: 6.618414 | lr 1.9222e-04| dt: 3356.15ms | tok/sec: 156216.95\n",
      "step   173 | loss: 6.590294 | lr 1.9333e-04| dt: 3358.23ms | tok/sec: 156120.11\n",
      "step   174 | loss: 6.488438 | lr 1.9444e-04| dt: 3357.47ms | tok/sec: 156155.87\n",
      "step   175 | loss: 6.540143 | lr 1.9556e-04| dt: 3355.81ms | tok/sec: 156232.71\n",
      "step   176 | loss: 6.547325 | lr 1.9667e-04| dt: 3353.83ms | tok/sec: 156324.96\n",
      "step   177 | loss: 6.513964 | lr 1.9778e-04| dt: 3355.01ms | tok/sec: 156270.14\n",
      "step   178 | loss: 6.488742 | lr 1.9889e-04| dt: 3357.74ms | tok/sec: 156143.23\n",
      "step   179 | loss: 6.464466 | lr 2.0000e-04| dt: 3357.84ms | tok/sec: 156138.62\n",
      "step   180 | loss: 6.452854 | lr 2.0111e-04| dt: 3356.55ms | tok/sec: 156198.58\n",
      "step   181 | loss: 6.494325 | lr 2.0222e-04| dt: 3353.85ms | tok/sec: 156324.10\n",
      "step   182 | loss: 6.503549 | lr 2.0333e-04| dt: 3355.56ms | tok/sec: 156244.75\n",
      "step   183 | loss: 6.430083 | lr 2.0444e-04| dt: 3357.56ms | tok/sec: 156151.68\n",
      "step   184 | loss: 6.438929 | lr 2.0556e-04| dt: 3357.99ms | tok/sec: 156131.48\n",
      "step   185 | loss: 6.442343 | lr 2.0667e-04| dt: 3353.90ms | tok/sec: 156321.70\n",
      "step   186 | loss: 6.468498 | lr 2.0778e-04| dt: 3354.77ms | tok/sec: 156281.43\n",
      "step   187 | loss: 6.560944 | lr 2.0889e-04| dt: 3359.50ms | tok/sec: 156061.13\n",
      "step   188 | loss: 6.574800 | lr 2.1000e-04| dt: 3356.59ms | tok/sec: 156196.79\n",
      "step   189 | loss: 6.556119 | lr 2.1111e-04| dt: 3353.35ms | tok/sec: 156347.54\n",
      "loading /home/ubuntu/data/train/train_fineweb_edu_000042.npy\n",
      "step   190 | loss: 6.504754 | lr 2.1222e-04| dt: 3355.71ms | tok/sec: 156237.54\n",
      "step   191 | loss: 6.491577 | lr 2.1333e-04| dt: 3353.03ms | tok/sec: 156362.40\n",
      "step   192 | loss: 6.446304 | lr 2.1444e-04| dt: 3355.48ms | tok/sec: 156248.06\n",
      "step   193 | loss: 6.475543 | lr 2.1556e-04| dt: 3353.91ms | tok/sec: 156321.62\n",
      "step   194 | loss: 6.438298 | lr 2.1667e-04| dt: 3354.92ms | tok/sec: 156274.34\n",
      "step   195 | loss: 6.456062 | lr 2.1778e-04| dt: 3354.01ms | tok/sec: 156316.82\n",
      "step   196 | loss: 6.523654 | lr 2.1889e-04| dt: 3357.20ms | tok/sec: 156168.23\n",
      "step   197 | loss: 6.496415 | lr 2.2000e-04| dt: 3356.17ms | tok/sec: 156216.28\n",
      "step   198 | loss: 6.404933 | lr 2.2111e-04| dt: 3354.16ms | tok/sec: 156310.00\n",
      "step   199 | loss: 6.468338 | lr 2.2222e-04| dt: 3355.62ms | tok/sec: 156241.55\n",
      "loading /home/ubuntu/data/test/test_fineweb_edu_000000.npy\n",
      "test loss: 6.4621\n",
      "rank 0 sample 0: Hello, I'm a language model, not a person is the 2009 on some of the time it that he was, this would not. The is\n",
      "rank 0 sample 1: Hello, I'm a language model, who should look at all the world have been in both is not. Although we have a part. So we must,\n",
      "step   200 | loss: 6.522018 | lr 2.2333e-04| dt: 5968.78ms | tok/sec: 87838.36\n",
      "step   201 | loss: 6.498263 | lr 2.2444e-04| dt: 3355.85ms | tok/sec: 156231.25\n",
      "step   202 | loss: 6.487253 | lr 2.2556e-04| dt: 3354.80ms | tok/sec: 156279.97\n",
      "step   203 | loss: 6.485002 | lr 2.2667e-04| dt: 3356.10ms | tok/sec: 156219.54\n",
      "step   204 | loss: 6.499707 | lr 2.2778e-04| dt: 3354.72ms | tok/sec: 156283.55\n",
      "step   205 | loss: 6.404840 | lr 2.2889e-04| dt: 3355.69ms | tok/sec: 156238.30\n",
      "step   206 | loss: 6.428760 | lr 2.3000e-04| dt: 3357.33ms | tok/sec: 156162.21\n",
      "step   207 | loss: 6.444669 | lr 2.3111e-04| dt: 3356.93ms | tok/sec: 156180.80\n",
      "step   208 | loss: 6.441346 | lr 2.3222e-04| dt: 3359.37ms | tok/sec: 156067.50\n",
      "step   209 | loss: 6.442323 | lr 2.3333e-04| dt: 3359.64ms | tok/sec: 156054.87\n",
      "step   210 | loss: 6.466493 | lr 2.3444e-04| dt: 3356.31ms | tok/sec: 156209.59\n",
      "step   211 | loss: 6.414279 | lr 2.3556e-04| dt: 3355.33ms | tok/sec: 156255.27\n",
      "step   212 | loss: 6.452778 | lr 2.3667e-04| dt: 3354.93ms | tok/sec: 156273.72\n",
      "step   213 | loss: 6.411885 | lr 2.3778e-04| dt: 3350.78ms | tok/sec: 156467.48\n",
      "step   214 | loss: 6.376482 | lr 2.3889e-04| dt: 3354.53ms | tok/sec: 156292.42\n",
      "step   215 | loss: 6.430173 | lr 2.4000e-04| dt: 3359.23ms | tok/sec: 156074.08\n",
      "step   216 | loss: 6.428843 | lr 2.4111e-04| dt: 3354.14ms | tok/sec: 156310.77\n",
      "step   217 | loss: 6.370186 | lr 2.4222e-04| dt: 3358.07ms | tok/sec: 156127.78\n",
      "step   218 | loss: 6.385312 | lr 2.4333e-04| dt: 3355.20ms | tok/sec: 156261.30\n",
      "step   219 | loss: 6.363964 | lr 2.4444e-04| dt: 3358.63ms | tok/sec: 156101.90\n",
      "step   220 | loss: 6.325248 | lr 2.4556e-04| dt: 3353.85ms | tok/sec: 156324.03\n",
      "step   221 | loss: 6.318515 | lr 2.4667e-04| dt: 3356.65ms | tok/sec: 156193.94\n",
      "step   222 | loss: 6.325449 | lr 2.4778e-04| dt: 3357.66ms | tok/sec: 156146.90\n",
      "step   223 | loss: 6.265154 | lr 2.4889e-04| dt: 3358.98ms | tok/sec: 156085.68\n",
      "step   224 | loss: 6.280416 | lr 2.5000e-04| dt: 3357.81ms | tok/sec: 156139.74\n",
      "step   225 | loss: 6.303136 | lr 2.5111e-04| dt: 3356.44ms | tok/sec: 156203.48\n",
      "step   226 | loss: 6.283275 | lr 2.5222e-04| dt: 3356.88ms | tok/sec: 156183.28\n",
      "step   227 | loss: 6.271190 | lr 2.5333e-04| dt: 3357.62ms | tok/sec: 156148.66\n",
      "step   228 | loss: 6.299950 | lr 2.5444e-04| dt: 3357.03ms | tok/sec: 156176.26\n",
      "step   229 | loss: 6.378628 | lr 2.5556e-04| dt: 3357.49ms | tok/sec: 156154.91\n",
      "step   230 | loss: 6.319876 | lr 2.5667e-04| dt: 3358.40ms | tok/sec: 156112.32\n",
      "step   231 | loss: 6.369683 | lr 2.5778e-04| dt: 3356.20ms | tok/sec: 156214.85\n",
      "step   232 | loss: 6.344084 | lr 2.5889e-04| dt: 3356.25ms | tok/sec: 156212.23\n",
      "step   233 | loss: 6.330707 | lr 2.6000e-04| dt: 3356.96ms | tok/sec: 156179.33\n",
      "step   234 | loss: 6.344133 | lr 2.6111e-04| dt: 3355.83ms | tok/sec: 156232.21\n",
      "step   235 | loss: 6.353747 | lr 2.6222e-04| dt: 3355.84ms | tok/sec: 156231.36\n",
      "step   236 | loss: 6.376596 | lr 2.6333e-04| dt: 3355.56ms | tok/sec: 156244.55\n",
      "step   237 | loss: 6.288407 | lr 2.6444e-04| dt: 3355.71ms | tok/sec: 156237.35\n",
      "step   238 | loss: 6.304148 | lr 2.6556e-04| dt: 3356.51ms | tok/sec: 156200.12\n",
      "step   239 | loss: 6.320280 | lr 2.6667e-04| dt: 3359.01ms | tok/sec: 156083.85\n",
      "step   240 | loss: 6.352381 | lr 2.6778e-04| dt: 3358.18ms | tok/sec: 156122.63\n",
      "step   241 | loss: 6.309753 | lr 2.6889e-04| dt: 3356.35ms | tok/sec: 156207.75\n",
      "step   242 | loss: 6.361414 | lr 2.7000e-04| dt: 3357.01ms | tok/sec: 156177.18\n",
      "step   243 | loss: 6.322607 | lr 2.7111e-04| dt: 3354.95ms | tok/sec: 156273.14\n",
      "step   244 | loss: 6.293328 | lr 2.7222e-04| dt: 3357.23ms | tok/sec: 156166.95\n",
      "step   245 | loss: 6.354223 | lr 2.7333e-04| dt: 3357.55ms | tok/sec: 156152.14\n",
      "step   246 | loss: 6.307302 | lr 2.7444e-04| dt: 3355.98ms | tok/sec: 156224.87\n",
      "step   247 | loss: 6.263841 | lr 2.7556e-04| dt: 3354.59ms | tok/sec: 156289.95\n",
      "step   248 | loss: 6.285870 | lr 2.7667e-04| dt: 3357.59ms | tok/sec: 156150.23\n",
      "step   249 | loss: 6.285832 | lr 2.7778e-04| dt: 3356.92ms | tok/sec: 156181.06\n",
      "loading /home/ubuntu/data/test/test_fineweb_edu_000000.npy\n",
      "test loss: 6.2355\n",
      "step   250 | loss: 6.237722 | lr 2.7889e-04| dt: 5890.42ms | tok/sec: 89006.96\n",
      "step   251 | loss: 6.232189 | lr 2.8000e-04| dt: 3358.70ms | tok/sec: 156098.62\n",
      "step   252 | loss: 6.152391 | lr 2.8111e-04| dt: 3358.71ms | tok/sec: 156098.16\n",
      "step   253 | loss: 6.198101 | lr 2.8222e-04| dt: 3360.24ms | tok/sec: 156026.77\n",
      "step   254 | loss: 6.220117 | lr 2.8333e-04| dt: 3361.89ms | tok/sec: 155950.30\n",
      "step   255 | loss: 6.133566 | lr 2.8444e-04| dt: 3360.03ms | tok/sec: 156036.56\n",
      "step   256 | loss: 6.195217 | lr 2.8556e-04| dt: 3358.98ms | tok/sec: 156085.64\n",
      "step   257 | loss: 6.234454 | lr 2.8667e-04| dt: 3359.56ms | tok/sec: 156058.36\n",
      "step   258 | loss: 6.189896 | lr 2.8778e-04| dt: 3360.10ms | tok/sec: 156033.67\n",
      "step   259 | loss: 6.181067 | lr 2.8889e-04| dt: 3361.76ms | tok/sec: 155956.20\n",
      "step   260 | loss: 6.186432 | lr 2.9000e-04| dt: 3363.14ms | tok/sec: 155892.35\n",
      "step   261 | loss: 6.204064 | lr 2.9111e-04| dt: 3361.81ms | tok/sec: 155954.28\n",
      "step   262 | loss: 6.245847 | lr 2.9222e-04| dt: 3358.51ms | tok/sec: 156107.52\n",
      "step   263 | loss: 6.196946 | lr 2.9333e-04| dt: 3361.27ms | tok/sec: 155979.11\n",
      "step   264 | loss: 6.226044 | lr 2.9444e-04| dt: 3357.56ms | tok/sec: 156151.39\n",
      "step   265 | loss: 6.238592 | lr 2.9556e-04| dt: 3360.15ms | tok/sec: 156031.34\n",
      "step   266 | loss: 6.256667 | lr 2.9667e-04| dt: 3358.22ms | tok/sec: 156120.58\n",
      "step   267 | loss: 6.222044 | lr 2.9778e-04| dt: 3361.23ms | tok/sec: 155980.89\n",
      "step   268 | loss: 6.245383 | lr 2.9889e-04| dt: 3363.24ms | tok/sec: 155887.70\n",
      "step   269 | loss: 6.272847 | lr 3.0000e-04| dt: 3359.97ms | tok/sec: 156039.72\n",
      "step   270 | loss: 6.200826 | lr 3.0111e-04| dt: 3359.59ms | tok/sec: 156057.11\n",
      "step   271 | loss: 6.205760 | lr 3.0222e-04| dt: 3358.11ms | tok/sec: 156125.97\n",
      "step   272 | loss: 6.156317 | lr 3.0333e-04| dt: 3359.70ms | tok/sec: 156052.06\n",
      "step   273 | loss: 6.187386 | lr 3.0444e-04| dt: 3356.57ms | tok/sec: 156197.37\n",
      "step   274 | loss: 6.189781 | lr 3.0556e-04| dt: 3362.63ms | tok/sec: 155916.00\n",
      "step   275 | loss: 6.191793 | lr 3.0667e-04| dt: 3361.65ms | tok/sec: 155961.31\n",
      "step   276 | loss: 6.211355 | lr 3.0778e-04| dt: 3361.26ms | tok/sec: 155979.75\n",
      "step   277 | loss: 6.169349 | lr 3.0889e-04| dt: 3362.20ms | tok/sec: 155935.81\n",
      "step   278 | loss: 6.121405 | lr 3.1000e-04| dt: 3358.38ms | tok/sec: 156113.45\n",
      "step   279 | loss: 6.168216 | lr 3.1111e-04| dt: 3355.50ms | tok/sec: 156247.18\n",
      "step   280 | loss: 6.164191 | lr 3.1222e-04| dt: 3358.00ms | tok/sec: 156131.19\n",
      "step   281 | loss: 6.105837 | lr 3.1333e-04| dt: 3359.71ms | tok/sec: 156051.38\n",
      "step   282 | loss: 6.128273 | lr 3.1444e-04| dt: 3359.05ms | tok/sec: 156082.01\n",
      "step   283 | loss: 6.126088 | lr 3.1556e-04| dt: 3356.85ms | tok/sec: 156184.71\n",
      "step   284 | loss: 6.103681 | lr 3.1667e-04| dt: 3358.54ms | tok/sec: 156105.99\n",
      "step   285 | loss: 6.090105 | lr 3.1778e-04| dt: 3360.55ms | tok/sec: 156012.37\n",
      "step   286 | loss: 6.098918 | lr 3.1889e-04| dt: 3357.99ms | tok/sec: 156131.60\n",
      "step   287 | loss: 6.066189 | lr 3.2000e-04| dt: 3357.47ms | tok/sec: 156155.62\n",
      "step   288 | loss: 6.105907 | lr 3.2111e-04| dt: 3357.68ms | tok/sec: 156146.13\n",
      "step   289 | loss: 6.057600 | lr 3.2222e-04| dt: 3359.73ms | tok/sec: 156050.85\n",
      "step   290 | loss: 6.013023 | lr 3.2333e-04| dt: 3358.72ms | tok/sec: 156097.56\n",
      "step   291 | loss: 6.087691 | lr 3.2444e-04| dt: 3357.29ms | tok/sec: 156164.09\n",
      "step   292 | loss: 6.085403 | lr 3.2556e-04| dt: 3358.73ms | tok/sec: 156097.30\n",
      "step   293 | loss: 6.072586 | lr 3.2667e-04| dt: 3356.48ms | tok/sec: 156201.72\n",
      "step   294 | loss: 6.053235 | lr 3.2778e-04| dt: 3357.15ms | tok/sec: 156170.65\n",
      "step   295 | loss: 6.015534 | lr 3.2889e-04| dt: 3357.56ms | tok/sec: 156151.67\n",
      "step   296 | loss: 6.116104 | lr 3.3000e-04| dt: 3357.96ms | tok/sec: 156132.80\n",
      "step   297 | loss: 6.153159 | lr 3.3111e-04| dt: 3358.38ms | tok/sec: 156113.26\n",
      "step   298 | loss: 6.111930 | lr 3.3222e-04| dt: 3359.98ms | tok/sec: 156039.05\n",
      "step   299 | loss: 6.117610 | lr 3.3333e-04| dt: 3359.32ms | tok/sec: 156069.83\n",
      "loading /home/ubuntu/data/test/test_fineweb_edu_000000.npy\n",
      "test loss: 6.0613\n",
      "rank 0 sample 0: Hello, I'm a language model, Iâ€™m at the world was, she got down a matter of us out that I would a way. I was going\n",
      "rank 0 sample 1: Hello, I'm a language model, if she saw the story, and this, he does something, it at my, and you have in his thing.\n",
      "\n",
      "step   300 | loss: 6.102914 | lr 3.3444e-04| dt: 6122.90ms | tok/sec: 85627.39\n",
      "step   301 | loss: 6.071406 | lr 3.3556e-04| dt: 3356.24ms | tok/sec: 156212.96\n",
      "step   302 | loss: 6.097832 | lr 3.3667e-04| dt: 3354.37ms | tok/sec: 156300.20\n",
      "step   303 | loss: 6.069065 | lr 3.3778e-04| dt: 3354.59ms | tok/sec: 156289.57\n",
      "step   304 | loss: 6.055999 | lr 3.3889e-04| dt: 3354.66ms | tok/sec: 156286.71\n",
      "step   305 | loss: 6.091259 | lr 3.4000e-04| dt: 3356.87ms | tok/sec: 156183.51\n",
      "step   306 | loss: 6.061538 | lr 3.4111e-04| dt: 3356.58ms | tok/sec: 156197.22\n",
      "step   307 | loss: 6.031144 | lr 3.4222e-04| dt: 3359.04ms | tok/sec: 156082.92\n",
      "step   308 | loss: 6.139420 | lr 3.4333e-04| dt: 3356.95ms | tok/sec: 156179.99\n",
      "step   309 | loss: 6.050467 | lr 3.4444e-04| dt: 3353.98ms | tok/sec: 156318.01\n",
      "step   310 | loss: 6.136279 | lr 3.4556e-04| dt: 3358.50ms | tok/sec: 156107.85\n",
      "step   311 | loss: 6.079333 | lr 3.4667e-04| dt: 3357.93ms | tok/sec: 156134.30\n",
      "step   312 | loss: 6.058088 | lr 3.4778e-04| dt: 3355.59ms | tok/sec: 156243.39\n",
      "step   313 | loss: 6.068442 | lr 3.4889e-04| dt: 3357.74ms | tok/sec: 156143.05\n",
      "step   314 | loss: 6.097839 | lr 3.5000e-04| dt: 3355.95ms | tok/sec: 156226.38\n",
      "step   315 | loss: 6.044780 | lr 3.5111e-04| dt: 3356.38ms | tok/sec: 156206.57\n",
      "step   316 | loss: 6.003936 | lr 3.5222e-04| dt: 3358.84ms | tok/sec: 156092.03\n",
      "step   317 | loss: 6.010043 | lr 3.5333e-04| dt: 3357.74ms | tok/sec: 156142.99\n",
      "step   318 | loss: 5.973312 | lr 3.5444e-04| dt: 3357.22ms | tok/sec: 156167.09\n",
      "step   319 | loss: 5.968194 | lr 3.5556e-04| dt: 3359.65ms | tok/sec: 156054.19\n",
      "step   320 | loss: 5.967871 | lr 3.5667e-04| dt: 3358.21ms | tok/sec: 156121.24\n",
      "step   321 | loss: 6.000506 | lr 3.5778e-04| dt: 3358.11ms | tok/sec: 156125.70\n",
      "step   322 | loss: 5.961474 | lr 3.5889e-04| dt: 3359.84ms | tok/sec: 156045.39\n",
      "step   323 | loss: 5.939570 | lr 3.6000e-04| dt: 3357.44ms | tok/sec: 156157.18\n",
      "step   324 | loss: 5.945381 | lr 3.6111e-04| dt: 3356.93ms | tok/sec: 156180.72\n",
      "step   325 | loss: 5.983810 | lr 3.6222e-04| dt: 3357.61ms | tok/sec: 156149.31\n",
      "step   326 | loss: 5.950000 | lr 3.6333e-04| dt: 3356.55ms | tok/sec: 156198.47\n",
      "step   327 | loss: 5.906121 | lr 3.6444e-04| dt: 3357.46ms | tok/sec: 156156.04\n",
      "step   328 | loss: 5.927064 | lr 3.6556e-04| dt: 3358.06ms | tok/sec: 156128.36\n",
      "step   329 | loss: 5.997515 | lr 3.6667e-04| dt: 3356.40ms | tok/sec: 156205.66\n",
      "step   330 | loss: 5.984255 | lr 3.6778e-04| dt: 3360.47ms | tok/sec: 156016.07\n",
      "step   331 | loss: 6.006923 | lr 3.6889e-04| dt: 3359.02ms | tok/sec: 156083.77\n",
      "step   332 | loss: 6.028868 | lr 3.7000e-04| dt: 3360.10ms | tok/sec: 156033.56\n",
      "step   333 | loss: 5.939949 | lr 3.7111e-04| dt: 3356.14ms | tok/sec: 156217.70\n",
      "step   334 | loss: 5.936568 | lr 3.7222e-04| dt: 3359.26ms | tok/sec: 156072.35\n",
      "step   335 | loss: 5.947262 | lr 3.7333e-04| dt: 3358.29ms | tok/sec: 156117.46\n",
      "step   336 | loss: 5.956046 | lr 3.7444e-04| dt: 3355.48ms | tok/sec: 156248.29\n",
      "step   337 | loss: 5.914601 | lr 3.7556e-04| dt: 3357.81ms | tok/sec: 156139.65\n",
      "step   338 | loss: 6.025669 | lr 3.7667e-04| dt: 3358.01ms | tok/sec: 156130.60\n",
      "step   339 | loss: 5.963440 | lr 3.7778e-04| dt: 3358.09ms | tok/sec: 156127.06\n",
      "step   340 | loss: 5.976128 | lr 3.7889e-04| dt: 3357.28ms | tok/sec: 156164.36\n",
      "step   341 | loss: 5.946444 | lr 3.8000e-04| dt: 3359.11ms | tok/sec: 156079.31\n",
      "step   342 | loss: 6.007844 | lr 3.8111e-04| dt: 3359.78ms | tok/sec: 156048.24\n",
      "step   343 | loss: 5.939389 | lr 3.8222e-04| dt: 3359.29ms | tok/sec: 156070.91\n",
      "step   344 | loss: 5.993301 | lr 3.8333e-04| dt: 3356.21ms | tok/sec: 156214.37\n",
      "step   345 | loss: 5.988376 | lr 3.8444e-04| dt: 3357.68ms | tok/sec: 156145.78\n",
      "step   346 | loss: 6.002064 | lr 3.8556e-04| dt: 3360.74ms | tok/sec: 156003.64\n",
      "step   347 | loss: 5.885924 | lr 3.8667e-04| dt: 3360.48ms | tok/sec: 156015.94\n",
      "step   348 | loss: 5.975609 | lr 3.8778e-04| dt: 3358.16ms | tok/sec: 156123.78\n",
      "step   349 | loss: 5.925887 | lr 3.8889e-04| dt: 3358.01ms | tok/sec: 156130.44\n",
      "loading /home/ubuntu/data/test/test_fineweb_edu_000000.npy\n",
      "test loss: 5.8848\n",
      "step   350 | loss: 5.893044 | lr 3.9000e-04| dt: 5937.15ms | tok/sec: 88306.30\n",
      "step   351 | loss: 5.935618 | lr 3.9111e-04| dt: 3356.80ms | tok/sec: 156186.72\n",
      "step   352 | loss: 5.915243 | lr 3.9222e-04| dt: 3356.40ms | tok/sec: 156205.58\n",
      "step   353 | loss: 5.845236 | lr 3.9333e-04| dt: 3357.04ms | tok/sec: 156175.79\n",
      "step   354 | loss: 5.895355 | lr 3.9444e-04| dt: 3356.50ms | tok/sec: 156200.76\n",
      "step   355 | loss: 5.910230 | lr 3.9556e-04| dt: 3355.87ms | tok/sec: 156230.13\n",
      "step   356 | loss: 5.836655 | lr 3.9667e-04| dt: 3357.42ms | tok/sec: 156157.98\n",
      "step   357 | loss: 5.845650 | lr 3.9778e-04| dt: 3357.61ms | tok/sec: 156149.06\n",
      "step   358 | loss: 5.889296 | lr 3.9889e-04| dt: 3357.10ms | tok/sec: 156172.90\n",
      "step   359 | loss: 5.856807 | lr 4.0000e-04| dt: 3358.95ms | tok/sec: 156086.76\n",
      "step   360 | loss: 5.829022 | lr 4.0111e-04| dt: 3361.18ms | tok/sec: 155983.09\n",
      "step   361 | loss: 5.840518 | lr 4.0222e-04| dt: 3358.43ms | tok/sec: 156111.03\n",
      "step   362 | loss: 5.824830 | lr 4.0333e-04| dt: 3359.48ms | tok/sec: 156062.17\n",
      "step   363 | loss: 5.826489 | lr 4.0444e-04| dt: 3360.43ms | tok/sec: 156018.03\n",
      "step   364 | loss: 5.900817 | lr 4.0556e-04| dt: 3359.45ms | tok/sec: 156063.77\n",
      "step   365 | loss: 5.855903 | lr 4.0667e-04| dt: 3358.66ms | tok/sec: 156100.44\n",
      "step   366 | loss: 5.880560 | lr 4.0778e-04| dt: 3360.59ms | tok/sec: 156010.86\n",
      "step   367 | loss: 5.829103 | lr 4.0889e-04| dt: 3357.36ms | tok/sec: 156160.65\n",
      "step   368 | loss: 5.866460 | lr 4.1000e-04| dt: 3359.05ms | tok/sec: 156082.42\n",
      "step   369 | loss: 5.866021 | lr 4.1111e-04| dt: 3362.50ms | tok/sec: 155922.03\n",
      "step   370 | loss: 5.833555 | lr 4.1222e-04| dt: 3359.00ms | tok/sec: 156084.74\n",
      "step   371 | loss: 5.879750 | lr 4.1333e-04| dt: 3359.81ms | tok/sec: 156046.88\n",
      "step   372 | loss: 5.862528 | lr 4.1444e-04| dt: 3361.56ms | tok/sec: 155965.49\n",
      "step   373 | loss: 5.876563 | lr 4.1556e-04| dt: 3360.33ms | tok/sec: 156022.88\n",
      "step   374 | loss: 5.851323 | lr 4.1667e-04| dt: 3359.66ms | tok/sec: 156054.11\n",
      "step   375 | loss: 5.808616 | lr 4.1778e-04| dt: 3356.09ms | tok/sec: 156220.01\n",
      "step   376 | loss: 5.780336 | lr 4.1889e-04| dt: 3358.20ms | tok/sec: 156121.85\n",
      "step   377 | loss: 5.865642 | lr 4.2000e-04| dt: 3357.62ms | tok/sec: 156148.51\n",
      "step   378 | loss: 5.827560 | lr 4.2111e-04| dt: 3359.28ms | tok/sec: 156071.50\n",
      "step   379 | loss: 5.797597 | lr 4.2222e-04| dt: 3361.00ms | tok/sec: 155991.82\n",
      "step   380 | loss: 5.814584 | lr 4.2333e-04| dt: 3358.34ms | tok/sec: 156115.38\n",
      "loading /home/ubuntu/data/train/train_fineweb_edu_000088.npy\n",
      "step   381 | loss: 5.785749 | lr 4.2444e-04| dt: 3368.22ms | tok/sec: 155657.11\n",
      "step   382 | loss: 5.755624 | lr 4.2556e-04| dt: 3359.39ms | tok/sec: 156066.26\n",
      "step   383 | loss: 5.770096 | lr 4.2667e-04| dt: 3357.86ms | tok/sec: 156137.72\n",
      "step   384 | loss: 5.782614 | lr 4.2778e-04| dt: 3360.22ms | tok/sec: 156028.05\n",
      "step   385 | loss: 5.740061 | lr 4.2889e-04| dt: 3360.91ms | tok/sec: 155995.99\n",
      "step   386 | loss: 5.723359 | lr 4.3000e-04| dt: 3360.04ms | tok/sec: 156036.21\n",
      "step   387 | loss: 5.735949 | lr 4.3111e-04| dt: 3358.63ms | tok/sec: 156101.60\n",
      "step   388 | loss: 5.739624 | lr 4.3222e-04| dt: 3353.49ms | tok/sec: 156340.92\n",
      "step   389 | loss: 5.757134 | lr 4.3333e-04| dt: 3356.03ms | tok/sec: 156222.70\n",
      "step   390 | loss: 5.674937 | lr 4.3444e-04| dt: 3357.44ms | tok/sec: 156156.98\n",
      "step   391 | loss: 5.660930 | lr 4.3556e-04| dt: 3358.23ms | tok/sec: 156120.37\n",
      "step   392 | loss: 5.707749 | lr 4.3667e-04| dt: 3356.38ms | tok/sec: 156206.58\n",
      "step   393 | loss: 5.702526 | lr 4.3778e-04| dt: 3356.99ms | tok/sec: 156177.93\n",
      "step   394 | loss: 5.700652 | lr 4.3889e-04| dt: 3354.75ms | tok/sec: 156282.10\n",
      "step   395 | loss: 5.652479 | lr 4.4000e-04| dt: 3359.91ms | tok/sec: 156042.09\n",
      "step   396 | loss: 5.608747 | lr 4.4111e-04| dt: 3359.12ms | tok/sec: 156079.03\n",
      "step   397 | loss: 5.614471 | lr 4.4222e-04| dt: 3356.48ms | tok/sec: 156201.74\n",
      "step   398 | loss: 5.650930 | lr 4.4333e-04| dt: 3357.73ms | tok/sec: 156143.40\n",
      "step   399 | loss: 5.618995 | lr 4.4444e-04| dt: 3360.40ms | tok/sec: 156019.60\n",
      "loading /home/ubuntu/data/test/test_fineweb_edu_000000.npy\n",
      "test loss: 5.6901\n",
      "rank 0 sample 0: Hello, I'm a language model, Iâ€™m doing a good part is it â€“ then I are going that something we are looking at what is to the text\n",
      "rank 0 sample 1: Hello, I'm a language model, 20, 2016. S. (R); the same case, I am also a large-scale at the\n",
      "step   400 | loss: 5.660442 | lr 4.4556e-04| dt: 5979.23ms | tok/sec: 87684.87\n",
      "step   401 | loss: 5.658157 | lr 4.4667e-04| dt: 3353.73ms | tok/sec: 156329.60\n",
      "step   402 | loss: 5.749251 | lr 4.4778e-04| dt: 3351.56ms | tok/sec: 156430.89\n",
      "step   403 | loss: 5.698859 | lr 4.4889e-04| dt: 3355.11ms | tok/sec: 156265.43\n",
      "step   404 | loss: 5.751951 | lr 4.5000e-04| dt: 3354.93ms | tok/sec: 156273.74\n",
      "step   405 | loss: 5.683059 | lr 4.5111e-04| dt: 3356.36ms | tok/sec: 156207.47\n",
      "step   406 | loss: 5.693018 | lr 4.5222e-04| dt: 3356.32ms | tok/sec: 156208.99\n",
      "step   407 | loss: 5.672379 | lr 4.5333e-04| dt: 3354.99ms | tok/sec: 156271.23\n",
      "step   408 | loss: 5.724021 | lr 4.5444e-04| dt: 3356.64ms | tok/sec: 156194.20\n",
      "step   409 | loss: 5.698020 | lr 4.5556e-04| dt: 3358.13ms | tok/sec: 156124.75\n",
      "step   410 | loss: 5.672198 | lr 4.5667e-04| dt: 3359.42ms | tok/sec: 156065.18\n",
      "step   411 | loss: 5.641241 | lr 4.5778e-04| dt: 3360.97ms | tok/sec: 155992.87\n",
      "step   412 | loss: 5.558138 | lr 4.5889e-04| dt: 3361.37ms | tok/sec: 155974.42\n",
      "step   413 | loss: 5.591732 | lr 4.6000e-04| dt: 3360.41ms | tok/sec: 156019.01\n",
      "step   414 | loss: 5.592174 | lr 4.6111e-04| dt: 3361.79ms | tok/sec: 155954.94\n",
      "step   415 | loss: 5.652822 | lr 4.6222e-04| dt: 3361.69ms | tok/sec: 155959.67\n",
      "step   416 | loss: 5.714314 | lr 4.6333e-04| dt: 3360.75ms | tok/sec: 156003.43\n",
      "step   417 | loss: 5.599835 | lr 4.6444e-04| dt: 3363.23ms | tok/sec: 155888.37\n",
      "step   418 | loss: 5.588562 | lr 4.6556e-04| dt: 3360.89ms | tok/sec: 155996.99\n",
      "step   419 | loss: 5.590164 | lr 4.6667e-04| dt: 3360.24ms | tok/sec: 156026.76\n",
      "step   420 | loss: 5.584595 | lr 4.6778e-04| dt: 3363.18ms | tok/sec: 155890.71\n",
      "step   421 | loss: 5.593506 | lr 4.6889e-04| dt: 3360.73ms | tok/sec: 156004.17\n",
      "step   422 | loss: 5.587018 | lr 4.7000e-04| dt: 3361.96ms | tok/sec: 155947.21\n",
      "step   423 | loss: 5.627324 | lr 4.7111e-04| dt: 3360.32ms | tok/sec: 156023.13\n",
      "step   424 | loss: 5.540902 | lr 4.7222e-04| dt: 3361.57ms | tok/sec: 155965.43\n",
      "step   425 | loss: 5.562380 | lr 4.7333e-04| dt: 3360.34ms | tok/sec: 156022.21\n",
      "step   426 | loss: 5.493463 | lr 4.7444e-04| dt: 3359.95ms | tok/sec: 156040.54\n",
      "step   427 | loss: 5.556418 | lr 4.7556e-04| dt: 3360.65ms | tok/sec: 156008.06\n",
      "step   428 | loss: 5.541063 | lr 4.7667e-04| dt: 3359.18ms | tok/sec: 156076.34\n",
      "step   429 | loss: 5.602527 | lr 4.7778e-04| dt: 3359.57ms | tok/sec: 156057.92\n",
      "step   430 | loss: 5.508616 | lr 4.7889e-04| dt: 3361.99ms | tok/sec: 155945.53\n",
      "step   431 | loss: 5.538126 | lr 4.8000e-04| dt: 3363.77ms | tok/sec: 155863.41\n",
      "step   432 | loss: 5.513350 | lr 4.8111e-04| dt: 3362.45ms | tok/sec: 155924.49\n",
      "step   433 | loss: 5.560207 | lr 4.8222e-04| dt: 3364.80ms | tok/sec: 155815.28\n",
      "step   434 | loss: 5.501208 | lr 4.8333e-04| dt: 3363.52ms | tok/sec: 155874.79\n",
      "step   435 | loss: 5.577136 | lr 4.8444e-04| dt: 3362.25ms | tok/sec: 155933.65\n",
      "step   436 | loss: 5.630068 | lr 4.8556e-04| dt: 3360.25ms | tok/sec: 156026.71\n",
      "step   437 | loss: 5.632778 | lr 4.8667e-04| dt: 3359.25ms | tok/sec: 156073.11\n",
      "step   438 | loss: 5.663821 | lr 4.8778e-04| dt: 3363.36ms | tok/sec: 155882.08\n",
      "step   439 | loss: 5.572610 | lr 4.8889e-04| dt: 3363.91ms | tok/sec: 155856.71\n",
      "step   440 | loss: 5.629997 | lr 4.9000e-04| dt: 3361.71ms | tok/sec: 155958.68\n",
      "step   441 | loss: 5.573571 | lr 4.9111e-04| dt: 3359.24ms | tok/sec: 156073.57\n",
      "step   442 | loss: 5.563723 | lr 4.9222e-04| dt: 3360.86ms | tok/sec: 155998.16\n",
      "step   443 | loss: 5.564824 | lr 4.9333e-04| dt: 3360.51ms | tok/sec: 156014.60\n",
      "step   444 | loss: 5.548103 | lr 4.9444e-04| dt: 3362.50ms | tok/sec: 155922.03\n",
      "step   445 | loss: 5.565237 | lr 4.9556e-04| dt: 3362.73ms | tok/sec: 155911.34\n",
      "step   446 | loss: 5.540852 | lr 4.9667e-04| dt: 3362.53ms | tok/sec: 155920.81\n",
      "step   447 | loss: 5.498677 | lr 4.9778e-04| dt: 3361.38ms | tok/sec: 155973.99\n",
      "step   448 | loss: 5.497721 | lr 4.9889e-04| dt: 3361.44ms | tok/sec: 155971.15\n",
      "step   449 | loss: 5.480248 | lr 5.0000e-04| dt: 3360.95ms | tok/sec: 155993.80\n",
      "loading /home/ubuntu/data/test/test_fineweb_edu_000000.npy\n",
      "test loss: 5.5027\n",
      "step   450 | loss: 5.493762 | lr 5.0111e-04| dt: 5894.52ms | tok/sec: 88944.97\n",
      "step   451 | loss: 5.440845 | lr 5.0222e-04| dt: 3359.87ms | tok/sec: 156044.36\n",
      "step   452 | loss: 5.508285 | lr 5.0333e-04| dt: 3358.16ms | tok/sec: 156123.81\n",
      "step   453 | loss: 5.451920 | lr 5.0444e-04| dt: 3361.15ms | tok/sec: 155984.74\n",
      "step   454 | loss: 5.478318 | lr 5.0556e-04| dt: 3359.85ms | tok/sec: 156045.25\n",
      "step   455 | loss: 5.499943 | lr 5.0667e-04| dt: 3364.43ms | tok/sec: 155832.74\n",
      "step   456 | loss: 5.470482 | lr 5.0778e-04| dt: 3360.63ms | tok/sec: 156008.85\n",
      "step   457 | loss: 5.480500 | lr 5.0889e-04| dt: 3362.72ms | tok/sec: 155911.66\n",
      "step   458 | loss: 5.460650 | lr 5.1000e-04| dt: 3360.64ms | tok/sec: 156008.50\n",
      "step   459 | loss: 5.439634 | lr 5.1111e-04| dt: 3361.70ms | tok/sec: 155959.06\n",
      "step   460 | loss: 5.409370 | lr 5.1222e-04| dt: 3362.31ms | tok/sec: 155930.87\n",
      "step   461 | loss: 5.426194 | lr 5.1333e-04| dt: 3363.14ms | tok/sec: 155892.53\n",
      "step   462 | loss: 5.387851 | lr 5.1444e-04| dt: 3360.07ms | tok/sec: 156034.80\n",
      "step   463 | loss: 5.378032 | lr 5.1556e-04| dt: 3361.61ms | tok/sec: 155963.54\n",
      "step   464 | loss: 5.446928 | lr 5.1667e-04| dt: 3360.02ms | tok/sec: 156037.10\n",
      "step   465 | loss: 5.438850 | lr 5.1778e-04| dt: 3363.21ms | tok/sec: 155889.21\n",
      "step   466 | loss: 5.402611 | lr 5.1889e-04| dt: 3363.44ms | tok/sec: 155878.29\n",
      "step   467 | loss: 5.410824 | lr 5.2000e-04| dt: 3362.34ms | tok/sec: 155929.57\n",
      "step   468 | loss: 5.380817 | lr 5.2111e-04| dt: 3362.50ms | tok/sec: 155922.05\n",
      "step   469 | loss: 5.343401 | lr 5.2222e-04| dt: 3359.41ms | tok/sec: 156065.71\n",
      "step   470 | loss: 5.544500 | lr 5.2333e-04| dt: 3356.70ms | tok/sec: 156191.36\n",
      "step   471 | loss: 5.490825 | lr 5.2444e-04| dt: 3359.40ms | tok/sec: 156065.77\n",
      "step   472 | loss: 5.481743 | lr 5.2556e-04| dt: 3360.01ms | tok/sec: 156037.69\n",
      "step   473 | loss: 5.522397 | lr 5.2667e-04| dt: 3359.66ms | tok/sec: 156053.80\n",
      "step   474 | loss: 5.460538 | lr 5.2778e-04| dt: 3356.78ms | tok/sec: 156187.91\n",
      "step   475 | loss: 5.460287 | lr 5.2889e-04| dt: 3357.08ms | tok/sec: 156173.79\n",
      "step   476 | loss: 5.468648 | lr 5.3000e-04| dt: 3356.15ms | tok/sec: 156217.28\n",
      "step   477 | loss: 5.493395 | lr 5.3111e-04| dt: 3356.81ms | tok/sec: 156186.20\n",
      "step   478 | loss: 5.449852 | lr 5.3222e-04| dt: 3360.28ms | tok/sec: 156025.17\n",
      "step   479 | loss: 5.450207 | lr 5.3333e-04| dt: 3361.97ms | tok/sec: 155946.68\n",
      "step   480 | loss: 5.456996 | lr 5.3444e-04| dt: 3358.75ms | tok/sec: 156096.24\n",
      "step   481 | loss: 5.367118 | lr 5.3556e-04| dt: 3359.72ms | tok/sec: 156051.24\n",
      "step   482 | loss: 5.409686 | lr 5.3667e-04| dt: 3357.42ms | tok/sec: 156157.94\n",
      "step   483 | loss: 5.413738 | lr 5.3778e-04| dt: 3357.14ms | tok/sec: 156170.93\n",
      "step   484 | loss: 5.353860 | lr 5.3889e-04| dt: 3358.85ms | tok/sec: 156091.45\n",
      "step   485 | loss: 5.369142 | lr 5.4000e-04| dt: 3359.83ms | tok/sec: 156046.17\n",
      "step   486 | loss: 5.386997 | lr 5.4111e-04| dt: 3359.81ms | tok/sec: 156046.87\n",
      "step   487 | loss: 5.354769 | lr 5.4222e-04| dt: 3358.86ms | tok/sec: 156090.93\n",
      "step   488 | loss: 5.370353 | lr 5.4333e-04| dt: 3360.14ms | tok/sec: 156031.68\n",
      "step   489 | loss: 5.342360 | lr 5.4444e-04| dt: 3359.64ms | tok/sec: 156054.59\n",
      "step   490 | loss: 5.350612 | lr 5.4556e-04| dt: 3359.18ms | tok/sec: 156076.40\n",
      "step   491 | loss: 5.353179 | lr 5.4667e-04| dt: 3358.58ms | tok/sec: 156103.94\n",
      "step   492 | loss: 5.331429 | lr 5.4778e-04| dt: 3359.49ms | tok/sec: 156061.94\n",
      "step   493 | loss: 5.290708 | lr 5.4889e-04| dt: 3360.56ms | tok/sec: 156012.10\n",
      "step   494 | loss: 5.337713 | lr 5.5000e-04| dt: 3361.58ms | tok/sec: 155964.76\n",
      "step   495 | loss: 5.324796 | lr 5.5111e-04| dt: 3357.88ms | tok/sec: 156136.42\n",
      "step   496 | loss: 5.316373 | lr 5.5222e-04| dt: 3358.31ms | tok/sec: 156116.64\n",
      "step   497 | loss: 5.279305 | lr 5.5333e-04| dt: 3358.06ms | tok/sec: 156128.45\n",
      "step   498 | loss: 5.357700 | lr 5.5444e-04| dt: 3355.93ms | tok/sec: 156227.12\n",
      "step   499 | loss: 5.278685 | lr 5.5556e-04| dt: 3358.10ms | tok/sec: 156126.46\n",
      "loading /home/ubuntu/data/test/test_fineweb_edu_000000.npy\n",
      "test loss: 5.3523\n",
      "rank 0 sample 0: Hello, I'm a language model, and I'll be able to do so the reader and create the right point, and you see this topic!\n",
      "The most\n",
      "rank 0 sample 1: Hello, I'm a language model, this doesnâ€™t be able to get help you.\n",
      "Some problems can you, like to get more than other people who have\n",
      "step   500 | loss: 5.301045 | lr 5.5667e-04| dt: 5980.07ms | tok/sec: 87672.54\n",
      "step   501 | loss: 5.247996 | lr 5.5778e-04| dt: 3354.81ms | tok/sec: 156279.67\n",
      "step   502 | loss: 5.246390 | lr 5.5889e-04| dt: 3353.99ms | tok/sec: 156317.73\n",
      "step   503 | loss: 5.220758 | lr 5.6000e-04| dt: 3357.15ms | tok/sec: 156170.76\n",
      "step   504 | loss: 5.331483 | lr 5.6111e-04| dt: 3356.76ms | tok/sec: 156188.59\n",
      "step   505 | loss: 5.339379 | lr 5.6222e-04| dt: 3356.08ms | tok/sec: 156220.39\n",
      "step   506 | loss: 5.331674 | lr 5.6333e-04| dt: 3359.47ms | tok/sec: 156062.82\n",
      "step   507 | loss: 5.352892 | lr 5.6444e-04| dt: 3358.00ms | tok/sec: 156131.10\n",
      "step   508 | loss: 5.365921 | lr 5.6556e-04| dt: 3358.33ms | tok/sec: 156115.78\n",
      "step   509 | loss: 5.386514 | lr 5.6667e-04| dt: 3359.11ms | tok/sec: 156079.42\n",
      "step   510 | loss: 5.312251 | lr 5.6778e-04| dt: 3356.11ms | tok/sec: 156218.95\n",
      "step   511 | loss: 5.360116 | lr 5.6889e-04| dt: 3358.01ms | tok/sec: 156130.57\n",
      "step   512 | loss: 5.360836 | lr 5.7000e-04| dt: 3355.78ms | tok/sec: 156234.25\n",
      "step   513 | loss: 5.347091 | lr 5.7111e-04| dt: 3359.20ms | tok/sec: 156075.03\n",
      "step   514 | loss: 5.363316 | lr 5.7222e-04| dt: 3355.89ms | tok/sec: 156229.26\n",
      "step   515 | loss: 5.276935 | lr 5.7333e-04| dt: 3358.33ms | tok/sec: 156115.71\n",
      "step   516 | loss: 5.331350 | lr 5.7444e-04| dt: 3360.24ms | tok/sec: 156026.79\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     86\u001b[39m     loss = loss / grad_accum_steps\n\u001b[32m     87\u001b[39m     loss_accum += loss.detach()\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ddp:\n\u001b[32m     90\u001b[39m     dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/general/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/general/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/general/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == max_steps - 1)\n",
    "    \n",
    "    # once in a while evaluate our test loss\n",
    "    if step % 50 == 0 or last_step:\n",
    "        model.eval()\n",
    "        test_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            test_loss_accum = 0.0\n",
    "            test_loss_steps = 20\n",
    "            for _ in range(test_loss_steps):\n",
    "                x, y = test_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(x, y)\n",
    "                loss = loss / test_loss_steps\n",
    "                test_loss_accum += loss.detach()\n",
    "        if ddp:\n",
    "            dist.all_reduce(test_loss_accum, op=dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f'test loss: {test_loss_accum.item():.4f}')\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "            if step > 0 or last_step:\n",
    "                # optionally write model checkpoints\n",
    "                checkpoint_path = model_path / f'model_{step:05d}.pt'\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'config': raw_model.config,\n",
    "                    'step': step,\n",
    "                    'test_loss': test_loss_accum.item()\n",
    "                }\n",
    "                # you might also want to add optimizer.state_dict() and\n",
    "                # rng seeds etc., if you wanted to more exactly resume training\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # once in a while generate from the model (except step 0, which is noise)\n",
    "    if ((step > 0 and step % 100 == 0) or last_step):\n",
    "        model.eval()\n",
    "        num_return_sequences = 2\n",
    "        max_length = 32\n",
    "        tokens = enc.encode('Hello, I\\'m a language model,')\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "        xgen = tokens.to(device)\n",
    "        sample_rng = torch.Generator(device=device)\n",
    "        sample_rng.manual_seed(42 + ddp_rank)\n",
    "        while xgen.size(1) < max_length:\n",
    "            # forward the model to get the logits\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "                # take the logits at the last position\n",
    "                logits = logits[:, -1, :] # (B, vocab_size)\n",
    "                # get the probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                # do top-k sampling of 50 (huggingface pipeline default)\n",
    "                # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "                # select a token from the top-k probabilities\n",
    "                # note: multinomial does not demand the input to sum to 1\n",
    "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "                # gather the corresponding indices\n",
    "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "                # append to the sequence\n",
    "                xgen = torch.cat((xgen, xcol), dim=1)\n",
    "        # print the generated text\n",
    "        for i in range(num_return_sequences):\n",
    "            tokens = xgen[i, :max_length].tolist()\n",
    "            decoded = enc.decode(tokens)\n",
    "            print(f'rank {ddp_rank} sample {i}: {decoded}')\n",
    "\n",
    "    # do one step of the optimization\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # added after video, this field is also used by the forward pass.\n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    if ddp:\n",
    "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    if device_type == 'cuda':\n",
    "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0 # time difference in seconds\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    if master_process:\n",
    "        lossi.append(loss_accum.item())\n",
    "        print(f'step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e}| dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}')\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f5aba-b3e9-484f-8145-e1a7215eb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9a377-4641-46f3-ade7-1d795260716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd60fd9-9adb-491d-a690-743aa927cb2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
