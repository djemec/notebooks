{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a8f1341-7d67-4e5b-9eda-23ee1e4fb4a2",
   "metadata": {},
   "source": [
    "# GPT Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af962655-070a-43d7-a3cf-a4ef90f3096d",
   "metadata": {},
   "source": [
    "the purpose of this is to walk through what happens during the forward pass and backward pass of GPT-2 like models.  To help display the transformation, we'll use the first sentence from the [linear algebra wiki page](https://en.wikipedia.org/wiki/Linear_algebra) and [lu decomposition wiki page](https://en.wikipedia.org/wiki/LU_decomposition) as the topic is fitting and it shows us some non-standard patterns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba0d1fa-baff-47b0-94ec-6bb0e4aaeded",
   "metadata": {},
   "source": [
    "## Text Prep/Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1067a-aa58-4916-af71-d24e1faa80da",
   "metadata": {},
   "source": [
    "we'll start with common preprocessing step of tokenizing the data.  This converts the string text into an array of numbers that can be used during the training loop.  I've built a very subtle byte-pair encdoing that has each unique character that appears and the top 5 merges. This keeps our vocab size small and managable for this example. Typically the vocab size is in the 100K+ range. A great library for this is `tiktoken`. Tokenization simply finds the longest pattern of characters that's in common with what was trained and replaces it with an integer that represents it.  This way we turn the text into a numeric array to simplify computing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0fa4a6-5aeb-4d5e-b233-5594176e55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c63305c1-847c-441e-8982-e76dddc05ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPETokenizer:\n",
    "    def __init__(self, num_merges=5, eot_token='<|endoftext|>'):\n",
    "        self.num_merges = num_merges\n",
    "        self.eot_token = eot_token\n",
    "        self.eot_id = None\n",
    "        self.merges = []\n",
    "        self.pair_ranks = {}\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "    def _add_token(self, tok):\n",
    "        if tok in self.vocab:\n",
    "            return self.vocab[tok]\n",
    "        i = len(self.vocab)\n",
    "        self.vocab[tok] = i\n",
    "        self.id_to_token[i] = tok\n",
    "        return i\n",
    "\n",
    "    def _get_bigrams(self, seq):\n",
    "        for i in range(len(seq) - 1):\n",
    "            yield (seq[i], seq[i + 1])\n",
    "\n",
    "    def _merge_once(self, seq, pair):\n",
    "        a, b = pair\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if i < len(seq) - 1 and seq[i] == a and seq[i + 1] == b:\n",
    "                out.append(a + b)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(seq[i])\n",
    "                i += 1\n",
    "        return out\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # corpus: list[str]\n",
    "        text = ''.join(corpus).lower()\n",
    "        seq = list(text)\n",
    "        merges = []\n",
    "        for _ in range(self.num_merges):\n",
    "            counts = Counter(self._get_bigrams(seq))\n",
    "            if not counts: break\n",
    "            best_pair, _ = counts.most_common(1)[0]\n",
    "            merges.append(best_pair)\n",
    "            seq = self._merge_once(seq, best_pair)\n",
    "        self.merges = merges\n",
    "        self.pair_ranks = {p: i for i, p in enumerate(self.merges)}\n",
    "\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "        for ch in sorted(set(text)):\n",
    "            self._add_token(ch)\n",
    "        for a, b in self.merges:\n",
    "            self._add_token(a + b)\n",
    "        self.eot_id = self._add_token(self.eot_token)\n",
    "\n",
    "    def encode(self, text, force_last_eot=True):\n",
    "        # treat literal eot marker as special; remove it from content\n",
    "        if self.eot_token in text:\n",
    "            text = text.replace(self.eot_token, '')\n",
    "        seq = list(text)\n",
    "\n",
    "        # make sure all seen base chars exist\n",
    "        for ch in set(seq):\n",
    "            if ch not in self.vocab:\n",
    "                self._add_token(ch)\n",
    "\n",
    "        # greedy BPE using learned pair ranks\n",
    "        if self.merges:\n",
    "            while True:\n",
    "                best_pair, best_rank = None, None\n",
    "                for p in self._get_bigrams(seq):\n",
    "                    r = self.pair_ranks.get(p)\n",
    "                    if r is not None and (best_rank is None or r < best_rank):\n",
    "                        best_pair, best_rank = p, r\n",
    "                if best_pair is None:\n",
    "                    break\n",
    "                seq = self._merge_once(seq, best_pair)\n",
    "\n",
    "        # ensure all tokens in seq exist in vocab (e.g., if new chars appeared)\n",
    "        for tok in seq:\n",
    "            if tok not in self.vocab:\n",
    "                self._add_token(tok)\n",
    "\n",
    "        ids = [self.vocab[tok] for tok in seq]\n",
    "\n",
    "        # FORCE: append EOT id if not already last\n",
    "        if force_last_eot:\n",
    "            if not ids or ids[-1] != self.eot_id:\n",
    "                ids.append(self.eot_id)\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # drop trailing EOT if present\n",
    "        if ids and self.eot_id is not None and ids[-1] == self.eot_id:\n",
    "            ids = ids[:-1]\n",
    "        toks = [self.id_to_token[i] for i in ids]\n",
    "        return ''.join(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a475495-4b2d-4484-ba7c-8c6fa7ecedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_example_1 = r'''Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as the application of linear algebra to function spaces.'''\n",
    "raw_example_2 = r'''In numerical analysis and linear algebra, lower–upper (LU) decomposition or factorization factors a matrix as the product of a lower triangular matrix and an upper triangular matrix (see matrix multiplication and matrix decomposition).'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6381f444-a72c-4093-9d91-9cec01e2c2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 'a'), ('a', 't'), ('i', 'n'), (' ', 'm'), ('i', 'o')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = SimpleBPETokenizer(num_merges=5)\n",
    "tok.train([raw_example_1,raw_example_2])\n",
    "tok.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ee9498d-67c1-486d-a7dc-5b1f704af69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '(': 1,\n",
       " ')': 2,\n",
       " ',': 3,\n",
       " '.': 4,\n",
       " 'a': 5,\n",
       " 'b': 6,\n",
       " 'c': 7,\n",
       " 'd': 8,\n",
       " 'e': 9,\n",
       " 'f': 10,\n",
       " 'g': 11,\n",
       " 'h': 12,\n",
       " 'i': 13,\n",
       " 'j': 14,\n",
       " 'l': 15,\n",
       " 'm': 16,\n",
       " 'n': 17,\n",
       " 'o': 18,\n",
       " 'p': 19,\n",
       " 'r': 20,\n",
       " 's': 21,\n",
       " 't': 22,\n",
       " 'u': 23,\n",
       " 'v': 24,\n",
       " 'w': 25,\n",
       " 'x': 26,\n",
       " 'y': 27,\n",
       " 'z': 28,\n",
       " '–': 29,\n",
       " ' a': 30,\n",
       " 'at': 31,\n",
       " 'in': 32,\n",
       " ' m': 33,\n",
       " 'io': 34,\n",
       " '<|endoftext|>': 35}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9de51b4-9f27-4f0b-ad46-98a2074b2363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tok.vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f1b9e7-4ec6-44fc-8e5d-6fab0fd0aff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0,  7,\n",
       "         9, 17, 22, 20,  5, 15,  0, 22, 18, 30, 15, 16, 18, 21, 22, 30, 15, 15,\n",
       "        30, 20,  9,  5, 21,  0, 18, 10, 33, 31, 12,  9, 16, 31, 13,  7, 21,  4,\n",
       "         0, 10, 18, 20,  0, 32, 21, 22,  5, 17,  7,  9,  3,  0, 15, 32,  9,  5,\n",
       "        20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0, 10, 23, 17,  8,  5, 16,\n",
       "         9, 17, 22,  5, 15,  0, 32, 33, 18,  8,  9, 20, 17,  0, 19, 20,  9, 21,\n",
       "         9, 17, 22, 31, 34, 17, 21,  0, 18, 10,  0, 11,  9, 18, 16,  9, 22, 20,\n",
       "        27,  3,  0, 32,  7, 15, 23,  8, 32, 11,  0, 10, 18, 20,  0,  8,  9, 10,\n",
       "        32, 32, 11,  0,  6,  5, 21, 13,  7,  0, 18,  6, 14,  9,  7, 22, 21,  0,\n",
       "        21, 23,  7, 12, 30, 21,  0, 15, 32,  9, 21,  3,  0, 19, 15,  5, 17,  9,\n",
       "        21, 30, 17,  8,  0, 20, 18, 22, 31, 34, 17, 21,  4, 30, 15, 21, 18,  3,\n",
       "         0, 10, 23, 17,  7, 22, 34, 17,  5, 15, 30, 17,  5, 15, 27, 21, 13, 21,\n",
       "         3, 30,  0,  6, 20,  5, 17,  7, 12,  0, 18, 10, 33, 31, 12,  9, 16, 31,\n",
       "        13,  7,  5, 15, 30, 17,  5, 15, 27, 21, 13, 21,  3, 33,  5, 27,  0,  6,\n",
       "         9,  0, 24, 13,  9, 25,  9,  8, 30, 21,  0, 22, 12,  9, 30, 19, 19, 15,\n",
       "        13,  7, 31, 34, 17,  0, 18, 10,  0, 15, 32,  9,  5, 20, 30, 15, 11,  9,\n",
       "         6, 20,  5,  0, 22, 18,  0, 10, 23, 17,  7, 22, 34, 17,  0, 21, 19,  5,\n",
       "         7,  9, 21,  4, 35, 35, 32,  0, 17, 23, 16,  9, 20, 13,  7,  5, 15, 30,\n",
       "        17,  5, 15, 27, 21, 13, 21, 30, 17,  8,  0, 15, 32,  9,  5, 20, 30, 15,\n",
       "        11,  9,  6, 20,  5,  3,  0, 15, 18, 25,  9, 20, 29, 23, 19, 19,  9, 20,\n",
       "         0,  1, 15, 23,  2,  0,  8,  9,  7, 18, 16, 19, 18, 21, 13, 22, 34, 17,\n",
       "         0, 18, 20,  0, 10,  5,  7, 22, 18, 20, 13, 28, 31, 34, 17,  0, 10,  5,\n",
       "         7, 22, 18, 20, 21, 30, 33, 31, 20, 13, 26, 30, 21,  0, 22, 12,  9,  0,\n",
       "        19, 20, 18,  8, 23,  7, 22,  0, 18, 10, 30,  0, 15, 18, 25,  9, 20,  0,\n",
       "        22, 20, 13,  5, 17, 11, 23, 15,  5, 20, 33, 31, 20, 13, 26, 30, 17,  8,\n",
       "        30, 17,  0, 23, 19, 19,  9, 20,  0, 22, 20, 13,  5, 17, 11, 23, 15,  5,\n",
       "        20, 33, 31, 20, 13, 26,  0,  1, 21,  9,  9, 33, 31, 20, 13, 26, 33, 23,\n",
       "        15, 22, 13, 19, 15, 13,  7, 31, 34, 17, 30, 17,  8, 33, 31, 20, 13, 26,\n",
       "         0,  8,  9,  7, 18, 16, 19, 18, 21, 13, 22, 34, 17,  2,  4, 35])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eot = tok.eot_id\n",
    "tokens = []\n",
    "for example in [raw_example_1, raw_example_2]:\n",
    "    tokens.extend([eot])\n",
    "    tokens.extend(tok.encode(example.lower()))\n",
    "all_tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d84e5c6-b409-40d7-a614-ee383b6066a4",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a853b63d-29e5-4786-bf59-6aba024a8e5b",
   "metadata": {},
   "source": [
    "A machine learning model forward pass now uses the tokenization information, runs several layers of linear algebra on it, and then \"predicts\" the next token. When it is noisy (like you will see in this example), this process results in gibberish.  The training process changes the noise to pattern during the \"backward pass\" as you'll see.    We'll show 3 steps that are focused on training:\n",
    "1. **Data Loading** `x, y = train_loader.next_batch()` - this step pulls from the raw data enough tokens to complete a forward and backward pass.  If the model is inference only, this step is replaced with taking in the inference input and preparing it similarly as the forward pass.\n",
    "2. **Forward Pass** `logits, loss = model(x, y)` - using the data and the model architecture to predict the next token. When training we also compare against the expected to get loss, but in infrerence, we use the logits to complete the inference task.\n",
    "3. **Backward Pass & Training** `loss.backward(); optimizer.step()` - using differentials to understand what parameters most impact the forward pass' impact on its prediction, comparing that against what is actually right based on the data loading step, and then making very minor adjustments to the impactful parameters with the hope it improves future predictions.\n",
    "\n",
    "The we'll show a final **Forward Pass** with the updated weights we did in #3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7182b5-5872-483b-93d8-a8fa61d5a8fe",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70511c70-5b74-407d-b439-0f6a24c12f8e",
   "metadata": {},
   "source": [
    "To start, we need to get enough data to run the forward and backward passes.  Since our total dataset is likely too big to hold all at once in real practice, we would read just enough file information into memory so that we can run the passes, leaving memory and compute to be used on the passes instead of static data holding. \n",
    "To start, we have to identify the batch size and the model context length to determine how much data we need.  Consequently, these dimensions also form 2 of the 3 dimensions in the initial matrix.\n",
    "- **Batch Size (B)** - This is the number of examples you'll train on in a single pass. \n",
    "- **Context Length (T)** - This is the max number of tokens that a model can use in a single pass to generat the next token. If an example is below this length, it can be padded.\n",
    "  \n",
    "*Ideally both B and T are multiples of 2 to work nicely with chip architecture. This is a common theme across the board*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b87eb7f-f648-459f-8770-05b41acf0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2 # Batch\n",
    "T = 8 # context length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f307d03f-e9b7-44c5-82ea-e25236612a0d",
   "metadata": {},
   "source": [
    "To start, we need to pull from our long raw_token list enough tokens for the forward pass. To be able to satisfy training `B` Batches `T` Context length, we need to pull out `B*T` tokens to slide the context window across the examples enough to satisfy the batch size.  Since the training will attempt to predict the last token given the previous tokens in context, we also need 1 more token at the end so that the last training example in the last batch can have the next token to validate against. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e06beb6-747c-4565-b730-a539bfd8bb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_position = 0\n",
    "tok_for_training = all_tokens[current_position:current_position + B*T +1 ]\n",
    "tok_for_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bfbb68-1575-4bba-95ee-9fe81f2d7eff",
   "metadata": {},
   "source": [
    "Now that we have our initial tokens to train on, we now need to convert it to a matrix that's ready for training. In this step we'll need to create our batches and setup two different arrays: 1/ the input, `x`, tokens that will result in 2/ the output `y` tokens. To create each example in the batch, every `T` tokens will be placed into it's own row. \n",
    "\n",
    "Recall that training takes in a string of tokens the length of the context and then predicts the next token. Recall that when we extracted `tok_for_training` we added 1 extra token so that we can evaluate the prediction for the last example. Because of this, the input, `x`, will be all of the tokens up to the second to last element `[:-1]`.  \n",
    "\n",
    "It might be natural to think the output `y` would then just be the last token.But this is actually wasting valuable training loops.  Yes, there is the example that fills the context `T`, but we also have enough tokens in `tok_for_training` where any context length of `n` where `n<T` can also be used for inference since we have the `n+1` token available.  You can think of the following example:\n",
    "\n",
    "sentence: `Hi I am learning`. This sentence contains the following \"next tokens\" that can be learned:\n",
    "1. x: Hi I am  | y: learning\n",
    "2. x: Hi I     | y: am\n",
    "3. x: Hi       | y: I\n",
    "\n",
    "Because we have this triangle to create, our `y` can be much larger.  We can start with the second token and, go all the way to the last element we added for the last example `[1:'`.   \n",
    "\n",
    "\n",
    "We will now put this together and do the following:\n",
    "1. Extract the input `x` and then split it into an example for each batch `B`\n",
    "2. Extract the output `y` and then split it into an example for each batch `B`\n",
    "\n",
    "*Note: View can take `-1` which allows the matrix to infer the dimension so we do not need to pass in `T`, but given how many matrices we'll work with we want to make sure we're controlling the dimensions or erroring out if they do not match our expectations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87de4a84-1b29-4ecb-9a9a-ad4938b73eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69e73535-556a-4282-861d-5e1edc86b319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[35, 15, 32,  9,  5, 20, 30, 15],\n",
       "        [11,  9,  6, 20,  5,  0, 13, 21]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tok_for_training[:-1].view(B, T)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f11b91e-26c6-4de9-95ef-576c4d54a162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15, 32,  9,  5, 20, 30, 15, 11],\n",
       "        [ 9,  6, 20,  5,  0, 13, 21,  0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=tok_for_training[1:].view(B, T)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d327003-6a9b-4f21-a9b3-76a0fbb2dd36",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef751eab-02d1-43eb-acd5-56357e705598",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/full_network.png\" width=\"200\">\n",
    "\n",
    "The forward pass takes a string of tokens in and predicts the next \"n\" tokens.  This step as we'll look at it is focused on training where we'll pass in the input `x`, carry that input through the layers, and generate a matrix of the probability of each token being the next one, something we call `logits`. During the forward pass at the end we then compare the probability to the actual next token in `y` and calculate `loss` based on the difference. \n",
    "\n",
    "*Note that we will do some layer initialization to simplify following along.  In reality layers are often initialized to normal distribution with some adjustments made for parameter sizes to keep the weights properly noisy.  We will not cover initialization in this series*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a77a8a-f93c-48b9-b715-d3196f402c28",
   "metadata": {},
   "source": [
    "We first rederive the batch size and context size based on the input to improve flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16252536-9637-4b9e-9a73-ed2a8ee2aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7291c75e-d8e0-49d0-9b40-8b6b5a22cfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T = x.size()\n",
    "B,T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40579b-7cd6-46cb-8ec7-6626390395bd",
   "metadata": {},
   "source": [
    "The first layer of our network creates an embedding representation of our input sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea845430-56e4-40bc-b3bf-84331257007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 # max sequence length/context\n",
    "vocab_size = vocab_size # 36 \n",
    "n_embd = 4 # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca4ad4b-c92d-4596-b255-e0e4042128f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Input Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5de68adb-24b4-4f23-b7c9-ff8f0a5ff6ca",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/input_layer.png\" width=\"200\">\n",
    "We'll first create an initiation for 2 of our input matrices: position and token embedding.  Both of these are a table of weigths that have `n_embd` number of columns to store information about the position or token. The more columns you add, the more complex information can be stored but the more compute is needed.  For now we'll let each position or column store up to 4 channels of information.  Before starting though we need to initialize the layer with a set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f37c4136-cd53-4534-84e6-0cdf509551b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0100, 0.0200, 0.0300, 0.0400],\n",
       "         [0.0200, 0.0300, 0.0400, 0.0500],\n",
       "         [0.0300, 0.0400, 0.0500, 0.0600],\n",
       "         [0.0400, 0.0500, 0.0600, 0.0700],\n",
       "         [0.0500, 0.0600, 0.0700, 0.0800],\n",
       "         [0.0600, 0.0700, 0.0800, 0.0900],\n",
       "         [0.0700, 0.0800, 0.0900, 0.1000],\n",
       "         [0.0800, 0.0900, 0.1000, 0.1100],\n",
       "         [0.0900, 0.1000, 0.1100, 0.1200],\n",
       "         [0.1000, 0.1100, 0.1200, 0.1300],\n",
       "         [0.1100, 0.1200, 0.1300, 0.1400],\n",
       "         [0.1200, 0.1300, 0.1400, 0.1500],\n",
       "         [0.1300, 0.1400, 0.1500, 0.1600],\n",
       "         [0.1400, 0.1500, 0.1600, 0.1700],\n",
       "         [0.1500, 0.1600, 0.1700, 0.1800],\n",
       "         [0.1600, 0.1700, 0.1800, 0.1900],\n",
       "         [0.1700, 0.1800, 0.1900, 0.2000],\n",
       "         [0.1800, 0.1900, 0.2000, 0.2100],\n",
       "         [0.1900, 0.2000, 0.2100, 0.2200],\n",
       "         [0.2000, 0.2100, 0.2200, 0.2300],\n",
       "         [0.2100, 0.2200, 0.2300, 0.2400],\n",
       "         [0.2200, 0.2300, 0.2400, 0.2500],\n",
       "         [0.2300, 0.2400, 0.2500, 0.2600],\n",
       "         [0.2400, 0.2500, 0.2600, 0.2700],\n",
       "         [0.2500, 0.2600, 0.2700, 0.2800],\n",
       "         [0.2600, 0.2700, 0.2800, 0.2900],\n",
       "         [0.2700, 0.2800, 0.2900, 0.3000],\n",
       "         [0.2800, 0.2900, 0.3000, 0.3100],\n",
       "         [0.2900, 0.3000, 0.3100, 0.3200],\n",
       "         [0.3000, 0.3100, 0.3200, 0.3300],\n",
       "         [0.3100, 0.3200, 0.3300, 0.3400],\n",
       "         [0.3200, 0.3300, 0.3400, 0.3500],\n",
       "         [0.3300, 0.3400, 0.3500, 0.3600],\n",
       "         [0.3400, 0.3500, 0.3600, 0.3700],\n",
       "         [0.3500, 0.3600, 0.3700, 0.3800],\n",
       "         [0.3600, 0.3700, 0.3800, 0.3900]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0.0100, 0.0200, 0.0300, 0.0400],\n",
       "         [0.0200, 0.0300, 0.0400, 0.0500],\n",
       "         [0.0300, 0.0400, 0.0500, 0.0600],\n",
       "         [0.0400, 0.0500, 0.0600, 0.0700],\n",
       "         [0.0500, 0.0600, 0.0700, 0.0800],\n",
       "         [0.0600, 0.0700, 0.0800, 0.0900],\n",
       "         [0.0700, 0.0800, 0.0900, 0.1000],\n",
       "         [0.0800, 0.0900, 0.1000, 0.1100]], requires_grad=True))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted token embedding\n",
    "wte = nn.Embedding(vocab_size, n_embd)\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = wte.num_embeddings, wte.embedding_dim\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.01*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    wte.weight.copy_(pattern)\n",
    "# weighted position embedding\n",
    "wpe = nn.Embedding(block_size, n_embd)\n",
    "with torch.no_grad(): # initilize to W[i,j] = 0.001*(1+i+j) for easy following \n",
    "    vs, d = wpe.num_embeddings, wpe.embedding_dim\n",
    "    rows = torch.arange(vs).unsqueeze(1)  # (vs,1)\n",
    "    cols = torch.arange(d).unsqueeze(0)  # (1,d)\n",
    "    pattern = 0.01*(1 + rows + cols)  # W[i,j] = 0.001*(1+i+j)\n",
    "    wpe.weight.copy_(pattern)\n",
    "wte.weight, wpe.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d28ccc-7516-4d85-ad4f-91cfd6eb20c2",
   "metadata": {},
   "source": [
    "**Positional Embeddings** - Now we need to pluck the weight of each position out of the position embedding.  Since we are creating a simple left to right, position 1 to n, we can just create an array from 0 to n based on the context, `T`, then pluck those rows out.  The resulting matrix from this operation is a `T, n_embd` based vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bcf9346-3b25-478a-9400-acef841d54a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = torch.arange(0, T, dtype=torch.long)\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a54e71-fd3b-46b1-9134-b2e597e01088",
   "metadata": {},
   "source": [
    "for each element, look up the row in `wpe` and pluck it out. Since the position is just `[0:T]` we can see we pluck out the position array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31ea57c2-0f34-446e-8e91-643db51b02af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 4]),\n",
       " tensor([[0.0100, 0.0200, 0.0300, 0.0400],\n",
       "         [0.0200, 0.0300, 0.0400, 0.0500],\n",
       "         [0.0300, 0.0400, 0.0500, 0.0600],\n",
       "         [0.0400, 0.0500, 0.0600, 0.0700],\n",
       "         [0.0500, 0.0600, 0.0700, 0.0800],\n",
       "         [0.0600, 0.0700, 0.0800, 0.0900],\n",
       "         [0.0700, 0.0800, 0.0900, 0.1000],\n",
       "         [0.0800, 0.0900, 0.1000, 0.1100]], grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb = wpe(pos)\n",
    "pos_emb.shape, pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89e0ad-87e5-4bbe-bccb-998549487425",
   "metadata": {},
   "source": [
    "**Word Embeddings** - Similarly we need to pluck out the rows from the token table, `wte` for the tokens in our example. Since our example is already represented as indices, we can simple use `x` directly. The resulting matrix from this operation is a `B,T, n_embd` based vector since `x` is `B,T` and `tok_emb` is `vocab_size,n_embd` and when we index `wte` by `x` each entry in x replaces `n_embd` based vector at that position in `wte`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bd724b1-0669-4b21-91d1-450b8231d6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[35, 15, 32,  9,  5, 20, 30, 15],\n",
       "        [11,  9,  6, 20,  5,  0, 13, 21]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e99ae5-4f5b-4939-bb8e-c754f0c5af77",
   "metadata": {},
   "source": [
    "for each position pull out the the row of weights that corresponds to the token. You can see in the print out that the rows are not in the same order as the layer is initiliazed as the token ids are not sequential.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecfbcd6e-eacb-415c-aabf-06b3bf51a378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[0.3600, 0.3700, 0.3800, 0.3900],\n",
       "          [0.1600, 0.1700, 0.1800, 0.1900],\n",
       "          [0.3300, 0.3400, 0.3500, 0.3600],\n",
       "          [0.1000, 0.1100, 0.1200, 0.1300],\n",
       "          [0.0600, 0.0700, 0.0800, 0.0900],\n",
       "          [0.2100, 0.2200, 0.2300, 0.2400],\n",
       "          [0.3100, 0.3200, 0.3300, 0.3400],\n",
       "          [0.1600, 0.1700, 0.1800, 0.1900]],\n",
       " \n",
       "         [[0.1200, 0.1300, 0.1400, 0.1500],\n",
       "          [0.1000, 0.1100, 0.1200, 0.1300],\n",
       "          [0.0700, 0.0800, 0.0900, 0.1000],\n",
       "          [0.2100, 0.2200, 0.2300, 0.2400],\n",
       "          [0.0600, 0.0700, 0.0800, 0.0900],\n",
       "          [0.0100, 0.0200, 0.0300, 0.0400],\n",
       "          [0.1400, 0.1500, 0.1600, 0.1700],\n",
       "          [0.2200, 0.2300, 0.2400, 0.2500]]], grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_emb = wte(x)\n",
    "tok_emb.shape, tok_emb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "099e55eb-8fb1-438a-a2ae-c05a83824b25",
   "metadata": {},
   "source": [
    "**Impact of the position and token together**\n",
    "To ensure that the position and token together impact the next token prediction, we sum the two so that the weight of each token is impacted by the weight of its relative position. To do this we sum `tok_emb` and `pos_emb` together. Quickly we can see the dimensions don't match as \n",
    "* `tok_emb` > `B,T,n_embd`\n",
    "* `pos_emb` >   `T,n_embd`\n",
    "\n",
    "Since we have multiple examples with the same ordering, we simply add pos_emb at the same level to each entry on the `B` dimension, something that pytorch does for us automatically resulting in a `B,T,n_embd` output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2aab5c4-b879-4d33-a9b3-25ee108e2559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3700, 0.3900, 0.4100, 0.4300],\n",
       "         [0.1800, 0.2000, 0.2200, 0.2400],\n",
       "         [0.3600, 0.3800, 0.4000, 0.4200],\n",
       "         [0.1400, 0.1600, 0.1800, 0.2000],\n",
       "         [0.1100, 0.1300, 0.1500, 0.1700],\n",
       "         [0.2700, 0.2900, 0.3100, 0.3300],\n",
       "         [0.3800, 0.4000, 0.4200, 0.4400],\n",
       "         [0.2400, 0.2600, 0.2800, 0.3000]],\n",
       "\n",
       "        [[0.1300, 0.1500, 0.1700, 0.1900],\n",
       "         [0.1200, 0.1400, 0.1600, 0.1800],\n",
       "         [0.1000, 0.1200, 0.1400, 0.1600],\n",
       "         [0.2500, 0.2700, 0.2900, 0.3100],\n",
       "         [0.1100, 0.1300, 0.1500, 0.1700],\n",
       "         [0.0700, 0.0900, 0.1100, 0.1300],\n",
       "         [0.2100, 0.2300, 0.2500, 0.2700],\n",
       "         [0.3000, 0.3200, 0.3400, 0.3600]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tok_emb + pos_emb\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70bf96d-a67f-43ce-9a19-9975e4198609",
   "metadata": {},
   "source": [
    "### Transformer Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34da8230-ef7f-4e68-b7a1-b0b73219c024",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/transformer.png\" width=\"200\">\n",
    "\n",
    "The transformer block is multiple parallel repetitions of the same matrix operations done independently.  This adds both depth and breadth to the computation.  Each block is the same steps of\n",
    "1. Layer normalization\n",
    "2. Causal self attention\n",
    "3. Layer normalization (again)\n",
    "4. Multi-layer perceptron (MLP)\n",
    "\n",
    "Both steps 2 and 4 are also multi-layered so we'll go through each layer independently.  You'll notice the arrows in the diagram bypassing the causal self attention and the MLP.  This is to ensure that the weights of any one layer do not get overweighted. We achieve this by simply adding the input with the layer's calculations together, as you'll see. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bc98ad-7f59-4b15-961c-233853e01c73",
   "metadata": {},
   "source": [
    "#### Transformer - Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0325d06-5ee3-43bb-8998-405079dfbecf",
   "metadata": {},
   "source": [
    "With Layer normalization, we review the row and adjust based on how far away it is from the mean. This means an array of `[1,2,3,4]` and `[2,4,6,8]` will actually have the same normalized entries after layer normalization.  This layer adds regularization which helps with overall learning speed. The formula applied is:\n",
    "\n",
    "$y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\operatorname{Var}[x] + \\epsilon}}$\n",
    "\n",
    "Layer normalization is applied on the input matrix in kind and creates default weights of 1's across the dimension to equally weight all values in the normalization. We'll keep this as is and not change the initialization.  \n",
    "\n",
    "*Note that even though we will do layer normalization again, we keep this as a separate layer so that its impact can be adjusted independent of other normalization layers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c75580-ff67-45e7-9621-2bc0b3f467cb",
   "metadata": {},
   "source": [
    "**Example** Let's see a quick example of how layer normalizaiton operates with an array of `[1,2,3,4]` and `[2,4,6,8]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fed78841-c30e-48e1-af87-fa07f0bf7402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3416, -0.4472,  0.4472,  1.3416],\n",
       "        [-1.3416, -0.4472,  0.4472,  1.3416]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_ln = nn.LayerNorm(n_embd)\n",
    "\n",
    "## Example \n",
    "example_ln(torch.tensor(\n",
    "    [\n",
    "        [1.0,2.0,3.0,4.0],\n",
    "        [2.0,4.0,6.0,8.0]\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd54631c-b366-4720-b717-ba22c3c6d043",
   "metadata": {},
   "source": [
    "**Normalize** Now let's apply it to x.  We'll save the output to a new variable `x_norm` so that we retain the input `x` for the *skip connection* (more details below).  Since our position embeddings are still in their initiaion step where each entries values are similarly distributed `[n,n+2,n+4,n+6]`  we'll see that with layer normalization the resulting matrix has all equal rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9afb21a5-1b1b-478f-be38-9cc2b6fba79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284]],\n",
       " \n",
       "         [[-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_1 = nn.LayerNorm(n_embd)\n",
    "x_norm = ln_1(x)\n",
    "x_norm.shape, x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b6a7a-0978-466d-8d90-3e80abb78754",
   "metadata": {},
   "source": [
    "#### Transformer - Flash Attnetion / Causal Self Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe140d19-25b1-4047-9536-ceb3bb1a8a34",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/flash_attention.png\" width=\"600\">\n",
    "\n",
    "Causal self-attention is self-attention with a strictly lower-triangular mask, so each position $i$ can attend only to positions $j≤i$ (no look-ahead). For each position in a sequence, the model decides which earlier tokens matter and blends their information to generate the upcoming token, but the model is not allowed to peek at tokens to the right. Recall that we created an input `x` that has all of the tokens in the sequence. With causal self attention we ensure that the prediction of `y` is not learned in an example from tokens to the right of it. \n",
    "\n",
    "Causal self attention uses 3 linear projections to create representations of 3 concepts to help derive the next token: query, key, and value. FlashAttention takes the input `x` of shape $(B,T,C)$, linearly projects the last dimension to `Q, K, V` by multiplying each token vector $x_{b,t,:}$ with learned weight matrices `Q, K,V` (often via one combined QKV linear) and adding biases (which we do not have. Then we compute the causal dot-product attention with softmax per head, concatenate the heads, and apply a final linear layer to produce the output.  Linear layers use the projection of $y=xA^\\top+b$\n",
    "\n",
    "The Q,K,V matrix do the following: \n",
    "* **Query** - the current position’s “search request.” Every layer and head issues queries that can look for many things (antecedent, rhythm, agreement, etc.).\n",
    "* **Key** - a matching tag/address for each allowed position. It is compared with the query to produce relevance scores\n",
    "* **Value** - the payload you actually mix in once something matches. It’s a learned projection of the token’s representation so the model can copy the right kind of information.\n",
    "\n",
    "This layer starts by taking a linear layer that has `n_embd` rows and then  `n_embd` columns for each: the query, key and value.  This allows the projection of the `x_norm` to be split into the 3 components, selected from them. We then use `scaled_dot_product_attention` to provide the attention masking and normalization, including a lower triangle mask to prevent look-ahead, and final project that onto a `n_embd x n_embd` matrix to return the output. \n",
    "\n",
    "We start by creating the 2 default layers. Also we'll set heads to 2.  The heads allow the layer to specialize in different concepts since each head creates its own Q,K,V.\n",
    "\n",
    "*for the linear layers we'll do some special initiations, mainly pyramidal so that we can see unique numbers. We also initialize the bias to 0 so there linear layer is now just $y=xA^\\top$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb3516ed-26ec-4cfc-abb3-c8a9e15ec996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "n_head = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4d6eeae-8522-4c8a-834f-e8285f6f14b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=4, out_features=12, bias=True),\n",
       " Parameter containing:\n",
       " tensor([[0.0010, 0.0020, 0.0030, 0.0040],\n",
       "         [0.0020, 0.0050, 0.0070, 0.0090],\n",
       "         [0.0030, 0.0090, 0.0120, 0.0150],\n",
       "         [0.0040, 0.0140, 0.0180, 0.0220],\n",
       "         [0.0050, 0.0200, 0.0250, 0.0300],\n",
       "         [0.0060, 0.0270, 0.0330, 0.0390],\n",
       "         [0.0070, 0.0350, 0.0420, 0.0490],\n",
       "         [0.0080, 0.0440, 0.0520, 0.0600],\n",
       "         [0.0090, 0.0540, 0.0630, 0.0720],\n",
       "         [0.0100, 0.0650, 0.0750, 0.0850],\n",
       "         [0.0110, 0.0770, 0.0880, 0.0990],\n",
       "         [0.0120, 0.0900, 0.1020, 0.1140]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "with torch.no_grad():\n",
    "    out, inp = c_attn.weight.shape  # (3*n_embd, n_embd)\n",
    "    r = torch.arange(1, out + 1).unsqueeze(1)  # [out,1], 1-indexed\n",
    "    c = torch.arange(1, inp + 1).unsqueeze(0)   # [1,inp], 1-indexed\n",
    "    base = r * c                          # rc\n",
    "    tri = r * (r - 1) / 2                 # T_{r-1} = r(r-1)/2, shape [out,1]\n",
    "    mask = (c >= 2)           # add T_{r-1} only from column 2 onward\n",
    "    pattern = 1e-3 * (base + tri * mask) # matches [[.001,.002,.003],[.002,.005,.007],[.003,.009,.012],...]\n",
    "    c_attn.weight.copy_(pattern)\n",
    "\n",
    "    c_attn.bias.zero_()\n",
    "    \n",
    "c_attn, c_attn.weight, c_attn.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e26b1cf6-4943-499a-b86b-2717462d8426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=4, out_features=4, bias=True),\n",
       " Parameter containing:\n",
       " tensor([[0.0010, 0.0020, 0.0030, 0.0040],\n",
       "         [0.0020, 0.0050, 0.0070, 0.0090],\n",
       "         [0.0030, 0.0090, 0.0120, 0.0150],\n",
       "         [0.0040, 0.0140, 0.0180, 0.0220]], requires_grad=True))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_proj = nn.Linear(n_embd, n_embd)\n",
    "with torch.no_grad():\n",
    "    out, inp = c_proj.weight.shape  # (3*n_embd, n_embd)\n",
    "    r = torch.arange(1, out + 1).unsqueeze(1)  # [out,1], 1-indexed\n",
    "    c = torch.arange(1, inp + 1).unsqueeze(0)   # [1,inp], 1-indexed\n",
    "    base = r * c                          # rc\n",
    "    tri = r * (r - 1) / 2                 # T_{r-1} = r(r-1)/2, shape [out,1]\n",
    "    mask = (c >= 2)           # add T_{r-1} only from column 2 onward\n",
    "    pattern = 1e-3 * (base + tri * mask) # matches [[.001,.002,.003],[.002,.005,.007],[.003,.009,.012],...]\n",
    "    c_proj.weight.copy_(pattern)\n",
    "    \n",
    "    c_proj.bias.zero_()\n",
    "c_proj, c_proj.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1543e367-62f7-4edc-be6b-877bec24e0aa",
   "metadata": {},
   "source": [
    "**Flash Attention - Creating Query, Key, Value**\n",
    "\n",
    "We'll now create the query, key, and value matrices. Let's also revisualize `x_norm` to make it easy to connect the dots in this complex layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80f9502f-a4eb-4465-87c7-4408e0653f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = x_norm.size()\n",
    "B, T, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56d6ff50-cc2a-4a43-8170-d79a1dea6ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284]],\n",
       " \n",
       "         [[-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284],\n",
       "          [-1.3284, -0.4428,  0.4428,  1.3284]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm.shape, x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24937337-9034-4106-8cee-350ef4dd6a6f",
   "metadata": {},
   "source": [
    "Now we'll take `x_norm` and the dot product of the weights in `c_attn` $y=x\\_norm \\cdot c\\_attn^\\top+bias$. This results in the the `qkv` combined matrix leading to a 3x size increase. Since every entry on the 3rd dimension in `x_norm` is the same, we will see that every row has the same repeated values given that's how dot products work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "766c6957-116e-404d-9e67-dde777e6de88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 12]),\n",
       " tensor([[[0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408]],\n",
       " \n",
       "         [[0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0044, 0.0102, 0.0173, 0.0257, 0.0354, 0.0465, 0.0589, 0.0726,\n",
       "           0.0877, 0.1041, 0.1218, 0.1408]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = c_attn(x_norm)\n",
    "qkv.shape, qkv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b126c-0c5b-485d-8b7b-cc4b0734de1d",
   "metadata": {},
   "source": [
    "**Flash Attention - Attention Head**\n",
    "\n",
    "Now, we will split up qkv to create the 3 separate matrices, one for the query, key, and value. Together they to work to create complex concept embeddings. We also then have to split up each matrix into its own heads (shown as columns `dim=3`). For each head we create its own Q,K,V, compute the causal dot-product attention with softmax per head and then eventually concatenate the heads. During back-propogation, different heads can get updates in different ways based on the examples and gradients.  This backprop and separation is what allows each attention head to specialize in different concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "720501ed-17a0-4c87-a9f6-9d800b339198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('q',\n",
       " torch.Size([2, 2, 8, 2]),\n",
       " tensor([[[[0.0044, 0.0102],\n",
       "           [0.0044, 0.0102],\n",
       "           [0.0044, 0.0102],\n",
       "           [0.0044, 0.0102],\n",
       "           [0.0044, 0.0102],\n",
       "           [0.0044, 0.0102],\n",
       "           [0.0044, 0.0102],\n",
       "           [0.0044, 0.0102]],\n",
       " \n",
       "          [[0.0173, 0.0257],\n",
       "           [0.0173, 0.0257],\n",
       "           [0.0173, 0.0257],\n",
       "           [0.0173, 0.0257],\n",
       "           [0.0173, 0.0257],\n",
       "           [0.0173, 0.0257],\n",
       "           [0.0173, 0.0257],\n",
       "           [0.0173, 0.0257]]],\n",
       " \n",
       " \n",
       "         [[[0.0044, 0.0102],\n",
       "           [0.0044, 0.0102],\n",
       "           [0.0044, 0.0102],\n",
       "           [0.0044, 0.0102],\n",
       "           [0.0044, 0.0102],\n",
       "           [0.0044, 0.0102],\n",
       "           [0.0044, 0.0102],\n",
       "           [0.0044, 0.0102]],\n",
       " \n",
       "          [[0.0173, 0.0257],\n",
       "           [0.0173, 0.0257],\n",
       "           [0.0173, 0.0257],\n",
       "           [0.0173, 0.0257],\n",
       "           [0.0173, 0.0257],\n",
       "           [0.0173, 0.0257],\n",
       "           [0.0173, 0.0257],\n",
       "           [0.0173, 0.0257]]]], grad_fn=<TransposeBackward0>),\n",
       " 'k',\n",
       " torch.Size([2, 2, 8, 2]),\n",
       " tensor([[[[0.0354, 0.0465],\n",
       "           [0.0354, 0.0465],\n",
       "           [0.0354, 0.0465],\n",
       "           [0.0354, 0.0465],\n",
       "           [0.0354, 0.0465],\n",
       "           [0.0354, 0.0465],\n",
       "           [0.0354, 0.0465],\n",
       "           [0.0354, 0.0465]],\n",
       " \n",
       "          [[0.0589, 0.0726],\n",
       "           [0.0589, 0.0726],\n",
       "           [0.0589, 0.0726],\n",
       "           [0.0589, 0.0726],\n",
       "           [0.0589, 0.0726],\n",
       "           [0.0589, 0.0726],\n",
       "           [0.0589, 0.0726],\n",
       "           [0.0589, 0.0726]]],\n",
       " \n",
       " \n",
       "         [[[0.0354, 0.0465],\n",
       "           [0.0354, 0.0465],\n",
       "           [0.0354, 0.0465],\n",
       "           [0.0354, 0.0465],\n",
       "           [0.0354, 0.0465],\n",
       "           [0.0354, 0.0465],\n",
       "           [0.0354, 0.0465],\n",
       "           [0.0354, 0.0465]],\n",
       " \n",
       "          [[0.0589, 0.0726],\n",
       "           [0.0589, 0.0726],\n",
       "           [0.0589, 0.0726],\n",
       "           [0.0589, 0.0726],\n",
       "           [0.0589, 0.0726],\n",
       "           [0.0589, 0.0726],\n",
       "           [0.0589, 0.0726],\n",
       "           [0.0589, 0.0726]]]], grad_fn=<TransposeBackward0>),\n",
       " 'v',\n",
       " torch.Size([2, 2, 8, 2]),\n",
       " tensor([[[[0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041]],\n",
       " \n",
       "          [[0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408]]],\n",
       " \n",
       " \n",
       "         [[[0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041]],\n",
       " \n",
       "          [[0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408]]]], grad_fn=<TransposeBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q,k,v = qkv.split(n_embd, dim=2)\n",
    "q = q.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "k = k.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "v = v.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "'q', q.shape, q, 'k', k.shape, k, 'v', v.shape, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385cd815-7936-496e-b980-d91b49727569",
   "metadata": {},
   "source": [
    "**Flash Attention - Cross Attention / Dot-product attention**\n",
    "\n",
    "With the separated query, key, value, we now need to calculate the dot product, but use a lower triangle to restrict each position to only be able to use embeddings for tokens that precede it.  To do this, flash attention tiles Q,K,V and apply masking and softmax on the fly. If done without flash attention, you'd need to materialize the large `(T,T)` matrix for all the queries and keys, then apply a lower triangle mask fill to zero out the upper triangle to prevent \"look ahead\", compute the dot product and finally apply a softmax.  After running this calculation per head we collapse the heads together back into the same dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db05b18b-3710-4649-ac29-1eca387a1322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041]],\n",
       " \n",
       "          [[0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408]]],\n",
       " \n",
       " \n",
       "         [[[0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041],\n",
       "           [0.0877, 0.1041]],\n",
       " \n",
       "          [[0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408],\n",
       "           [0.1218, 0.1408]]]],\n",
       "        grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>),\n",
       " torch.Size([2, 2, 8, 2]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "fa, fa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305e2fe-00e2-4631-a937-978b9d60957c",
   "metadata": {},
   "source": [
    "**Flash Attention - collapse heads**  Now we'll reshape back to remove the heads\n",
    "\n",
    "*Note we use `contiguous` here to force `transpose()` to create a new matrix in memory. This allows the heads to learn independently*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1331243-d917-45ef-9f1c-10f5bb24fe33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408]],\n",
       " \n",
       "         [[0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408],\n",
       "          [0.0877, 0.1041, 0.1218, 0.1408]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa = fa.transpose(1, 2).contiguous().view(B, T, C)\n",
    "fa.shape, fa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023e209-4e96-46c2-9f24-b0c6117fe650",
   "metadata": {},
   "source": [
    "**Flash Attention - final projection and output**\n",
    "Finally, we will now project the cross attention matrix on another final linear layer `n_embd x n_embd` that we initialized as `c_proj`.  Once agian, because our rows are identical and we use `c_attn` $y=x\\_norm \\cdot c\\_proj^\\top$ we will result in the same values. Note the removal of bias since we set it to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a36fb4f0-481b-4638-b376-2ca6bf0d5e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071]],\n",
       " \n",
       "         [[0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071],\n",
       "          [0.0012, 0.0028, 0.0048, 0.0071]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm = c_proj(fa)\n",
    "x_norm.shape, x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ece0b-3a38-44d2-9fcc-cc9a01ea9eaa",
   "metadata": {},
   "source": [
    "#### Transformer - Residual (skip) connection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24d8108a-616b-4d56-a3a0-c333430b924c",
   "metadata": {},
   "source": [
    "<img src=\"explainer_screenshots/skip_layer.png\" width=\"200\">\n",
    "Modern networks also use skip connections, meaning they allow for pathways to bypass around \"boxes\", passing through gradients during the backward pass.  This attribute ensures that the impact of each layer and head is normalized against the input embeddings themselves. Recall in the diagram we had the arrow that bypassed \"masked multiheaded attention\".  Functionally this is represented as\n",
    "\n",
    "$y = f(x) + x$\n",
    "\n",
    "To achieve this we simply sum the projection matrix `x` with the flash attention output `x_norm`.  As a reminder we'll print out X.  As you can see, because `x` was based on the tokens, it has a different value per row, so even though `x_norm` has the same value per row, we'll result in a diverse set of weights.  With this you can see the power of skip connections passing through weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecc71ade-9154-481e-935c-f5d077a3b549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3700, 0.3900, 0.4100, 0.4300],\n",
       "         [0.1800, 0.2000, 0.2200, 0.2400],\n",
       "         [0.3600, 0.3800, 0.4000, 0.4200],\n",
       "         [0.1400, 0.1600, 0.1800, 0.2000],\n",
       "         [0.1100, 0.1300, 0.1500, 0.1700],\n",
       "         [0.2700, 0.2900, 0.3100, 0.3300],\n",
       "         [0.3800, 0.4000, 0.4200, 0.4400],\n",
       "         [0.2400, 0.2600, 0.2800, 0.3000]],\n",
       "\n",
       "        [[0.1300, 0.1500, 0.1700, 0.1900],\n",
       "         [0.1200, 0.1400, 0.1600, 0.1800],\n",
       "         [0.1000, 0.1200, 0.1400, 0.1600],\n",
       "         [0.2500, 0.2700, 0.2900, 0.3100],\n",
       "         [0.1100, 0.1300, 0.1500, 0.1700],\n",
       "         [0.0700, 0.0900, 0.1100, 0.1300],\n",
       "         [0.2100, 0.2300, 0.2500, 0.2700],\n",
       "         [0.3000, 0.3200, 0.3400, 0.3600]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c5bc275-e624-47ac-8958-735e3227b239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[0.3712, 0.3928, 0.4148, 0.4371],\n",
       "          [0.1812, 0.2028, 0.2248, 0.2471],\n",
       "          [0.3612, 0.3828, 0.4048, 0.4271],\n",
       "          [0.1412, 0.1628, 0.1848, 0.2071],\n",
       "          [0.1112, 0.1328, 0.1548, 0.1771],\n",
       "          [0.2712, 0.2928, 0.3148, 0.3371],\n",
       "          [0.3812, 0.4028, 0.4248, 0.4471],\n",
       "          [0.2412, 0.2628, 0.2848, 0.3071]],\n",
       " \n",
       "         [[0.1312, 0.1528, 0.1748, 0.1971],\n",
       "          [0.1212, 0.1428, 0.1648, 0.1871],\n",
       "          [0.1012, 0.1228, 0.1448, 0.1671],\n",
       "          [0.2512, 0.2728, 0.2948, 0.3171],\n",
       "          [0.1112, 0.1328, 0.1548, 0.1771],\n",
       "          [0.0712, 0.0928, 0.1148, 0.1371],\n",
       "          [0.2112, 0.2328, 0.2548, 0.2771],\n",
       "          [0.3012, 0.3228, 0.3448, 0.3671]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x + x_norm\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958bfe1-5bac-4d7d-80fe-585f915dbffb",
   "metadata": {},
   "source": [
    "#### Transformer - Layer Normalization 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb3d5d-98f0-4c80-b0c0-e600c915b2c2",
   "metadata": {},
   "source": [
    "We'll run another round of normalization now on the outputs of the masked multi-head attention and skip connection to ensure our values are not too spread apart. This layer will run the same normalization formula as before, but is it's own independent layer as it has different inputs. Recall the formula is: \n",
    "\n",
    "$y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\operatorname{Var}[x] + \\epsilon}}\\$\n",
    "\n",
    "Because we will do a skip connection again for the next layer, MLP, we'll once again branch `x` for the normalization and MLP and then sum it back togehter with `x`. \n",
    "\n",
    "Similar to before you'll notice that normalization brings all the values back to the same entry.  Despite `x` having different values, the distribution of the values is the same which is why normalization brings them back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc3b130a-fb31-442b-a66e-ad44e75ab00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_2 = nn.LayerNorm(n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1529dc49-c8c2-409f-ac41-97cceca40030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380]],\n",
       " \n",
       "         [[-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380],\n",
       "          [-1.3232, -0.4509,  0.4361,  1.3380]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm_2 = ln_2(x)\n",
    "x_norm_2.shape, x_norm_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca684f-4d15-409b-889f-781a39170b32",
   "metadata": {},
   "source": [
    "#### Transformer - Feed Forward (aka Multi-layer Perceptron)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd73af80-ed3e-4353-8450-5f7f62da5cde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<img src=\"explainer_screenshots/mlp.png\" width=\"400\">\n",
    "\n",
    "The feed-forward sublayer consists of two-layer mirroring a multi-layer perceptron, MLP.  These layers mix the features within each token vector but never across time. The output `x` from multi-headed attention starts as `B x T x C`.  Feed forward \n",
    "1. Calculates `4C` using a $XA^\\top + B$ linear layer \n",
    "2. Normalizes the data using a `tanh` based GELU layer. This layer pushes extreme values to +/- 1\n",
    "3. Projects back down to `C` with a final $XW^\\top + B$ linear layer.\n",
    "\n",
    "The MLP nonlinearly re-expresses each token's channel features before being aggregated across the hidden layers and passed to the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9efc46-5aec-4bea-8169-7c2302385aa2",
   "metadata": {},
   "source": [
    "We'll first start by creating the 3 different layers:\n",
    "1. `mlp_fc` - Linear layer to project up to `4C`\n",
    "2. `mlp_gelu` - tanh layer\n",
    "3. `mlp_proj` - Linear layer to project down to `C`\n",
    "\n",
    "We'll do similar initiation to our linear layers as before. Since the tanh step is a calculation per row (similar to layer normalization), we will not do initiation for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ca50703-eb86-4945-ad25-eab514ba66d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 4]),\n",
       " Parameter containing:\n",
       " tensor([[0.0010, 0.0020, 0.0030, 0.0040],\n",
       "         [0.0020, 0.0050, 0.0070, 0.0090],\n",
       "         [0.0030, 0.0090, 0.0120, 0.0150],\n",
       "         [0.0040, 0.0140, 0.0180, 0.0220],\n",
       "         [0.0050, 0.0200, 0.0250, 0.0300],\n",
       "         [0.0060, 0.0270, 0.0330, 0.0390],\n",
       "         [0.0070, 0.0350, 0.0420, 0.0490],\n",
       "         [0.0080, 0.0440, 0.0520, 0.0600],\n",
       "         [0.0090, 0.0540, 0.0630, 0.0720],\n",
       "         [0.0100, 0.0650, 0.0750, 0.0850],\n",
       "         [0.0110, 0.0770, 0.0880, 0.0990],\n",
       "         [0.0120, 0.0900, 0.1020, 0.1140],\n",
       "         [0.0130, 0.1040, 0.1170, 0.1300],\n",
       "         [0.0140, 0.1190, 0.1330, 0.1470],\n",
       "         [0.0150, 0.1350, 0.1500, 0.1650],\n",
       "         [0.0160, 0.1520, 0.1680, 0.1840]], requires_grad=True))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_fc = nn.Linear(n_embd, 4 * n_embd)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, inp = mlp_fc.weight.shape  # (3*n_embd, n_embd)\n",
    "    r = torch.arange(1, out + 1).unsqueeze(1)  # [out,1], 1-indexed\n",
    "    c = torch.arange(1, inp + 1).unsqueeze(0)   # [1,inp], 1-indexed\n",
    "    base = r * c                          # rc\n",
    "    tri = r * (r - 1) / 2                 # T_{r-1} = r(r-1)/2, shape [out,1]\n",
    "    mask = (c >= 2)           # add T_{r-1} only from column 2 onward\n",
    "    pattern = 1e-3 * (base + tri * mask) # matches [[.001,.002,.003],[.002,.005,.007],[.003,.009,.012],...]\n",
    "    mlp_fc.weight.copy_(pattern)\n",
    "\n",
    "mlp_fc.weight.shape, mlp_fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fc7f2a9-b7bb-4d13-b342-50bd9a9cb978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GELU(approximate='tanh')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_gelu = nn.GELU(approximate='tanh')\n",
    "mlp_gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0232f666-9a5e-4d13-b566-4db54f4757f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 16]),\n",
       " Parameter containing:\n",
       " tensor([[0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090,\n",
       "          0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160],\n",
       "         [0.0020, 0.0050, 0.0070, 0.0090, 0.0110, 0.0130, 0.0150, 0.0170, 0.0190,\n",
       "          0.0210, 0.0230, 0.0250, 0.0270, 0.0290, 0.0310, 0.0330],\n",
       "         [0.0030, 0.0090, 0.0120, 0.0150, 0.0180, 0.0210, 0.0240, 0.0270, 0.0300,\n",
       "          0.0330, 0.0360, 0.0390, 0.0420, 0.0450, 0.0480, 0.0510],\n",
       "         [0.0040, 0.0140, 0.0180, 0.0220, 0.0260, 0.0300, 0.0340, 0.0380, 0.0420,\n",
       "          0.0460, 0.0500, 0.0540, 0.0580, 0.0620, 0.0660, 0.0700]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_proj = nn.Linear(4 * n_embd, n_embd)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, inp = mlp_proj.weight.shape  # (3*n_embd, n_embd)\n",
    "    r = torch.arange(1, out + 1).unsqueeze(1)  # [out,1], 1-indexed\n",
    "    c = torch.arange(1, inp + 1).unsqueeze(0)   # [1,inp], 1-indexed\n",
    "    base = r * c                          # rc\n",
    "    tri = r * (r - 1) / 2                 # T_{r-1} = r(r-1)/2, shape [out,1]\n",
    "    mask = (c >= 2)           # add T_{r-1} only from column 2 onward\n",
    "    pattern = 1e-3 * (base + tri * mask) # matches [[.001,.002,.003],[.002,.005,.007],[.003,.009,.012],...]\n",
    "    mlp_proj.weight.copy_(pattern)\n",
    "\n",
    "mlp_proj.weight.shape, mlp_proj.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad01ac2-6018-48bb-917c-1b2d293b056c",
   "metadata": {},
   "source": [
    "**MLP - 4c projection**  - Now we'll take `x_norm_2` and apply the first linear layer that projects upward to `4C`.  Reminder that the  calculation is $x\\_mlp = x\\_norm\\_2 \\cdot mlp\\_fc^\\top$. Once again we'll see a repetition of values in `x_mlp` as the rows in `x_norm_2` are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6d86c8c-1003-48c1-9617-de556aaed302",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 16]),\n",
       " tensor([[[-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6769,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6769,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6768,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6769,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6769,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6768,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6768,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6768,\n",
       "           -0.2593, -0.0073]],\n",
       " \n",
       "         [[-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6769,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6769,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6768,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6768,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6769,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6769,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6769,\n",
       "           -0.2593, -0.0073],\n",
       "          [-0.1647, -0.3401, -0.0664, -0.4235, -0.3288, -0.3806,  0.2099,\n",
       "           -0.1826,  0.0884,  0.5907, -0.1220, -0.2782,  0.4654,  0.6768,\n",
       "           -0.2593, -0.0073]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mlp = mlp_fc(x_norm_2)\n",
    "x_mlp.shape, x_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86159a6-dbeb-48f4-9e79-1e9a74577496",
   "metadata": {},
   "source": [
    "**MLP - tanh**  - Now we'll apply the tanh approximation (GELU - tanh) which smoothly gates each input `x_mlp`. The formula applied is \n",
    "\n",
    "$\\tanh(x)=\\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-x)}$\n",
    "\n",
    "The Tanh formula pushes large positive numbers to 1 and large negative numbers to -1.  `tanh` is applied element wise across the full `x_mlp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a089adf6-8301-4b09-9c7e-647bdb750041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 16]),\n",
       " tensor([[[-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036]],\n",
       " \n",
       "         [[-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036],\n",
       "          [-0.0716, -0.1248, -0.0315, -0.1423, -0.1220, -0.1339,  0.1224,\n",
       "           -0.0781,  0.0473,  0.4268, -0.0551, -0.1086,  0.3161,  0.5081,\n",
       "           -0.1031, -0.0036]]], grad_fn=<GeluBackward0>))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mlp = mlp_gelu(x_mlp)\n",
    "x_mlp.shape, x_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b0108-4a93-4231-bd9f-05da60815e9c",
   "metadata": {},
   "source": [
    "**MLP - down projection**  - Finally we'll take `x_mlp` and project `4C` back down to `C` using the weights in `mlp_proj`. Recall that we apply $x\\_mlp = x\\_mlp\\_2 \\cdot mlp\\_proj^\\top$.  Even though the layer weights are `4x16` the transpose in $XW^\\top + B$ allows to project back down. We'll maintain the row repetion though this process for all the same reasons we've seen this far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe0d6434-43d2-46c7-bd18-c6e659205b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559]],\n",
       " \n",
       "         [[ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559],\n",
       "          [ 0.2121, -0.1647, -0.0989,  0.1559]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mlp = mlp_proj(x_mlp)\n",
    "x_mlp.shape, x_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a61c0-d1c8-4820-8744-1de06d820269",
   "metadata": {},
   "source": [
    "#### Transformer - Residual (skip) connection 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b46b76-7fef-4c2f-8bd2-a86d8da405b6",
   "metadata": {},
   "source": [
    "Once again our transformer uses a skip connection to allow for passing gradients through the Feed Forward, aka MLP, layer. Just like the first skip connection, functionally this is represented as\n",
    "\n",
    "$y = f(x) + x$\n",
    "\n",
    "To achieve this we simply sum the MLP input matrix `x` with the MLP output `x_mlp`.  As a reminder we'll print out `x`.  As you can see, because `x` was based on the tokens, it has a different value per row, so even though `x_mlp` has the same value per row, we'll result in a diverse set of weights.  With this you can see the power of skip connections passing through weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c773183-3483-4b3b-bbaa-4505072cfc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3712, 0.3928, 0.4148, 0.4371],\n",
       "         [0.1812, 0.2028, 0.2248, 0.2471],\n",
       "         [0.3612, 0.3828, 0.4048, 0.4271],\n",
       "         [0.1412, 0.1628, 0.1848, 0.2071],\n",
       "         [0.1112, 0.1328, 0.1548, 0.1771],\n",
       "         [0.2712, 0.2928, 0.3148, 0.3371],\n",
       "         [0.3812, 0.4028, 0.4248, 0.4471],\n",
       "         [0.2412, 0.2628, 0.2848, 0.3071]],\n",
       "\n",
       "        [[0.1312, 0.1528, 0.1748, 0.1971],\n",
       "         [0.1212, 0.1428, 0.1648, 0.1871],\n",
       "         [0.1012, 0.1228, 0.1448, 0.1671],\n",
       "         [0.2512, 0.2728, 0.2948, 0.3171],\n",
       "         [0.1112, 0.1328, 0.1548, 0.1771],\n",
       "         [0.0712, 0.0928, 0.1148, 0.1371],\n",
       "         [0.2112, 0.2328, 0.2548, 0.2771],\n",
       "         [0.3012, 0.3228, 0.3448, 0.3671]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b611c470-d48d-4c86-a1a3-7a0489066e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[ 0.5833,  0.2281,  0.3159,  0.5930],\n",
       "          [ 0.3933,  0.0381,  0.1259,  0.4030],\n",
       "          [ 0.5733,  0.2181,  0.3059,  0.5830],\n",
       "          [ 0.3533, -0.0019,  0.0859,  0.3630],\n",
       "          [ 0.3233, -0.0319,  0.0559,  0.3330],\n",
       "          [ 0.4833,  0.1281,  0.2159,  0.4930],\n",
       "          [ 0.5933,  0.2381,  0.3259,  0.6030],\n",
       "          [ 0.4533,  0.0981,  0.1859,  0.4630]],\n",
       " \n",
       "         [[ 0.3433, -0.0119,  0.0759,  0.3530],\n",
       "          [ 0.3333, -0.0219,  0.0659,  0.3430],\n",
       "          [ 0.3133, -0.0419,  0.0459,  0.3230],\n",
       "          [ 0.4633,  0.1081,  0.1959,  0.4730],\n",
       "          [ 0.3233, -0.0319,  0.0559,  0.3330],\n",
       "          [ 0.2833, -0.0719,  0.0159,  0.2930],\n",
       "          [ 0.4233,  0.0681,  0.1559,  0.4330],\n",
       "          [ 0.5133,  0.1581,  0.2459,  0.5230]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_mlp + x\n",
    "\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d01f807-bb7c-4cff-a5fa-44ae767a3431",
   "metadata": {},
   "source": [
    "#### Transformer - Final Layer Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9fa593-6cb1-488f-b33c-46b817b2bfe7",
   "metadata": {},
   "source": [
    "The final step in the transformer is to aggregate and normalize before calculating the final projections. This layer is similar to the previous normalization layers. This layer will run the same normalization formula as before, but is its own independent layer as it has different inputs. Recall the formula is: \n",
    "\n",
    "$y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\operatorname{Var}[x] + \\epsilon}}$\n",
    "\n",
    "Since this is the final layer, we will not have a residual connection so we do not need to branch `x`.  As with previous normalization layers, we'll once again see the rows unify in value as the distribution per row is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "647f8975-ce82-41ee-9b99-9fb80f4ffa0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((4,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_f = nn.LayerNorm(n_embd)\n",
    "ln_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c5f7b20-da66-4fe6-ba7e-df19c162a124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 4]),\n",
       " tensor([[[ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108]],\n",
       " \n",
       "         [[ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "          [ 0.9509, -1.2532, -0.7086,  1.0108]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ln_f(x)\n",
    "x.shape, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794c634-a6cf-49b9-85df-f96488b4021b",
   "metadata": {},
   "source": [
    "### Output Layers AKA Model Head."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec2f6bc6-edfc-4d19-ac3a-f8cb02d39ad1",
   "metadata": {},
   "source": [
    "The combination of masked multi-head attention and feed forward, along with the normalization and residual connections is considered the \"transformer\".  In practice this layer is horizontally scaled to run many layers in parallel.  Once those layers are complete during the forward pass we then start the output process that results in `logits` which is a representation of the probability of each token being the next token given the input.  \n",
    "\n",
    "This layer is also known as the model **head**, not to be confused with attention heads. This layer is called this because it is a small, task-specific module attached to a model’s shared backbone that maps hidden features to the final outputs.  In our example case, this is a linear layer mapping the backbone to vocab logits. The benefit of this structure is that you can use the shared hidden features and train different heads for different tasks without starting from scratch. An example would be a classifier head, or policy head in RL.\n",
    "\n",
    "<img src=\"explainer_screenshots/output_layer.png\" width=\"200\">\n",
    "\n",
    "For our head we want to map to a predicted token which we'll look at as `logits`. In the process to generate `logits` we take the normalized output `x` of the transformers, then project, using a linear layer, to the vocabulary resulting in a `B, T, vocab_size` matrix known as `logits`.  \n",
    "\n",
    "In training, the `logits` are then compared with `y` to see how close the  model is to predicting the correct next token. For inference, the `logits` are then used to drive sampling which is how the next token is then derived. \n",
    "\n",
    "\n",
    "Instead of initializing weights this time around, we'll do **Weight Tying**.  Weight tying sets the output softmax matrix equal to the transpose of the input embedding matrix $W_{\\text{out}} = W_e^\\top$, forcing the model to “read” and “predict” in the same token space. This reduces parameters and acts as a useful prior, improving sample-efficiency and often perplexity by aligning input–output geometry. Modern LLMs have seemed to ditch this though to gain the extra capacity, but, for our example, we'll maintain it.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65a9da09-0da1-433a-b276-780875c56b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=4, out_features=36, bias=False),\n",
       " Parameter containing:\n",
       " tensor([[0.0100, 0.0200, 0.0300, 0.0400],\n",
       "         [0.0200, 0.0300, 0.0400, 0.0500],\n",
       "         [0.0300, 0.0400, 0.0500, 0.0600],\n",
       "         [0.0400, 0.0500, 0.0600, 0.0700],\n",
       "         [0.0500, 0.0600, 0.0700, 0.0800],\n",
       "         [0.0600, 0.0700, 0.0800, 0.0900],\n",
       "         [0.0700, 0.0800, 0.0900, 0.1000],\n",
       "         [0.0800, 0.0900, 0.1000, 0.1100],\n",
       "         [0.0900, 0.1000, 0.1100, 0.1200],\n",
       "         [0.1000, 0.1100, 0.1200, 0.1300],\n",
       "         [0.1100, 0.1200, 0.1300, 0.1400],\n",
       "         [0.1200, 0.1300, 0.1400, 0.1500],\n",
       "         [0.1300, 0.1400, 0.1500, 0.1600],\n",
       "         [0.1400, 0.1500, 0.1600, 0.1700],\n",
       "         [0.1500, 0.1600, 0.1700, 0.1800],\n",
       "         [0.1600, 0.1700, 0.1800, 0.1900],\n",
       "         [0.1700, 0.1800, 0.1900, 0.2000],\n",
       "         [0.1800, 0.1900, 0.2000, 0.2100],\n",
       "         [0.1900, 0.2000, 0.2100, 0.2200],\n",
       "         [0.2000, 0.2100, 0.2200, 0.2300],\n",
       "         [0.2100, 0.2200, 0.2300, 0.2400],\n",
       "         [0.2200, 0.2300, 0.2400, 0.2500],\n",
       "         [0.2300, 0.2400, 0.2500, 0.2600],\n",
       "         [0.2400, 0.2500, 0.2600, 0.2700],\n",
       "         [0.2500, 0.2600, 0.2700, 0.2800],\n",
       "         [0.2600, 0.2700, 0.2800, 0.2900],\n",
       "         [0.2700, 0.2800, 0.2900, 0.3000],\n",
       "         [0.2800, 0.2900, 0.3000, 0.3100],\n",
       "         [0.2900, 0.3000, 0.3100, 0.3200],\n",
       "         [0.3000, 0.3100, 0.3200, 0.3300],\n",
       "         [0.3100, 0.3200, 0.3300, 0.3400],\n",
       "         [0.3200, 0.3300, 0.3400, 0.3500],\n",
       "         [0.3300, 0.3400, 0.3500, 0.3600],\n",
       "         [0.3400, 0.3500, 0.3600, 0.3700],\n",
       "         [0.3500, 0.3600, 0.3700, 0.3800],\n",
       "         [0.3600, 0.3700, 0.3800, 0.3900]], requires_grad=True))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "# weight sharing scheme\n",
    "lm_head.weight = wte.weight\n",
    "\n",
    "lm_head, lm_head.weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6601a613-4384-4e20-bd67-152c2248873f",
   "metadata": {},
   "source": [
    "now let's check that the values are the same and that the underlying objects `data_ptr()` are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "172cc399-ac64-4fc7-96bd-50bb2ccd80ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head.weight is wte.weight, lm_head.weight.data_ptr() == wte.weight.data_ptr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda51be-2d33-42e4-87c7-1b12065c61b5",
   "metadata": {},
   "source": [
    "#### Output layer - LM Head aka logits\n",
    "We now project `x` onto the vocabulary resulting in a `B X T X vocab_size` final array `logits`.  This output correlates with the \n",
    "probabilty of each output token given the input context.  The best way to read  this is:\n",
    "\n",
    "(dimension 0) we have 2 batches B, \n",
    "(dimension 1) each batch has an example for each value between 1 and context length T \n",
    "(dimension 2) for each example we see the probability of each token in our vocabulary\n",
    "\n",
    "Since our `x` at this point has the same values across the row, for every row, we fully expect that our logits will have the same value.  In practice this means that our model will have the same probability of a token output as the 'next token' regardless of the preceeding text, meaning it's shit. Luckily backpropogation has a way of updating this so that with enough data and time the probabilities change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d39d0446-9f58-425c-a5c2-7f7049639137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108]],\n",
       "\n",
       "        [[ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108],\n",
       "         [ 0.9509, -1.2532, -0.7086,  1.0108]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c2b2104-0286-413f-acfd-911df164caff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 36]),\n",
       " tensor([[[0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036]],\n",
       " \n",
       "         [[0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036],\n",
       "          [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "           0.0036, 0.0036, 0.0036, 0.0036]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = lm_head(x)\n",
    "\n",
    "logits.shape, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c613e5-00a2-4b94-b967-d978798e8155",
   "metadata": {},
   "source": [
    "## Loss calculation\n",
    "Now we have to see how good our ~shit~ prediction is.  Since we haven't done training, and we saw that regardless of example we had the same exact logit values, we can expect it's bad. That said, we need to know how bad. For this example we'll use cross entropy, also known as the negative log likelihood of the softmax.  Our loss calculates\n",
    "\n",
    "$$\n",
    "\\ell_i=-\\log\\big(\\mathrm{softmax}(z_i)\\_{y_i}\\big)\n",
    "= -z_{i,y_i}+\\log\\!\\sum_{c=1}^C e^{z_{i,c}},\n",
    "$$\n",
    "\n",
    "\n",
    "To calculate loss we'll pass in the calculated `logits` and our next tokens stored in `y`. The cross entropy function does not respect batches so we'll flatten the `B` dimension for both `logits` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18b3260e-a069-4ac7-8c4a-85c9d05b437d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16]),\n",
       " tensor([15, 32,  9,  5, 20, 30, 15, 11,  9,  6, 20,  5,  0, 13, 21,  0]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_flat = y.view(-1)\n",
    "y_flat.shape, y_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7defc97f-68af-46dd-b4bc-617c6e1a6be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 36]),\n",
       " tensor([[0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036],\n",
       "         [0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036,\n",
       "          0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "logits_flat.shape, logits_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8f4a08e-f5c3-440e-aa58-612700d882a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([]), tensor(3.5835, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits_flat, y_flat)\n",
    "loss.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c71deeb-6f5d-45a2-ac5f-07357cf2d3b1",
   "metadata": {},
   "source": [
    "## Back Propogation\n",
    "\n",
    "We now know just how well the current model, with its weights and biases, does predicting the next token given the input context. We now need to know how to change the different weigths and biases to improve the formula.  We could do this by guessing through making minor changes and seeing what improves, or we can think through this more critically.\n",
    "\n",
    "If you review the chain of layers above, you can see that it's a series of formulas.  We can think of this as $f(g(x))$, except with many many more layers and complexities.  Since this is a formula, we can dig into our math toolbox and find a better way to determine what parts need to update.  Recall that in our calculus we learned that differentiation tells us the rate of change in a graph.  So if we treat the loss function $\\mathcal{L}$ as $\\mathcal{L}(f(g(x)))$ taking the partial differential \n",
    "\n",
    "$\\delta=\\partial \\mathcal{L}/\\partial h$\n",
    "\n",
    "at each layer will give us the impact of each weight/bias on our final out (alebit the inverse since our loss function is the negative log likelihood). \n",
    "\n",
    "Lucky for us, each layer of our model already has a placeholder for the partial differential called the **Gradient**. We'll use this field to store it.  We'll start by first zeroing out the gradients. We do this because of the nature of handling partial differentials for multiple dependencies. Recall that in mulitple places we had a formula structure of \n",
    "\n",
    "$a+b=c ; a+c= d$\n",
    "\n",
    "In this case $a$ has 2 dependencies and determining the partial derivative of $\\partial d / \\partial a$ requires understanding both the path from $d$ and $c$.  To determine the true impact of a we would sum both partial derivatives together.  Because of this property, the tool we use, the built in `.backwards()` automatically sums gradients, `+=`, so if we do not set the gradient to `0` we then end up with eroneous gradients. \n",
    "\n",
    "Finally, we start `.backwards` from the `loss`, not `logits` as our goal is to minimize loss, we need to ensure we are looking at the calcualations that impact loss which requires the whole forward pass to be able to generated the prediction `logits_flat`.  If we think of it as $\\mathcal{L}(f(x))$ where $f(x)$ is the forward pass to generate logits, then a simple chain rule is applied:\n",
    "\n",
    "${\\partial}/{\\partial x} =  \\mathcal{L}'(f(x)) f'(x)$\n",
    "\n",
    "Lets start by zeroing the gradients and leaning on pytorch to calculate the gradients for us. We'll also validate the gradients were `none`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5bd1ff71-31ae-47f8-b345-df28a0e24da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head.zero_grad()\n",
    "ln_f.zero_grad()\n",
    "mlp_proj.zero_grad()\n",
    "mlp_fc.zero_grad()\n",
    "ln_2.zero_grad()\n",
    "c_proj.zero_grad()\n",
    "c_attn.zero_grad()\n",
    "ln_1.zero_grad()\n",
    "wpe.zero_grad()\n",
    "wte.zero_grad()\n",
    "\n",
    "# validate gradients\n",
    "lm_head.weight.grad, ln_f.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b83c2e41-3f89-4961-8876-caf399b9db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6275a322-b7e4-4e56-977d-9ef7a48f9f30",
   "metadata": {},
   "source": [
    "**Auto-Diff** - Now let's see the magic of the gradients populate.  This magic is called auto-differentiation, or auto-diff for short. This allows us to not have to write write many layers of nasty code to do the differentiation for us, but, if you're a sadist, you can surely find people who have written out that code (it's not too bad since you just do one layer at a time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "320f3634-84e3-46f0-b7d1-91125a4b328c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281]]),\n",
       " tensor([ 0.0410, -0.0540, -0.0306,  0.0436]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head.weight.grad, ln_f.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7cc61a-4c4b-41d7-8989-e96b0263e386",
   "metadata": {},
   "source": [
    "## Learning\n",
    "The process of learning now requires us to update our weights based on this gradient. To really feel the \"back propogation\" we'll start with the last layer and work backwards, though, since we have all of the gradients calculated already, the order does not matter. Recall that our loss function is the negative log likelihood ratio so our gradient signs are flipped.  If a parameter is important, the gradient will be more negaitve, and vice versa. The gradients are a ratio of importance of each parameter and we need to know how much of that gradient to apply to our weights. This \"how much\" is referred to as the *learning rate*.  In modern training learning rate schedulers and optimzers are used to vary the rate and application by layer and by training round.  We'll keep it simple and use an astronomically high learning rate of `1.000` which applies the gradient directly to the weights via a `-=`. Gradient for the weights and the biases is different as the partial differential with respect to each is different. We need to remember in the layers with bias to apply it to both.\n",
    "\n",
    "*Note that since our vocab is very small, our context is small, and our batch is small, relatively our model is very deep so we will see a lot of exceptionally small gradients*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1f0a2de-fc67-43ed-9196-40a513eed032",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Huge learning rate to emphasize\n",
    "learning_rate = 1.000 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa3ed8-fb43-4fa3-be4b-cd9b1a70dcf4",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Let's start with our output layer.  If you recall we did *weight tying* of our head `lm_head` to our token projection `wte`.  Because of this applying loss to the `lm_head` will automatically apply it to `wte`. Since we're doing this manually and not through an optimizer we need to be careful not to apply it twice. Since we're starting with the last layer first, the head, we'll see that the gradients are equal and that updating just the `lm_head` weights updates `wte` weights also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac3ee682-bcc2-4c06-84b1-9d6ca4c368b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " tensor([[-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281]]),\n",
       " tensor([[-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0924,  0.1218,  0.0689, -0.0983],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [-0.0330,  0.0435,  0.0246, -0.0351],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281],\n",
       "         [ 0.0264, -0.0348, -0.0197,  0.0281]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head.weight.grad is wte.weight.grad, lm_head.weight.grad, wte.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a440926-e4ed-49b5-b395-b41f9644919b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " Parameter containing:\n",
       " tensor([[ 0.1024, -0.1018, -0.0389,  0.1383],\n",
       "         [-0.0064,  0.0648,  0.0597,  0.0219],\n",
       "         [ 0.0036,  0.0748,  0.0697,  0.0319],\n",
       "         [ 0.0136,  0.0848,  0.0797,  0.0419],\n",
       "         [ 0.0236,  0.0948,  0.0897,  0.0519],\n",
       "         [ 0.1524, -0.0518,  0.0111,  0.1883],\n",
       "         [ 0.1030,  0.0365,  0.0654,  0.1351],\n",
       "         [ 0.0536,  0.1248,  0.1197,  0.0819],\n",
       "         [ 0.0636,  0.1348,  0.1297,  0.0919],\n",
       "         [ 0.1924, -0.0118,  0.0511,  0.2283],\n",
       "         [ 0.0836,  0.1548,  0.1497,  0.1119],\n",
       "         [ 0.1530,  0.0865,  0.1154,  0.1851],\n",
       "         [ 0.1036,  0.1748,  0.1697,  0.1319],\n",
       "         [ 0.1730,  0.1065,  0.1354,  0.2051],\n",
       "         [ 0.1236,  0.1948,  0.1897,  0.1519],\n",
       "         [ 0.2524,  0.0482,  0.1111,  0.2883],\n",
       "         [ 0.1436,  0.2148,  0.2097,  0.1719],\n",
       "         [ 0.1536,  0.2248,  0.2197,  0.1819],\n",
       "         [ 0.1636,  0.2348,  0.2297,  0.1919],\n",
       "         [ 0.1736,  0.2448,  0.2397,  0.2019],\n",
       "         [ 0.3024,  0.0982,  0.1611,  0.3383],\n",
       "         [ 0.2530,  0.1865,  0.2154,  0.2851],\n",
       "         [ 0.2036,  0.2748,  0.2697,  0.2319],\n",
       "         [ 0.2136,  0.2848,  0.2797,  0.2419],\n",
       "         [ 0.2236,  0.2948,  0.2897,  0.2519],\n",
       "         [ 0.2336,  0.3048,  0.2997,  0.2619],\n",
       "         [ 0.2436,  0.3148,  0.3097,  0.2719],\n",
       "         [ 0.2536,  0.3248,  0.3197,  0.2819],\n",
       "         [ 0.2636,  0.3348,  0.3297,  0.2919],\n",
       "         [ 0.2736,  0.3448,  0.3397,  0.3019],\n",
       "         [ 0.3430,  0.2765,  0.3054,  0.3751],\n",
       "         [ 0.2936,  0.3648,  0.3597,  0.3219],\n",
       "         [ 0.3630,  0.2965,  0.3254,  0.3951],\n",
       "         [ 0.3136,  0.3848,  0.3797,  0.3419],\n",
       "         [ 0.3236,  0.3948,  0.3897,  0.3519],\n",
       "         [ 0.3336,  0.4048,  0.3997,  0.3619]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1024, -0.1018, -0.0389,  0.1383],\n",
       "         [-0.0064,  0.0648,  0.0597,  0.0219],\n",
       "         [ 0.0036,  0.0748,  0.0697,  0.0319],\n",
       "         [ 0.0136,  0.0848,  0.0797,  0.0419],\n",
       "         [ 0.0236,  0.0948,  0.0897,  0.0519],\n",
       "         [ 0.1524, -0.0518,  0.0111,  0.1883],\n",
       "         [ 0.1030,  0.0365,  0.0654,  0.1351],\n",
       "         [ 0.0536,  0.1248,  0.1197,  0.0819],\n",
       "         [ 0.0636,  0.1348,  0.1297,  0.0919],\n",
       "         [ 0.1924, -0.0118,  0.0511,  0.2283],\n",
       "         [ 0.0836,  0.1548,  0.1497,  0.1119],\n",
       "         [ 0.1530,  0.0865,  0.1154,  0.1851],\n",
       "         [ 0.1036,  0.1748,  0.1697,  0.1319],\n",
       "         [ 0.1730,  0.1065,  0.1354,  0.2051],\n",
       "         [ 0.1236,  0.1948,  0.1897,  0.1519],\n",
       "         [ 0.2524,  0.0482,  0.1111,  0.2883],\n",
       "         [ 0.1436,  0.2148,  0.2097,  0.1719],\n",
       "         [ 0.1536,  0.2248,  0.2197,  0.1819],\n",
       "         [ 0.1636,  0.2348,  0.2297,  0.1919],\n",
       "         [ 0.1736,  0.2448,  0.2397,  0.2019],\n",
       "         [ 0.3024,  0.0982,  0.1611,  0.3383],\n",
       "         [ 0.2530,  0.1865,  0.2154,  0.2851],\n",
       "         [ 0.2036,  0.2748,  0.2697,  0.2319],\n",
       "         [ 0.2136,  0.2848,  0.2797,  0.2419],\n",
       "         [ 0.2236,  0.2948,  0.2897,  0.2519],\n",
       "         [ 0.2336,  0.3048,  0.2997,  0.2619],\n",
       "         [ 0.2436,  0.3148,  0.3097,  0.2719],\n",
       "         [ 0.2536,  0.3248,  0.3197,  0.2819],\n",
       "         [ 0.2636,  0.3348,  0.3297,  0.2919],\n",
       "         [ 0.2736,  0.3448,  0.3397,  0.3019],\n",
       "         [ 0.3430,  0.2765,  0.3054,  0.3751],\n",
       "         [ 0.2936,  0.3648,  0.3597,  0.3219],\n",
       "         [ 0.3630,  0.2965,  0.3254,  0.3951],\n",
       "         [ 0.3136,  0.3848,  0.3797,  0.3419],\n",
       "         [ 0.3236,  0.3948,  0.3897,  0.3519],\n",
       "         [ 0.3336,  0.4048,  0.3997,  0.3619]], requires_grad=True))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    lm_head.weight -= learning_rate * lm_head.weight.grad\n",
    "lm_head.weight is wte.weight, lm_head.weight, wte.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5e4cae-5b45-4e08-a267-f07e80d5f275",
   "metadata": {},
   "source": [
    "### Transformer - Final Layer Normalization\n",
    "Now let's move to the layer normalization.  In our forward pass we did not change the weights, but in training we still will as the normalization can impact the outputs.  Additionally layer normalization has bias so we have to remember to add it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07245c28-6172-4cab-8304-4db806132373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_f.weight, ln_f.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "24b44391-8904-4ffc-855b-7c37b5422aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0410, -0.0540, -0.0306,  0.0436]),\n",
       " tensor([0.0431, 0.0431, 0.0431, 0.0431]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_f.weight.grad, ln_f.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "432f1496-607f-42b5-bb0f-6fa47a4e2354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([0.9590, 1.0540, 1.0306, 0.9564], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0431, -0.0431, -0.0431, -0.0431], requires_grad=True))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ln_f.weight -= learning_rate * ln_f.weight.grad\n",
    "    ln_f.bias   -= learning_rate * ln_f.bias.grad\n",
    "ln_f.weight, ln_f.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3915ed-1681-4d32-9566-1e75cc396396",
   "metadata": {},
   "source": [
    "### Tranformer - Feed Froward Updates\n",
    "In feed forward we had 3 laysers: the upward projection `mlp_fc`, the tanh layer `mlp_gelu`, and the downward projection `mlp_proj`.  The tanh layer normalizes each element using tanh so it has no trainable parameters, or weights, so there is no updates to be made, the gradient passes through it. But the other two layers have both weights and biases that we can update.  \n",
    "\n",
    "One thing you may notice is that the gradients are exceptionally small in some cases, leading to minimal updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12d1458b-1da5-4268-a0a7-f5a1a5c5f6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5.9999e-10,  1.0460e-09,  2.6364e-10,  1.1927e-09,  1.0229e-09,\n",
       "           1.1223e-09, -1.0259e-09,  6.5427e-10, -3.9661e-10, -3.5777e-09,\n",
       "           4.6170e-10,  9.1048e-10, -2.6494e-09, -4.2588e-09,  8.6449e-10,\n",
       "           3.0261e-11],\n",
       "         [-1.8000e-09, -3.1380e-09, -7.9090e-10, -3.5782e-09, -3.0686e-09,\n",
       "          -3.3668e-09,  3.0778e-09, -1.9628e-09,  1.1898e-09,  1.0733e-08,\n",
       "          -1.3851e-09, -2.7314e-09,  7.9483e-09,  1.2776e-08, -2.5935e-09,\n",
       "          -9.0780e-11],\n",
       "         [-1.7333e-09, -3.0218e-09, -7.6161e-10, -3.4456e-09, -2.9550e-09,\n",
       "          -3.2421e-09,  2.9638e-09, -1.8901e-09,  1.1458e-09,  1.0336e-08,\n",
       "          -1.3338e-09, -2.6303e-09,  7.6539e-09,  1.2303e-08, -2.4974e-09,\n",
       "          -8.7420e-11],\n",
       "         [ 2.6666e-09,  4.6489e-09,  1.1717e-09,  5.3010e-09,  4.5461e-09,\n",
       "           4.9878e-09, -4.5597e-09,  2.9079e-09, -1.7627e-09, -1.5901e-08,\n",
       "           2.0520e-09,  4.0466e-09, -1.1775e-08, -1.8928e-08,  3.8422e-09,\n",
       "           1.3450e-10]]),\n",
       " tensor([-8.3819e-09,  2.5146e-08,  2.4214e-08, -3.7253e-08]),\n",
       " tensor([[ 1.6860e-11,  5.7458e-12, -5.5570e-12, -1.7049e-11],\n",
       "         [ 6.1519e-11,  2.0965e-11, -2.0277e-11, -6.2207e-11],\n",
       "         [ 1.3553e-10,  4.6190e-11, -4.4672e-11, -1.3705e-10],\n",
       "         [ 6.3320e-11,  2.1579e-11, -2.0870e-11, -6.4028e-11],\n",
       "         [ 9.7379e-11,  3.3186e-11, -3.2096e-11, -9.8469e-11],\n",
       "         [ 9.2632e-11,  3.1569e-11, -3.0532e-11, -9.3669e-11],\n",
       "         [ 3.2290e-10,  1.1004e-10, -1.0643e-10, -3.2651e-10],\n",
       "         [ 1.8906e-10,  6.4431e-11, -6.2314e-11, -1.9118e-10],\n",
       "         [ 3.2894e-10,  1.1210e-10, -1.0842e-10, -3.3262e-10],\n",
       "         [ 5.7278e-10,  1.9520e-10, -1.8879e-10, -5.7919e-10],\n",
       "         [ 2.6926e-10,  9.1763e-11, -8.8748e-11, -2.7227e-10],\n",
       "         [ 2.0240e-10,  6.8978e-11, -6.6712e-11, -2.0467e-10],\n",
       "         [ 6.4198e-10,  2.1878e-10, -2.1160e-10, -6.4916e-10],\n",
       "         [ 7.7675e-10,  2.6471e-10, -2.5602e-10, -7.8544e-10],\n",
       "         [ 2.5311e-10,  8.6259e-11, -8.3425e-11, -2.5594e-10],\n",
       "         [ 4.4276e-10,  1.5089e-10, -1.4593e-10, -4.4772e-10]]),\n",
       " tensor([-1.2742e-11, -4.6492e-11, -1.0243e-10, -4.7853e-11, -7.3593e-11,\n",
       "         -7.0006e-11, -2.4403e-10, -1.4288e-10, -2.4859e-10, -4.3287e-10,\n",
       "         -2.0349e-10, -1.5296e-10, -4.8517e-10, -5.8702e-10, -1.9129e-10,\n",
       "         -3.3461e-10]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_proj.weight.grad, mlp_proj.bias.grad, mlp_fc.weight.grad,mlp_fc.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d43ddcad-1e49-4287-bf74-e3fd047dd43d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080, 0.0090,\n",
       "          0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0160],\n",
       "         [0.0020, 0.0050, 0.0070, 0.0090, 0.0110, 0.0130, 0.0150, 0.0170, 0.0190,\n",
       "          0.0210, 0.0230, 0.0250, 0.0270, 0.0290, 0.0310, 0.0330],\n",
       "         [0.0030, 0.0090, 0.0120, 0.0150, 0.0180, 0.0210, 0.0240, 0.0270, 0.0300,\n",
       "          0.0330, 0.0360, 0.0390, 0.0420, 0.0450, 0.0480, 0.0510],\n",
       "         [0.0040, 0.0140, 0.0180, 0.0220, 0.0260, 0.0300, 0.0340, 0.0380, 0.0420,\n",
       "          0.0460, 0.0500, 0.0540, 0.0580, 0.0620, 0.0660, 0.0700]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.2019, -0.1857, -0.1312,  0.1118], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0.0010, 0.0020, 0.0030, 0.0040],\n",
       "         [0.0020, 0.0050, 0.0070, 0.0090],\n",
       "         [0.0030, 0.0090, 0.0120, 0.0150],\n",
       "         [0.0040, 0.0140, 0.0180, 0.0220],\n",
       "         [0.0050, 0.0200, 0.0250, 0.0300],\n",
       "         [0.0060, 0.0270, 0.0330, 0.0390],\n",
       "         [0.0070, 0.0350, 0.0420, 0.0490],\n",
       "         [0.0080, 0.0440, 0.0520, 0.0600],\n",
       "         [0.0090, 0.0540, 0.0630, 0.0720],\n",
       "         [0.0100, 0.0650, 0.0750, 0.0850],\n",
       "         [0.0110, 0.0770, 0.0880, 0.0990],\n",
       "         [0.0120, 0.0900, 0.1020, 0.1140],\n",
       "         [0.0130, 0.1040, 0.1170, 0.1300],\n",
       "         [0.0140, 0.1190, 0.1330, 0.1470],\n",
       "         [0.0150, 0.1350, 0.1500, 0.1650],\n",
       "         [0.0160, 0.1520, 0.1680, 0.1840]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1691, -0.3503, -0.0837, -0.4492, -0.3642, -0.4271,  0.1511, -0.2551,\n",
       "          0.0009,  0.4868, -0.2436, -0.4188,  0.3045,  0.4943, -0.4648, -0.2370],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    mlp_proj.weight -= learning_rate * mlp_proj.weight.grad\n",
    "    mlp_proj.bias -= learning_rate * mlp_proj.bias.grad\n",
    "    mlp_fc.weight -= learning_rate * mlp_fc.weight.grad\n",
    "    mlp_fc.bias -= learning_rate * mlp_fc.bias.grad\n",
    "mlp_proj.weight, mlp_proj.bias, mlp_fc.weight,mlp_fc.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67318475-fb26-43db-ad67-7ed276f58cf2",
   "metadata": {},
   "source": [
    "### Tranformer - Layer Norm 2\n",
    "Now on to the next layer normalization.  Similarly we have both weights and biases to update again.  We can see once again these are exceptionally small gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5564e6d6-b7ef-4c03-8a4c-fdc0511fe86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4.9795e-11,  1.2989e-10, -1.4203e-10, -4.8610e-10]),\n",
       " tensor([-3.7632e-11, -2.8803e-10, -3.2567e-10, -3.6330e-10]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_2.weight.grad, ln_2.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "155bb665-f3af-4573-883f-cd47712379ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([3.7632e-11, 2.8803e-10, 3.2567e-10, 3.6330e-10], requires_grad=True))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ln_2.weight -= learning_rate * ln_2.weight.grad\n",
    "    ln_2.bias   -= learning_rate * ln_2.bias.grad\n",
    "ln_2.weight, ln_2.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab1e67e-e883-4fcf-b5cf-dc0774435318",
   "metadata": {},
   "source": [
    "### Tranformer - Flash Attention\n",
    "Even though flash attention was excpetionally complicated with multiple attnetion heads and the query, key, and value, we only have 2 sets of weights, and biases, to update: `c_attn`, `c_proj`.  Back propogation aggregated the different impacts across on each.  \n",
    "\n",
    "Similar to what we've seen this far, these gradiens are very small but we can see large order of magnitudedifferences within the values starting to suggest how they impact the ouputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "79182f08-1ad0-411d-8034-efa1546e3514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.0336e-20,  6.7788e-21, -6.7789e-21, -2.0336e-20],\n",
       "         [ 2.6692e-20,  8.8972e-21, -8.8973e-21, -2.6692e-20],\n",
       "         [-1.5235e-19, -5.0782e-20,  5.0782e-20,  1.5235e-19],\n",
       "         [-1.8786e-19, -6.2619e-20,  6.2619e-20,  1.8786e-19],\n",
       "         [ 2.5421e-21,  8.4737e-22, -8.4735e-22, -2.5421e-21],\n",
       "         [ 5.8467e-21,  1.9490e-21, -1.9489e-21, -5.8468e-21],\n",
       "         [-4.4673e-20, -1.4891e-20,  1.4891e-20,  4.4673e-20],\n",
       "         [-6.6437e-20, -2.2145e-20,  2.2146e-20,  6.6437e-20],\n",
       "         [ 4.6316e-11,  1.5439e-11, -1.5439e-11, -4.6316e-11],\n",
       "         [ 2.5493e-10,  8.4978e-11, -8.4978e-11, -2.5493e-10],\n",
       "         [ 3.0125e-10,  1.0042e-10, -1.0042e-10, -3.0125e-10],\n",
       "         [ 3.4757e-10,  1.1586e-10, -1.1586e-10, -3.4757e-10]]),\n",
       " tensor([-1.5309e-20, -2.0093e-20,  1.1468e-19,  1.4141e-19, -1.9136e-21,\n",
       "         -4.4013e-21,  3.3629e-20,  5.0012e-20, -3.4865e-11, -1.9191e-10,\n",
       "         -2.2677e-10, -2.6164e-10]),\n",
       " tensor([[-4.9517e-10, -5.8770e-10, -6.8774e-10, -7.9527e-10],\n",
       "         [ 1.9038e-09,  2.2596e-09,  2.6442e-09,  3.0577e-09],\n",
       "         [ 2.0412e-09,  2.4226e-09,  2.8349e-09,  3.2782e-09],\n",
       "         [-3.1232e-09, -3.7068e-09, -4.3378e-09, -5.0161e-09]]),\n",
       " tensor([-5.6477e-09,  2.1715e-08,  2.3281e-08, -3.5622e-08]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_attn.weight.grad,c_attn.bias.grad, c_proj.weight.grad, c_proj.bias.grad,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "573983a0-2c3e-419a-b261-f00b988fc915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.0010, 0.0020, 0.0030, 0.0040],\n",
       "         [0.0020, 0.0050, 0.0070, 0.0090],\n",
       "         [0.0030, 0.0090, 0.0120, 0.0150],\n",
       "         [0.0040, 0.0140, 0.0180, 0.0220],\n",
       "         [0.0050, 0.0200, 0.0250, 0.0300],\n",
       "         [0.0060, 0.0270, 0.0330, 0.0390],\n",
       "         [0.0070, 0.0350, 0.0420, 0.0490],\n",
       "         [0.0080, 0.0440, 0.0520, 0.0600],\n",
       "         [0.0090, 0.0540, 0.0630, 0.0720],\n",
       "         [0.0100, 0.0650, 0.0750, 0.0850],\n",
       "         [0.0110, 0.0770, 0.0880, 0.0990],\n",
       "         [0.0120, 0.0900, 0.1020, 0.1140]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 1.5309e-20,  2.0093e-20, -1.1468e-19, -1.4141e-19,  1.9136e-21,\n",
       "          4.4013e-21, -3.3629e-20, -5.0012e-20,  3.4865e-11,  1.9191e-10,\n",
       "          2.2677e-10,  2.6164e-10], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0.0010, 0.0020, 0.0030, 0.0040],\n",
       "         [0.0020, 0.0050, 0.0070, 0.0090],\n",
       "         [0.0030, 0.0090, 0.0120, 0.0150],\n",
       "         [0.0040, 0.0140, 0.0180, 0.0220]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 5.6477e-09, -2.1715e-08, -2.3281e-08,  3.5622e-08],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    c_attn.weight -= learning_rate * c_attn.weight.grad\n",
    "    c_attn.bias -= learning_rate * c_attn.bias.grad\n",
    "    c_proj.weight -= learning_rate * c_proj.weight.grad\n",
    "    c_proj.bias -= learning_rate * c_proj.bias.grad\n",
    "c_attn.weight,c_attn.bias, c_proj.weight, c_proj.bias,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819257f1-2327-4688-9a3d-611ed915b679",
   "metadata": {},
   "source": [
    "### Tranformer - Layer Norm 1\n",
    "Now on to the first layer normalization in a transfomer.  Similarly to the other layer normalizations we have both weights and biases to update again.  We can see once again these are exceptionally small gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "50270466-6a75-423d-99bb-6ee57ed86e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.0451e-11,  2.4516e-11, -2.8000e-11, -9.4450e-11]),\n",
       " tensor([-7.8670e-12, -5.5366e-11, -6.3233e-11, -7.1100e-11]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_1.weight.grad, ln_1.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9f1a1bdf-67ad-49a1-b387-3337f1e42555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([7.8670e-12, 5.5366e-11, 6.3233e-11, 7.1100e-11], requires_grad=True))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ln_1.weight -= learning_rate * ln_1.weight.grad\n",
    "    ln_1.bias   -= learning_rate * ln_1.bias.grad\n",
    "ln_1.weight, ln_1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce66d047-cbed-4f71-80e1-8d2c0d2559e2",
   "metadata": {},
   "source": [
    "### Input Layer \n",
    "Now on to the input layer.  Recall that this layer had 2 components: the positional embeddings `wpe` and the token embeddings `wte`. We used *weight tying* to tie our token project `wte` with our head `lm_head` which has already been updated so we do not need to touch it.  We do however still need to update our positional embeddings.   Remember that `wpe` as an embedding has only weights and no biases. We'll see that once again we have relatively small gradients but we can see that they do very a few orders of magnitude showing that even with our small vocab and context, the position are starting to show some impact. \n",
    "\n",
    "*Note that this is the final layer of update we need to make.  Tokenization and data loading is separate from the model and does not impact loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a85197cf-2fb8-42d6-8a6e-b184878050cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.4412e-09,  2.6099e-09,  1.2057e-08, -9.2258e-09],\n",
       "        [ 4.7085e-10,  7.8105e-09,  8.5616e-09,  1.7835e-09],\n",
       "        [ 3.3993e-09, -1.0549e-08, -6.4291e-09,  5.3988e-10],\n",
       "        [ 1.7873e-09, -9.6942e-09, -4.3349e-09,  1.0659e-09],\n",
       "        [ 6.7273e-11,  2.6944e-08,  6.8373e-09, -4.0460e-09],\n",
       "        [ 6.3407e-09, -8.8710e-09, -8.3426e-09, -3.0296e-10],\n",
       "        [ 8.0353e-09, -4.4592e-09,  6.3201e-09, -6.1710e-09],\n",
       "        [-1.9755e-08,  1.7230e-08,  8.4270e-09, -1.8940e-08]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpe.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4de278b6-99aa-4690-86b8-4828664f80d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0100, 0.0200, 0.0300, 0.0400],\n",
       "        [0.0200, 0.0300, 0.0400, 0.0500],\n",
       "        [0.0300, 0.0400, 0.0500, 0.0600],\n",
       "        [0.0400, 0.0500, 0.0600, 0.0700],\n",
       "        [0.0500, 0.0600, 0.0700, 0.0800],\n",
       "        [0.0600, 0.0700, 0.0800, 0.0900],\n",
       "        [0.0700, 0.0800, 0.0900, 0.1000],\n",
       "        [0.0800, 0.0900, 0.1000, 0.1100]], requires_grad=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    wpe.weight -= learning_rate * wpe.weight.grad\n",
    "wpe.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82e17c-b038-4975-8c97-914e5b15a5cf",
   "metadata": {},
   "source": [
    "## Forward Pass with Updated Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a16c16-3b97-4457-9c18-df6c0b8b10ee",
   "metadata": {},
   "source": [
    "Now that we have the updated weights for each layer, let's do another forward pass and compare the loss. Since each layer was previously explained we will instead focus on just showing the outputs of the different layers and the final loss. If you want, you can check the previous outputs in the cached cell outputs above and compare them to see how the weight changes impacted the values at each layer. \n",
    "\n",
    "One key sign that our weights were updated is that you'll see quickly that the values at each layer are no longer repeated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad4b84-b10a-4888-b8ce-3eb3affb6b26",
   "metadata": {},
   "source": [
    "### Data Re-loading\n",
    "Repulling to a new `x_2`. We'll keep `y` to emphasize the same examples are being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ae802f01-b89b-49cf-b833-924287d34c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[35, 15, 32,  9,  5, 20, 30, 15],\n",
       "         [11,  9,  6, 20,  5,  0, 13, 21]]),\n",
       " tensor([[15, 32,  9,  5, 20, 30, 15, 11],\n",
       "         [ 9,  6, 20,  5,  0, 13, 21,  0]]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = tok_for_training[:-1].view(B, T)\n",
    "x_2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb9a710-987c-4be9-9d4d-7a740f56760b",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "Note that in `wte` you can already see the impact of updated weights with the negative values. This was not originally present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6fe50b1e-a64a-4b2c-ba4e-f6103c2ffd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4]) tensor([[0.0100, 0.0200, 0.0300, 0.0400],\n",
      "        [0.0200, 0.0300, 0.0400, 0.0500],\n",
      "        [0.0300, 0.0400, 0.0500, 0.0600],\n",
      "        [0.0400, 0.0500, 0.0600, 0.0700],\n",
      "        [0.0500, 0.0600, 0.0700, 0.0800],\n",
      "        [0.0600, 0.0700, 0.0800, 0.0900],\n",
      "        [0.0700, 0.0800, 0.0900, 0.1000],\n",
      "        [0.0800, 0.0900, 0.1000, 0.1100]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pos = torch.arange(0, T, dtype=torch.long)\n",
    "pos_emb = wpe(pos)\n",
    "print(pos_emb.shape, pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "428da2cc-8bea-493f-af51-9212ed4b5204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4]) tensor([[[ 0.3336,  0.4048,  0.3997,  0.3619],\n",
      "         [ 0.2524,  0.0482,  0.1111,  0.2883],\n",
      "         [ 0.3630,  0.2965,  0.3254,  0.3951],\n",
      "         [ 0.1924, -0.0118,  0.0511,  0.2283],\n",
      "         [ 0.1524, -0.0518,  0.0111,  0.1883],\n",
      "         [ 0.3024,  0.0982,  0.1611,  0.3383],\n",
      "         [ 0.3430,  0.2765,  0.3054,  0.3751],\n",
      "         [ 0.2524,  0.0482,  0.1111,  0.2883]],\n",
      "\n",
      "        [[ 0.1530,  0.0865,  0.1154,  0.1851],\n",
      "         [ 0.1924, -0.0118,  0.0511,  0.2283],\n",
      "         [ 0.1030,  0.0365,  0.0654,  0.1351],\n",
      "         [ 0.3024,  0.0982,  0.1611,  0.3383],\n",
      "         [ 0.1524, -0.0518,  0.0111,  0.1883],\n",
      "         [ 0.1024, -0.1018, -0.0389,  0.1383],\n",
      "         [ 0.1730,  0.1065,  0.1354,  0.2051],\n",
      "         [ 0.2530,  0.1865,  0.2154,  0.2851]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tok_emb = wte(x_2)\n",
    "print(tok_emb.shape, tok_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f6269b91-414c-4180-979a-74b3425eb16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3436,  0.4248,  0.4297,  0.4019],\n",
      "         [ 0.2724,  0.0782,  0.1511,  0.3383],\n",
      "         [ 0.3930,  0.3365,  0.3754,  0.4551],\n",
      "         [ 0.2324,  0.0382,  0.1111,  0.2983],\n",
      "         [ 0.2024,  0.0082,  0.0811,  0.2683],\n",
      "         [ 0.3624,  0.1682,  0.2411,  0.4283],\n",
      "         [ 0.4130,  0.3565,  0.3954,  0.4751],\n",
      "         [ 0.3324,  0.1382,  0.2111,  0.3983]],\n",
      "\n",
      "        [[ 0.1630,  0.1065,  0.1454,  0.2251],\n",
      "         [ 0.2124,  0.0182,  0.0911,  0.2783],\n",
      "         [ 0.1330,  0.0765,  0.1154,  0.1951],\n",
      "         [ 0.3424,  0.1482,  0.2211,  0.4083],\n",
      "         [ 0.2024,  0.0082,  0.0811,  0.2683],\n",
      "         [ 0.1624, -0.0318,  0.0411,  0.2283],\n",
      "         [ 0.2430,  0.1865,  0.2254,  0.3051],\n",
      "         [ 0.3330,  0.2765,  0.3154,  0.3951]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = tok_emb + pos_emb\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2b426-93e4-4072-be08-e15524bc4e8e",
   "metadata": {},
   "source": [
    "### Transformer - Layer Normalization\n",
    "If you recall, the values here were all uniform. The updated weights and changed input now result in changes, showing the impact of our learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "847139dc-0e01-4340-858c-bcb1513d7e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4]) tensor([[[-1.6418,  0.7220,  0.8638,  0.0559],\n",
      "         [ 0.6150, -1.2983, -0.5800,  1.2633],\n",
      "         [ 0.0703, -1.2472, -0.3404,  1.5172],\n",
      "         [ 0.6150, -1.2983, -0.5800,  1.2633],\n",
      "         [ 0.6150, -1.2983, -0.5800,  1.2633],\n",
      "         [ 0.6150, -1.2983, -0.5800,  1.2633],\n",
      "         [ 0.0703, -1.2472, -0.3404,  1.5172],\n",
      "         [ 0.6150, -1.2983, -0.5800,  1.2633]],\n",
      "\n",
      "        [[ 0.0703, -1.2472, -0.3404,  1.5172],\n",
      "         [ 0.6150, -1.2983, -0.5800,  1.2633],\n",
      "         [ 0.0703, -1.2472, -0.3404,  1.5172],\n",
      "         [ 0.6150, -1.2983, -0.5800,  1.2633],\n",
      "         [ 0.6150, -1.2983, -0.5800,  1.2633],\n",
      "         [ 0.6150, -1.2983, -0.5800,  1.2633],\n",
      "         [ 0.0703, -1.2472, -0.3404,  1.5172],\n",
      "         [ 0.0703, -1.2472, -0.3404,  1.5172]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_norm = ln_1(x)\n",
    "print(x_norm.shape, x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a6d83-639b-4215-a91d-5b257aa1d14c",
   "metadata": {},
   "source": [
    "### Transformer - Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "061825ec-08d2-4e93-bd92-17a0cc1d801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 12]) tensor([[[ 0.0026,  0.0069,  0.0128,  0.0203,  0.0295,  0.0403,  0.0528,\n",
      "           0.0669,  0.0827,  0.1001,  0.1191,  0.1398],\n",
      "         [ 0.0013,  0.0020,  0.0021,  0.0016,  0.0005, -0.0012, -0.0036,\n",
      "          -0.0066, -0.0102, -0.0144, -0.0192, -0.0246],\n",
      "         [ 0.0026,  0.0052,  0.0077,  0.0101,  0.0124,  0.0147,  0.0169,\n",
      "           0.0190,  0.0211,  0.0231,  0.0250,  0.0268],\n",
      "         [ 0.0013,  0.0020,  0.0021,  0.0016,  0.0005, -0.0012, -0.0036,\n",
      "          -0.0066, -0.0102, -0.0144, -0.0192, -0.0246],\n",
      "         [ 0.0013,  0.0020,  0.0021,  0.0016,  0.0005, -0.0012, -0.0036,\n",
      "          -0.0066, -0.0102, -0.0144, -0.0192, -0.0246],\n",
      "         [ 0.0013,  0.0020,  0.0021,  0.0016,  0.0005, -0.0012, -0.0036,\n",
      "          -0.0066, -0.0102, -0.0144, -0.0192, -0.0246],\n",
      "         [ 0.0026,  0.0052,  0.0077,  0.0101,  0.0124,  0.0147,  0.0169,\n",
      "           0.0190,  0.0211,  0.0231,  0.0250,  0.0268],\n",
      "         [ 0.0013,  0.0020,  0.0021,  0.0016,  0.0005, -0.0012, -0.0036,\n",
      "          -0.0066, -0.0102, -0.0144, -0.0192, -0.0246]],\n",
      "\n",
      "        [[ 0.0026,  0.0052,  0.0077,  0.0101,  0.0124,  0.0147,  0.0169,\n",
      "           0.0190,  0.0211,  0.0231,  0.0250,  0.0268],\n",
      "         [ 0.0013,  0.0020,  0.0021,  0.0016,  0.0005, -0.0012, -0.0036,\n",
      "          -0.0066, -0.0102, -0.0144, -0.0192, -0.0246],\n",
      "         [ 0.0026,  0.0052,  0.0077,  0.0101,  0.0124,  0.0147,  0.0169,\n",
      "           0.0190,  0.0211,  0.0231,  0.0250,  0.0268],\n",
      "         [ 0.0013,  0.0020,  0.0021,  0.0016,  0.0005, -0.0012, -0.0036,\n",
      "          -0.0066, -0.0102, -0.0144, -0.0192, -0.0246],\n",
      "         [ 0.0013,  0.0020,  0.0021,  0.0016,  0.0005, -0.0012, -0.0036,\n",
      "          -0.0066, -0.0102, -0.0144, -0.0192, -0.0246],\n",
      "         [ 0.0013,  0.0020,  0.0021,  0.0016,  0.0005, -0.0012, -0.0036,\n",
      "          -0.0066, -0.0102, -0.0144, -0.0192, -0.0246],\n",
      "         [ 0.0026,  0.0052,  0.0077,  0.0101,  0.0124,  0.0147,  0.0169,\n",
      "           0.0190,  0.0211,  0.0231,  0.0250,  0.0268],\n",
      "         [ 0.0026,  0.0052,  0.0077,  0.0101,  0.0124,  0.0147,  0.0169,\n",
      "           0.0190,  0.0211,  0.0231,  0.0250,  0.0268]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "B, T, C = x_norm.size()\n",
    "qkv = c_attn(x_norm)\n",
    "print(qkv.shape, qkv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6b00f6b8-35bf-4120-adb5-9cd63130cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4]) tensor([[[ 1.1991e-03,  2.7571e-03,  4.6740e-03,  6.9499e-03],\n",
      "         [ 5.0219e-04,  1.1547e-03,  1.9577e-03,  2.9112e-03],\n",
      "         [ 4.1812e-04,  9.6148e-04,  1.6301e-03,  2.4241e-03],\n",
      "         [ 2.6479e-04,  6.0894e-04,  1.0325e-03,  1.5355e-03],\n",
      "         [ 1.7287e-04,  3.9759e-04,  6.7422e-04,  1.0028e-03],\n",
      "         [ 1.1159e-04,  2.5669e-04,  4.3536e-04,  6.4765e-04],\n",
      "         [ 1.3139e-04,  3.0223e-04,  5.1258e-04,  7.6247e-04],\n",
      "         [ 9.0535e-05,  2.0829e-04,  3.5334e-04,  5.2570e-04]],\n",
      "\n",
      "        [[ 2.4960e-04,  5.7407e-04,  9.7348e-04,  1.4479e-03],\n",
      "         [ 2.7383e-05,  6.3117e-05,  1.0726e-04,  1.5985e-04],\n",
      "         [ 1.0147e-04,  2.3348e-04,  3.9608e-04,  5.8930e-04],\n",
      "         [ 2.7383e-05,  6.3118e-05,  1.0726e-04,  1.5985e-04],\n",
      "         [-1.7061e-05, -3.9076e-05, -6.5987e-05, -9.7759e-05],\n",
      "         [-4.6690e-05, -1.0721e-04, -1.8149e-04, -2.6950e-04],\n",
      "         [-4.3421e-06, -9.8311e-06, -1.6408e-05, -2.4038e-05],\n",
      "         [ 2.7404e-05,  6.3165e-05,  1.0734e-04,  1.5997e-04]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "q,k,v = qkv.split(n_embd, dim=2)\n",
    "q = q.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "k = k.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "v = v.view(B, T, n_head, C // n_head).transpose(1, 2)\n",
    "fa = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "fa = fa.transpose(1, 2).contiguous().view(B, T, C)\n",
    "x_norm = c_proj(fa)\n",
    "print(x_norm.shape, x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f590d9-60cc-4fed-b97e-5290b54856f6",
   "metadata": {},
   "source": [
    "### Transformer - Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0b1e8544-1e80-4efd-99bf-a00953be4ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4]) tensor([[[ 0.3448,  0.4276,  0.4344,  0.4089],\n",
      "         [ 0.2730,  0.0793,  0.1531,  0.3412],\n",
      "         [ 0.3934,  0.3374,  0.3770,  0.4575],\n",
      "         [ 0.2327,  0.0388,  0.1121,  0.2998],\n",
      "         [ 0.2026,  0.0086,  0.0818,  0.2693],\n",
      "         [ 0.3626,  0.1684,  0.2415,  0.4289],\n",
      "         [ 0.4131,  0.3568,  0.3959,  0.4759],\n",
      "         [ 0.3325,  0.1384,  0.2115,  0.3988]],\n",
      "\n",
      "        [[ 0.1633,  0.1071,  0.1464,  0.2265],\n",
      "         [ 0.2125,  0.0182,  0.0912,  0.2784],\n",
      "         [ 0.1331,  0.0767,  0.1158,  0.1957],\n",
      "         [ 0.3425,  0.1482,  0.2212,  0.4084],\n",
      "         [ 0.2024,  0.0081,  0.0810,  0.2682],\n",
      "         [ 0.1624, -0.0319,  0.0409,  0.2280],\n",
      "         [ 0.2430,  0.1865,  0.2254,  0.3051],\n",
      "         [ 0.3330,  0.2766,  0.3155,  0.3953]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = x + x_norm\n",
    "print(x.shape, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b725c-ca56-4011-a8d7-0ba05441edd1",
   "metadata": {},
   "source": [
    "### Transformer - Layer Normalization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9653f04-ba59-4025-9773-fd77e4f466e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4]) tensor([[[-1.6641,  0.6664,  0.8576,  0.1401],\n",
      "         [ 0.6019, -1.2987, -0.5748,  1.2716],\n",
      "         [ 0.0479, -1.2422, -0.3302,  1.5246],\n",
      "         [ 0.6081, -1.2985, -0.5772,  1.2677],\n",
      "         [ 0.6105, -1.2985, -0.5782,  1.2662],\n",
      "         [ 0.6121, -1.2984, -0.5788,  1.2652],\n",
      "         [ 0.0632, -1.2456, -0.3372,  1.5196],\n",
      "         [ 0.6126, -1.2984, -0.5790,  1.2648]],\n",
      "\n",
      "        [[ 0.0568, -1.2442, -0.3343,  1.5217],\n",
      "         [ 0.6143, -1.2983, -0.5797,  1.2637],\n",
      "         [ 0.0648, -1.2460, -0.3379,  1.5191],\n",
      "         [ 0.6143, -1.2983, -0.5797,  1.2637],\n",
      "         [ 0.6154, -1.2983, -0.5801,  1.2630],\n",
      "         [ 0.6162, -1.2983, -0.5804,  1.2625],\n",
      "         [ 0.0705, -1.2472, -0.3405,  1.5172],\n",
      "         [ 0.0688, -1.2469, -0.3397,  1.5177]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_norm_2 = ln_2(x)\n",
    "print(x_norm_2.shape, x_norm_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575539a-048b-49ae-91f9-b3365472b8b3",
   "metadata": {},
   "source": [
    "### Transformer - Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1866a5f6-0961-4e9b-b858-950911d44d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4]) tensor([[[ 0.2124, -0.1642, -0.0980,  0.1571],\n",
      "         [ 0.2018, -0.1862, -0.1324,  0.1096],\n",
      "         [ 0.2048, -0.1800, -0.1227,  0.1229],\n",
      "         [ 0.2018, -0.1863, -0.1325,  0.1095],\n",
      "         [ 0.2018, -0.1863, -0.1325,  0.1094],\n",
      "         [ 0.2017, -0.1863, -0.1325,  0.1094],\n",
      "         [ 0.2047, -0.1802, -0.1230,  0.1226],\n",
      "         [ 0.2017, -0.1863, -0.1325,  0.1094]],\n",
      "\n",
      "        [[ 0.2047, -0.1801, -0.1229,  0.1227],\n",
      "         [ 0.2017, -0.1863, -0.1326,  0.1093],\n",
      "         [ 0.2047, -0.1802, -0.1230,  0.1225],\n",
      "         [ 0.2017, -0.1863, -0.1326,  0.1093],\n",
      "         [ 0.2017, -0.1863, -0.1326,  0.1093],\n",
      "         [ 0.2017, -0.1863, -0.1326,  0.1093],\n",
      "         [ 0.2046, -0.1803, -0.1231,  0.1224],\n",
      "         [ 0.2046, -0.1803, -0.1231,  0.1224]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_mlp = mlp_fc(x_norm_2)\n",
    "x_mlp = mlp_gelu(x_mlp)\n",
    "x_mlp = mlp_proj(x_mlp)\n",
    "print(x_mlp.shape, x_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aeb5b3-e7be-480c-bb84-27ead9591531",
   "metadata": {},
   "source": [
    "### Transformer - Residual (skip) connection 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ecbd7463-e9c3-48fe-9afc-733ab67f7f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4]) tensor([[[ 0.5572,  0.2634,  0.3363,  0.5660],\n",
      "         [ 0.4747, -0.1069,  0.0207,  0.4508],\n",
      "         [ 0.5982,  0.1574,  0.2543,  0.5805],\n",
      "         [ 0.4345, -0.1475, -0.0203,  0.4093],\n",
      "         [ 0.4044, -0.1777, -0.0507,  0.3787],\n",
      "         [ 0.5643, -0.0179,  0.1090,  0.5383],\n",
      "         [ 0.6178,  0.1766,  0.2729,  0.5984],\n",
      "         [ 0.5343, -0.0479,  0.0789,  0.5082]],\n",
      "\n",
      "        [[ 0.3680, -0.0731,  0.0235,  0.3493],\n",
      "         [ 0.4142, -0.1681, -0.0414,  0.3878],\n",
      "         [ 0.3378, -0.1035, -0.0072,  0.3182],\n",
      "         [ 0.5442, -0.0381,  0.0886,  0.5178],\n",
      "         [ 0.4042, -0.1782, -0.0515,  0.3775],\n",
      "         [ 0.3641, -0.2183, -0.0917,  0.3373],\n",
      "         [ 0.4476,  0.0062,  0.1022,  0.4275],\n",
      "         [ 0.5377,  0.0963,  0.1924,  0.5177]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = x_mlp + x\n",
    "print(x.shape, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236268f-7eee-465f-87c2-6cf308db70a2",
   "metadata": {},
   "source": [
    "### Transformer - Final Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "191ca349-ee8a-4cd1-8995-32a00d19f84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4]) tensor([[[ 0.8657, -1.3647, -0.7721,  0.9262],\n",
      "         [ 0.9450, -1.3417, -0.8013,  0.8533],\n",
      "         [ 0.9439, -1.3420, -0.8009,  0.8543],\n",
      "         [ 0.9474, -1.3409, -0.8022,  0.8510],\n",
      "         [ 0.9483, -1.3406, -0.8025,  0.8501],\n",
      "         [ 0.9490, -1.3404, -0.8027,  0.8495],\n",
      "         [ 0.9481, -1.3406, -0.8023,  0.8502],\n",
      "         [ 0.9492, -1.3404, -0.8028,  0.8493]],\n",
      "\n",
      "        [[ 0.9464, -1.3412, -0.8017,  0.8519],\n",
      "         [ 0.9498, -1.3401, -0.8030,  0.8487],\n",
      "         [ 0.9486, -1.3405, -0.8025,  0.8498],\n",
      "         [ 0.9498, -1.3401, -0.8030,  0.8487],\n",
      "         [ 0.9502, -1.3400, -0.8031,  0.8483],\n",
      "         [ 0.9505, -1.3399, -0.8033,  0.8480],\n",
      "         [ 0.9501, -1.3400, -0.8030,  0.8483],\n",
      "         [ 0.9496, -1.3401, -0.8029,  0.8488]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = ln_f(x)\n",
    "print(x.shape, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f5e19-72c4-41f5-9442-9d6f777730f1",
   "metadata": {},
   "source": [
    "### Output Layers AKA Model Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4919d39b-1465-44e8-8be6-36e535bc8c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 36]) tensor([[[ 0.3858, -0.1198, -0.1232, -0.1267, -0.1301,  0.3685,  0.1140,\n",
      "          -0.1405, -0.1439,  0.3547, -0.1508,  0.0968, -0.1577,  0.0899,\n",
      "          -0.1646,  0.3340, -0.1715, -0.1750, -0.1784, -0.1819,  0.3168,\n",
      "           0.0623, -0.1922, -0.1957, -0.1991, -0.2026, -0.2060, -0.2095,\n",
      "          -0.2129, -0.2164,  0.0312, -0.2233,  0.0243, -0.2302, -0.2336,\n",
      "          -0.2370],\n",
      "         [ 0.3826, -0.1221, -0.1256, -0.1290, -0.1325,  0.3654,  0.1113,\n",
      "          -0.1428, -0.1463,  0.3516, -0.1532,  0.0940, -0.1601,  0.0871,\n",
      "          -0.1669,  0.3309, -0.1738, -0.1773, -0.1807, -0.1842,  0.3137,\n",
      "           0.0596, -0.1945, -0.1980, -0.2014, -0.2049, -0.2083, -0.2118,\n",
      "          -0.2152, -0.2186,  0.0286, -0.2255,  0.0217, -0.2324, -0.2359,\n",
      "          -0.2393],\n",
      "         [ 0.3826, -0.1221, -0.1255, -0.1290, -0.1324,  0.3654,  0.1113,\n",
      "          -0.1428, -0.1462,  0.3516, -0.1531,  0.0941, -0.1600,  0.0872,\n",
      "          -0.1669,  0.3309, -0.1738, -0.1772, -0.1807, -0.1841,  0.3137,\n",
      "           0.0596, -0.1945, -0.1979, -0.2014, -0.2048, -0.2083, -0.2117,\n",
      "          -0.2152, -0.2186,  0.0286, -0.2255,  0.0217, -0.2324, -0.2358,\n",
      "          -0.2393],\n",
      "         [ 0.3825, -0.1222, -0.1256, -0.1291, -0.1325,  0.3653,  0.1112,\n",
      "          -0.1429, -0.1463,  0.3515, -0.1532,  0.0940, -0.1601,  0.0871,\n",
      "          -0.1670,  0.3308, -0.1739, -0.1773, -0.1808, -0.1842,  0.3136,\n",
      "           0.0595, -0.1946, -0.1980, -0.2015, -0.2049, -0.2084, -0.2118,\n",
      "          -0.2153, -0.2187,  0.0285, -0.2256,  0.0216, -0.2325, -0.2359,\n",
      "          -0.2394],\n",
      "         [ 0.3824, -0.1222, -0.1257, -0.1291, -0.1326,  0.3652,  0.1111,\n",
      "          -0.1429, -0.1464,  0.3514, -0.1532,  0.0939, -0.1601,  0.0870,\n",
      "          -0.1670,  0.3307, -0.1739, -0.1774, -0.1808, -0.1843,  0.3135,\n",
      "           0.0595, -0.1946, -0.1981, -0.2015, -0.2049, -0.2084, -0.2118,\n",
      "          -0.2153, -0.2187,  0.0284, -0.2256,  0.0215, -0.2325, -0.2360,\n",
      "          -0.2394],\n",
      "         [ 0.3824, -0.1222, -0.1257, -0.1291, -0.1326,  0.3652,  0.1111,\n",
      "          -0.1429, -0.1464,  0.3514, -0.1533,  0.0939, -0.1602,  0.0870,\n",
      "          -0.1670,  0.3307, -0.1739, -0.1774, -0.1808, -0.1843,  0.3135,\n",
      "           0.0594, -0.1946, -0.1981, -0.2015, -0.2050, -0.2084, -0.2119,\n",
      "          -0.2153, -0.2187,  0.0284, -0.2256,  0.0215, -0.2325, -0.2360,\n",
      "          -0.2394],\n",
      "         [ 0.3824, -0.1222, -0.1257, -0.1291, -0.1326,  0.3652,  0.1111,\n",
      "          -0.1429, -0.1463,  0.3514, -0.1532,  0.0939, -0.1601,  0.0870,\n",
      "          -0.1670,  0.3307, -0.1739, -0.1774, -0.1808, -0.1843,  0.3135,\n",
      "           0.0595, -0.1946, -0.1980, -0.2015, -0.2049, -0.2084, -0.2118,\n",
      "          -0.2153, -0.2187,  0.0284, -0.2256,  0.0215, -0.2325, -0.2359,\n",
      "          -0.2394],\n",
      "         [ 0.3824, -0.1222, -0.1257, -0.1291, -0.1326,  0.3652,  0.1111,\n",
      "          -0.1429, -0.1464,  0.3514, -0.1533,  0.0939, -0.1602,  0.0870,\n",
      "          -0.1671,  0.3307, -0.1739, -0.1774, -0.1808, -0.1843,  0.3135,\n",
      "           0.0594, -0.1946, -0.1981, -0.2015, -0.2050, -0.2084, -0.2119,\n",
      "          -0.2153, -0.2187,  0.0284, -0.2256,  0.0215, -0.2325, -0.2360,\n",
      "          -0.2394]],\n",
      "\n",
      "        [[ 0.3825, -0.1222, -0.1256, -0.1291, -0.1325,  0.3653,  0.1112,\n",
      "          -0.1428, -0.1463,  0.3515, -0.1532,  0.0940, -0.1601,  0.0871,\n",
      "          -0.1670,  0.3308, -0.1739, -0.1773, -0.1808, -0.1842,  0.3136,\n",
      "           0.0595, -0.1945, -0.1980, -0.2014, -0.2049, -0.2083, -0.2118,\n",
      "          -0.2152, -0.2187,  0.0285, -0.2256,  0.0216, -0.2325, -0.2359,\n",
      "          -0.2394],\n",
      "         [ 0.3824, -0.1223, -0.1257, -0.1292, -0.1326,  0.3651,  0.1111,\n",
      "          -0.1429, -0.1464,  0.3513, -0.1533,  0.0939, -0.1602,  0.0870,\n",
      "          -0.1671,  0.3307, -0.1740, -0.1774, -0.1809, -0.1843,  0.3134,\n",
      "           0.0594, -0.1946, -0.1981, -0.2015, -0.2050, -0.2084, -0.2119,\n",
      "          -0.2153, -0.2188,  0.0284, -0.2257,  0.0215, -0.2326, -0.2360,\n",
      "          -0.2394],\n",
      "         [ 0.3824, -0.1222, -0.1257, -0.1291, -0.1326,  0.3652,  0.1111,\n",
      "          -0.1429, -0.1464,  0.3514, -0.1532,  0.0939, -0.1601,  0.0870,\n",
      "          -0.1670,  0.3307, -0.1739, -0.1774, -0.1808, -0.1843,  0.3135,\n",
      "           0.0594, -0.1946, -0.1980, -0.2015, -0.2049, -0.2084, -0.2118,\n",
      "          -0.2153, -0.2187,  0.0284, -0.2256,  0.0215, -0.2325, -0.2360,\n",
      "          -0.2394],\n",
      "         [ 0.3824, -0.1223, -0.1257, -0.1292, -0.1326,  0.3651,  0.1111,\n",
      "          -0.1429, -0.1464,  0.3513, -0.1533,  0.0939, -0.1602,  0.0870,\n",
      "          -0.1671,  0.3307, -0.1740, -0.1774, -0.1809, -0.1843,  0.3134,\n",
      "           0.0594, -0.1946, -0.1981, -0.2015, -0.2050, -0.2084, -0.2119,\n",
      "          -0.2153, -0.2188,  0.0284, -0.2257,  0.0215, -0.2326, -0.2360,\n",
      "          -0.2394],\n",
      "         [ 0.3823, -0.1223, -0.1257, -0.1292, -0.1326,  0.3651,  0.1111,\n",
      "          -0.1430, -0.1464,  0.3513, -0.1533,  0.0938, -0.1602,  0.0870,\n",
      "          -0.1671,  0.3306, -0.1740, -0.1774, -0.1809, -0.1843,  0.3134,\n",
      "           0.0594, -0.1947, -0.1981, -0.2015, -0.2050, -0.2084, -0.2119,\n",
      "          -0.2153, -0.2188,  0.0284, -0.2257,  0.0215, -0.2326, -0.2360,\n",
      "          -0.2395],\n",
      "         [ 0.3823, -0.1223, -0.1257, -0.1292, -0.1326,  0.3651,  0.1111,\n",
      "          -0.1430, -0.1464,  0.3513, -0.1533,  0.0938, -0.1602,  0.0869,\n",
      "          -0.1671,  0.3306, -0.1740, -0.1774, -0.1809, -0.1843,  0.3134,\n",
      "           0.0594, -0.1947, -0.1981, -0.2016, -0.2050, -0.2084, -0.2119,\n",
      "          -0.2153, -0.2188,  0.0284, -0.2257,  0.0215, -0.2326, -0.2360,\n",
      "          -0.2395],\n",
      "         [ 0.3823, -0.1223, -0.1257, -0.1292, -0.1326,  0.3651,  0.1111,\n",
      "          -0.1429, -0.1464,  0.3513, -0.1533,  0.0938, -0.1602,  0.0869,\n",
      "          -0.1671,  0.3306, -0.1740, -0.1774, -0.1809, -0.1843,  0.3134,\n",
      "           0.0594, -0.1946, -0.1981, -0.2015, -0.2050, -0.2084, -0.2119,\n",
      "          -0.2153, -0.2188,  0.0284, -0.2257,  0.0215, -0.2325, -0.2360,\n",
      "          -0.2394],\n",
      "         [ 0.3823, -0.1223, -0.1257, -0.1291, -0.1326,  0.3651,  0.1111,\n",
      "          -0.1429, -0.1464,  0.3513, -0.1533,  0.0939, -0.1602,  0.0870,\n",
      "          -0.1671,  0.3307, -0.1740, -0.1774, -0.1808, -0.1843,  0.3134,\n",
      "           0.0594, -0.1946, -0.1981, -0.2015, -0.2050, -0.2084, -0.2119,\n",
      "          -0.2153, -0.2188,  0.0284, -0.2256,  0.0215, -0.2325, -0.2360,\n",
      "          -0.2394]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = lm_head(x)\n",
    "print(logits.shape, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da577f44-a315-47d3-be3b-f30b8d7ad9a1",
   "metadata": {},
   "source": [
    "### Updated Loss calculation\n",
    "Now we'll calculate the updated loss.  Our first pass's loss was 3.5835. Since we're passing through the same example and used a fairly high learning rate we should see a significant improvement with just 1 learning pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "92956b47-d4d9-4f2d-97ae-fe17ad93ce97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5835, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0a64b50c-6eb5-4050-8930-bae485741d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) tensor(3.2931, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_flat = y.view(-1)\n",
    "logits_flat = logits.view(-1, logits.size(-1))\n",
    "updated_loss = F.cross_entropy(logits_flat, y_flat)\n",
    "print(updated_loss.shape, updated_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2ef0b062-6dd0-4ccd-adb4-3e24bb075980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 round of training resulted in an loss improvment of 0.2904'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'1 round of training resulted in an loss improvment of {loss.item() - updated_loss.item():.4f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4532f32-85a4-4260-bf1f-f2c6f14e31f8",
   "metadata": {},
   "source": [
    "# SUCCESS!\n",
    "Our training improved the loss by almost **7%**. There are flaws with this, mainly passing the same example through a second time, but this helps show the fundamentals of what learning does inside a GPT-2 style model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35f747-322a-4a99-8429-841bd6a338ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
