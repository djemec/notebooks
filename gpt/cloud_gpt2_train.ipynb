{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b9eae6-e5e7-4d57-8f89-d83d77aa91bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff856719-149e-4976-aebf-9e4e38979bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# distributed training\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83823c2-8f2f-4e04-983e-79822e1a224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug  4 21:56:47 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   33C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94cae7e-b668-4a79-8bb8-5468986672a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)\n",
    "data_path = Path('/home/ubuntu/data')\n",
    "log_path = Path('/home/ubuntu/log')\n",
    "model_path = Path('/home/ubuntu/model')\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    # elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    #     device = 'mps'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32912c63-ab50-48a4-8a3c-bf57cb67fdae",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d55293c-13e7-473f-bab8-81a175d9c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # ensures that you can split embeddings across the heads\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projection for all heads in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, n_embd embedding dimensionality \n",
    "        # calculate query, key, value for all heads in batch, then move head forward\n",
    "        # nh - num heads, hs - head size, C  nh*hs aka channels\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # attention (materializes the large (T,T) matrix for all the queries and keys)\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v # (B, nh, T, T) X (B, nh, T, hs) - > (B, nh, T, hs)\n",
    "        # replace attention with flash attention \n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # allows for pathway to pass through gradients instead of going through each \"box\"\n",
    "        # this is a feed forward network\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length/context\n",
    "    vocab_size: int = 50257 # num of tokens, 50k merges, 256 bytes, 1 EOT\n",
    "    n_layer: int = 12 \n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # weight tokenizer element\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # weight position element\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # hidden layers aka Transformers\n",
    "            ln_f = nn.LayerNorm(config.n_embd), #log normalization \n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # language model head going from embeddings to vocab\n",
    "\n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        mean = 0.0\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=mean, std=std)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f'Cannot forward sequence, out of context'\n",
    "        # forward the token and positions\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the block\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and head\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        if master_process:\n",
    "            print(f'num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters')\n",
    "            print(f'num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters')\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        if master_process:\n",
    "            print(f'using fused AdamW: {use_fused}')\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48217319-530f-4d86-9373-688e1ad20d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "    print(f'loading {filename}')\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, process_rank, num_processes, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in {'train','test','val'}\n",
    "\n",
    "        # get the shard filenames\n",
    "        data_root = data_path / f'{split}'\n",
    "        shards = list(data_root.iterdir())\n",
    "        self.shards = sorted(shards)\n",
    "        assert len(shards) > 0, f'no shards found for split {split}'\n",
    "        if master_process:\n",
    "            print(f'found {len(shards)} shards for split {split}')\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # state, init at shard zero\n",
    "        self.remaining_shards = self.shards\n",
    "        random.shuffle(self.remaining_shards)\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.remaining_shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        if len(buf) < B * T + 1:\n",
    "            self.current_shard += 1\n",
    "            if self.current_shard >= len(self.remaining_shards):\n",
    "                self.reset()\n",
    "            self.tokens = load_tokens(self.remaining_shards[self.current_shard])\n",
    "            self.current_position = B * T * self.process_rank\n",
    "            return self.next_batch()\n",
    "        x = (buf[:-1]).view(B, T)\n",
    "        y = (buf[1:]).view(B, T)\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce223b4f-f8ac-4516-9e45-3bee0c9bf5db",
   "metadata": {},
   "source": [
    "## Setup Distributed Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1affdbf8-fd8c-4938-a010-84f2d943c750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not using ddp, using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# set up DDP (distributed data parallel).\n",
    "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
    "device_type = device # override device if using ddp do device_type acts as backup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
    "    assert torch.cuda.is_available(), 'for now i think we need CUDA for DDP'\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "else:\n",
    "    # vanilla, non-DDP run\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    print(f'not using ddp, using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5dd887-eab0-46a0-9f62-0fe9a97b9468",
   "metadata": {},
   "source": [
    "## Create model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da37ac22-1031-4410-a046-bb87a577a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 131072\n",
      "=> calculated gradient accumulation steps: 32\n",
      "found 22 shards for split train\n",
      "loading /home/ubuntu/data/train/train_wikitext-103_000010.npy\n",
      "found 1 shards for split test\n",
      "loading /home/ubuntu/data/test/test_wikitext-103_000000.npy\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 2**17 #524288 # 2**19, ~0.5M, in number of tokens, made smaller for testing \n",
    "B = 4 # micro batch size\n",
    "T = 1024 # sequence length\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, 'make sure total_batch_size is divisible by B * T * ddp_world_size'\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "if master_process:\n",
    "    print(f'total desired batch size: {total_batch_size}')\n",
    "    print(f'=> calculated gradient accumulation steps: {grad_accum_steps}')\n",
    "\n",
    "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split='train')\n",
    "test_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split='test')\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=2**17)) # make divisible by power of 2 was 50304\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "    \n",
    "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40477f04-4281-49a5-8f72-024d87a81fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 1e-3\n",
    "min_lr = max_lr * 0.1\n",
    "max_steps = 100 # to be adjusted in a bit, should be ~ 1-2 epochs so total training tokens / batch size\n",
    "warmup_steps = 0.05 * max_steps # 5% warmup\n",
    "weight_decay = 0.1\n",
    "def get_lr(it):\n",
    "    # 1/ linear warmup \n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "    # 2/ if iterations > lr_decay_iters, return min learning rate\n",
    "    if it > max_steps: \n",
    "        return min_lr\n",
    "\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a87d6fd-1a76-4a86-b43d-3a13818d5801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 186,384,384 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=min_lr, betas=(0.9, 0.95), eps=1e-8)\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=max_lr, device_type=device_type)\n",
    "enc = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b55e4563-bc69-4a4e-ab29-1fd161835603",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = log_path / 'log.txt'\n",
    "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc6e0e-c232-4985-af9d-234c729e3b6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /home/ubuntu/data/test/test_wikitext-103_000000.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0804 21:57:49.588000 8559 site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 11.9658\n",
      "step     0 | loss: 11.954853 | lr 2.0000e-04| dt: 75169.09ms | tok/sec: 1743.70\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == max_steps - 1)\n",
    "    \n",
    "    # once in a while evaluate our test loss\n",
    "    if step % 10 == 0 or last_step:\n",
    "        model.eval()\n",
    "        test_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            test_loss_accum = 0.0\n",
    "            test_loss_steps = 20\n",
    "            for _ in range(test_loss_steps):\n",
    "                x, y = test_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                #with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x, y)\n",
    "                loss = loss / test_loss_steps\n",
    "                test_loss_accum += loss.detach()\n",
    "        if ddp:\n",
    "            dist.all_reduce(test_loss_accum, op=dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f'test loss: {test_loss_accum.item():.4f}')\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f'{step} test {test_loss_accum.item():.4f}\\n')\n",
    "            if step > 0 or last_step:\n",
    "                # optionally write model checkpoints\n",
    "                checkpoint_path = model_path / f'model_{step:05d}.pt'\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'config': raw_model.config,\n",
    "                    'step': step,\n",
    "                    'test_loss': test_loss_accum.item()\n",
    "                }\n",
    "                # you might also want to add optimizer.state_dict() and\n",
    "                # rng seeds etc., if you wanted to more exactly resume training\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # once in a while generate from the model (except step 0, which is noise)\n",
    "    if ((step > 0 and step % 10 == 0) or last_step):\n",
    "        model.eval()\n",
    "        num_return_sequences = 2\n",
    "        max_length = 32\n",
    "        tokens = enc.encode('Hello, I\\'m a language model,')\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "        xgen = tokens.to(device)\n",
    "        sample_rng = torch.Generator(device=device)\n",
    "        sample_rng.manual_seed(42 + ddp_rank)\n",
    "        while xgen.size(1) < max_length:\n",
    "            # forward the model to get the logits\n",
    "            with torch.no_grad():\n",
    "                #with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "                # take the logits at the last position\n",
    "                logits = logits[:, -1, :] # (B, vocab_size)\n",
    "                # get the probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                # do top-k sampling of 50 (huggingface pipeline default)\n",
    "                # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "                # select a token from the top-k probabilities\n",
    "                # note: multinomial does not demand the input to sum to 1\n",
    "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "                # gather the corresponding indices\n",
    "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "                # append to the sequence\n",
    "                xgen = torch.cat((xgen, xcol), dim=1)\n",
    "        # print the generated text\n",
    "        for i in range(num_return_sequences):\n",
    "            tokens = xgen[i, :max_length].tolist()\n",
    "            decoded = enc.decode(tokens)\n",
    "            print(f'rank {ddp_rank} sample {i}: {decoded}')\n",
    "\n",
    "    # do one step of the optimization\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # added after video, this field is also used by the forward pass.\n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
    "        #with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    if ddp:\n",
    "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    if device_type == 'cuda':\n",
    "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0 # time difference in seconds\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    if master_process:\n",
    "        lossi.append(loss_accum.item())\n",
    "        print(f'step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e}| dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}')\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2c75b-cf44-497e-83dd-a004367dbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e2f890-77b4-4248-b101-8ed73a000122",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918ef3d-93f3-407c-8572-cb38875e782a",
   "metadata": {},
   "source": [
    "## Manual model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee0b4c-e63a-4804-9770-b802829bc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual Save\n",
    "# step = 12\n",
    "# cp_path = model_path / f'model_{step:05d}.pt'\n",
    "# checkpoint = {\n",
    "#     'model': model.state_dict(),\n",
    "#     'config': model.config,\n",
    "#     'step': step, \n",
    "#     'val_loss': loss_accum.item()\n",
    "# }\n",
    "# torch.save(checkpoint, cp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c5bce-5115-4c6f-8afb-f45abef3d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332c42c-da62-4a86-9706-9331e0cf30c6",
   "metadata": {},
   "source": [
    "### Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfbeb1-ebb7-42df-baed-724eba8dbdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "num_return_sequences = 2\n",
    "max_length = 32\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode('Hello, I\\'m a language model,')\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "xgen = tokens.to(device)\n",
    "sample_rng = torch.Generator(device=device)\n",
    "sample_rng.manual_seed(42)\n",
    "while xgen.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(xgen)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # limit tensor size by sampling\n",
    "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"sample {i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b90ee8-6a38-47fa-8937-49b7696b7640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
