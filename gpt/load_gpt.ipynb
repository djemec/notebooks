{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b74864c-cc81-412f-a4f9-860bc041a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d6c24-755f-44af-a47e-caaee0061c67",
   "metadata": {},
   "source": [
    "### Default Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229b6370-d6d3-4097-980d-05b073f2d3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "random.seed(1337)\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(1337)\n",
    "        device = 'cuda'\n",
    "    # elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    #     device = 'mps'\n",
    "    print(f'using {device}')\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c51ac-ffcd-470d-babd-66714fea5a01",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ff081f9-9626-4ac0-9bfd-667a56fcbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # ensures that you can split embeddings across the heads\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projection for all heads in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, n_embd embedding dimensionality \n",
    "        # calculate query, key, value for all heads in batch, then move head forward\n",
    "        # nh - num heads, hs - head size, C  nh*hs aka channels\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # attention (materializes the large (T,T) matrix for all the queries and keys)\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v # (B, nh, T, T) X (B, nh, T, hs) - > (B, nh, T, hs)\n",
    "        # replace attention with flash attention \n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # allows for pathway to pass through gradients instead of going through each \"box\"\n",
    "        # this is a feed forward network\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length/context\n",
    "    vocab_size: int = 100276 # switched to GP4 tokenizer \n",
    "    n_layer: int = 12 \n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # weight tokenizer element\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # weight position element\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # hidden layers aka Transformers\n",
    "            ln_f = nn.LayerNorm(config.n_embd), #log normalization \n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # language model head going from embeddings to vocab\n",
    "\n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        mean = 0.0\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=mean, std=std)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f'Cannot forward sequence, out of context'\n",
    "        # forward the token and positions\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the block\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and head\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        if master_process:\n",
    "            print(f'num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters')\n",
    "            print(f'num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters')\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        if master_process:\n",
    "            print(f'using fused AdamW: {use_fused}')\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, ckpt_path, device):\n",
    "        ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "        cfg = ckpt['config'] if isinstance(ckpt['config'], GPTConfig) else GPTConfig(**ckpt['config'])\n",
    "        print(f'training steps {ckpt['step']}  |  test loss {ckpt['test_loss']:.4f}')\n",
    "        \n",
    "        def _clean(name):\n",
    "            if name.startswith('_orig_mod.'):\n",
    "                name = name[len('_orig_mod.'):]\n",
    "            if name.startswith('module.'):\n",
    "                name = name[len('module.'):]\n",
    "            return name\n",
    "\n",
    "        raw_sd   = ckpt['model']\n",
    "        clean_sd = { _clean(k): v for k, v in raw_sd.items() }\n",
    "        \n",
    "        model = cls(cfg).to(device)\n",
    "        model.load_state_dict(clean_sd, strict=True)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144801b-73f1-4197-9be9-b40364e42d98",
   "metadata": {},
   "source": [
    "## setup configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d55f343a-dde7-4638-8eb2-f91846d19135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not using ddp, using device: cpu\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# set up DDP (distributed data parallel).\n",
    "device_type = device # override device if using ddp do device_type acts as backup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
    "    assert torch.cuda.is_available(), 'for now i think we need CUDA for DDP'\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "else:\n",
    "    # vanilla, non-DDP run\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    print(f'not using ddp, using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc98e093-eae0-4b29-acc0-4832b07ece18",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "```\n",
    "{\n",
    "    'model': raw_model.state_dict(),\n",
    "    'config': raw_model.config,\n",
    "    'step': step,\n",
    "    'test_loss': test_loss_accum.item()\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "318f85d5-caa6-42b2-9cb4-7f5421353b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training steps 9999  |  test loss 3.0506\n"
     ]
    }
   ],
   "source": [
    "model_path = Path('~/code/gpt/model_final.pt').expanduser() \n",
    "model  = GPT.from_pretrained(model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eac8a9de-d0b1-4ff9-b26a-ccd33be65725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 0 sample 0: Hello, I'm a language model, and we do it all the time. You can't learn anything about speech without reading some books. And you can't learn anything about language without reading a book.\n",
      "That is the big deal. This is why it should be a big deal.\n",
      "The biggest reason is that you will never see the speaker's words in general on the first day you go over. If you can't read anything, then if you'll almost sure not read enough. Because you'll\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_return_sequences = 1\n",
    "max_length = 100\n",
    "tokens = enc.encode('Hello, I\\'m a language model,')\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "xgen = tokens.to(device)\n",
    "sample_rng = torch.Generator(device=device)\n",
    "sample_rng.manual_seed(42 + random.randint(0,9999))\n",
    "while xgen.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "            logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f'rank {ddp_rank} sample {i}: {decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a9513-ac09-44db-8010-061c488c58e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
