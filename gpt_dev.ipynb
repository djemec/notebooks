{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f57eae-6a20-418a-be94-738505161363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49d8eabb-8520-4513-b95e-4337ac53b571",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16cdec6e-5870-4b8d-8a74-e2b007f216a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1153b9db0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32 #examples at once\n",
    "block_size = 64 # max context\n",
    "max_iters = 5000\n",
    "eval_interval = 25\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "eval_iters = 200\n",
    "training_split = 0.9\n",
    "n_embd = 48\n",
    "n_heads = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41e9d71-040d-49b7-9195-5a086ce3611f",
   "metadata": {},
   "source": [
    "## Load data to run tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3e3478-5ec3-4149-bd33-31db9d584bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data = Path('/Users/djemec/data/gpt_train_data/tiny_stories/ts_v2_train.txt')\n",
    "#validation_data = Path('/Users/djemec/data/gpt_train_data/tiny_stories/ts_v2_valid.txt')\n",
    "shake_data = Path('/Users/djemec/data/gpt_train_data/tiny_shakespeare/input.txt')\n",
    "\n",
    "with open(shake_data, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ba43f0-6d58-4db7-a8b9-6d898434cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build out unique characters for the test\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf21a1-8485-44d7-814e-ddf6d58ae911",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "First we'll show an example but then base it off of tiktoken which is BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32314b2d-80cd-4307-866b-6ed4f7d6bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "# enc = tiktoken.encoding_for_model('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bec78a9-bac1-4edb-86f4-fef5fd0ca5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate small batches of data of input X and output Y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def generate_n_ex(m, max_new_tokens):\n",
    "    # start with 0,0 which is initial character and generate\n",
    "    context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "    generated_tokens = m.generate(context, max_new_tokens=max_new_tokens)\n",
    "    # decode token to characters\n",
    "    print(decode(generated_tokens[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c90cf0-982b-4d59-b466-63124f73c980",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97b5c40-d9b7-45a2-908a-7e5ff6ed396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    ''' one head of self attention '''\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B,T,C) @ (B,C,T) --> (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # decoder block to make sure it only knows the past, not future\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' multiple heads of self attention in parallel'''\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out =  torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "        \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    ''' simple linear layer followed by non-linearity'''\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566d734b-b0e6-4fd6-854f-5e4f88143695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    ''' transformer block: communication followed by computation'''\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "301c686a-519e-4cfa-8119-95682d561417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_heads) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx) # B,T,C\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T, C\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # B, T, vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # stretch out the array\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff6db38-9eed-4636-83d7-555d8ca2544f",
   "metadata": {},
   "source": [
    "## load data & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e5d788-61b4-4b94-8058-445389910f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(training_split*len(data))\n",
    "train_data = data[:n] \n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3790229d-1200-4996-a3df-26b2df7f2e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.178241 M parameters\n"
     ]
    }
   ],
   "source": [
    "# initiate model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6792089-2dc8-4a85-b469-371f660a8dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better optimizer over stochastic gradient descent\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b68dde-bcdf-4f2d-abc1-4b9f9d3f72da",
   "metadata": {},
   "source": [
    "### Training with optimmizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81450c78-65df-4f9c-a1ce-46b37d7a4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "502e996c-dc74-46c8-a794-5a0e6ec81647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.8345, val loss 1.9429\n",
      "step 25: train loss 1.8319, val loss 1.9409\n",
      "step 50: train loss 1.8328, val loss 1.9449\n",
      "step 75: train loss 1.8311, val loss 1.9458\n",
      "step 100: train loss 1.8287, val loss 1.9389\n",
      "step 125: train loss 1.8315, val loss 1.9421\n",
      "step 150: train loss 1.8242, val loss 1.9359\n",
      "step 175: train loss 1.8369, val loss 1.9473\n",
      "step 200: train loss 1.8307, val loss 1.9360\n",
      "step 225: train loss 1.8322, val loss 1.9438\n",
      "step 250: train loss 1.8316, val loss 1.9341\n",
      "step 275: train loss 1.8275, val loss 1.9478\n",
      "step 300: train loss 1.8280, val loss 1.9383\n",
      "step 325: train loss 1.8254, val loss 1.9373\n",
      "step 350: train loss 1.8276, val loss 1.9384\n",
      "step 375: train loss 1.8238, val loss 1.9438\n",
      "step 400: train loss 1.8295, val loss 1.9426\n",
      "step 425: train loss 1.8290, val loss 1.9350\n",
      "step 450: train loss 1.8279, val loss 1.9352\n",
      "step 475: train loss 1.8220, val loss 1.9385\n",
      "step 500: train loss 1.8249, val loss 1.9386\n",
      "step 525: train loss 1.8225, val loss 1.9330\n",
      "step 550: train loss 1.8192, val loss 1.9392\n",
      "step 575: train loss 1.8288, val loss 1.9401\n",
      "step 600: train loss 1.8260, val loss 1.9341\n",
      "step 625: train loss 1.8207, val loss 1.9277\n",
      "step 650: train loss 1.8206, val loss 1.9326\n",
      "step 675: train loss 1.8209, val loss 1.9264\n",
      "step 700: train loss 1.8236, val loss 1.9337\n",
      "step 725: train loss 1.8220, val loss 1.9305\n",
      "step 750: train loss 1.8219, val loss 1.9356\n",
      "step 775: train loss 1.8158, val loss 1.9288\n",
      "step 800: train loss 1.8137, val loss 1.9329\n",
      "step 825: train loss 1.8163, val loss 1.9363\n",
      "step 850: train loss 1.8148, val loss 1.9330\n",
      "step 875: train loss 1.8165, val loss 1.9313\n",
      "step 900: train loss 1.8115, val loss 1.9332\n",
      "step 925: train loss 1.8171, val loss 1.9372\n",
      "step 950: train loss 1.8178, val loss 1.9296\n",
      "step 975: train loss 1.8092, val loss 1.9342\n",
      "step 1000: train loss 1.8099, val loss 1.9372\n",
      "step 1025: train loss 1.8211, val loss 1.9360\n",
      "step 1050: train loss 1.8179, val loss 1.9274\n",
      "step 1075: train loss 1.8086, val loss 1.9264\n",
      "step 1100: train loss 1.8101, val loss 1.9364\n",
      "step 1125: train loss 1.8089, val loss 1.9290\n",
      "step 1150: train loss 1.8143, val loss 1.9286\n",
      "step 1175: train loss 1.8130, val loss 1.9287\n",
      "step 1200: train loss 1.8133, val loss 1.9313\n",
      "step 1225: train loss 1.8086, val loss 1.9256\n",
      "step 1250: train loss 1.8076, val loss 1.9289\n",
      "step 1275: train loss 1.8071, val loss 1.9219\n",
      "step 1300: train loss 1.8018, val loss 1.9241\n",
      "step 1325: train loss 1.8037, val loss 1.9246\n",
      "step 1350: train loss 1.8066, val loss 1.9307\n",
      "step 1375: train loss 1.8108, val loss 1.9244\n",
      "step 1400: train loss 1.8028, val loss 1.9182\n",
      "step 1425: train loss 1.8073, val loss 1.9236\n",
      "step 1450: train loss 1.8055, val loss 1.9193\n",
      "step 1475: train loss 1.8103, val loss 1.9169\n",
      "step 1500: train loss 1.8083, val loss 1.9289\n",
      "step 1525: train loss 1.7964, val loss 1.9215\n",
      "step 1550: train loss 1.8028, val loss 1.9180\n",
      "step 1575: train loss 1.7987, val loss 1.9200\n",
      "step 1600: train loss 1.8073, val loss 1.9209\n",
      "step 1625: train loss 1.8019, val loss 1.9220\n",
      "step 1650: train loss 1.7984, val loss 1.9201\n",
      "step 1675: train loss 1.8000, val loss 1.9222\n",
      "step 1700: train loss 1.7935, val loss 1.9212\n",
      "step 1725: train loss 1.7960, val loss 1.9239\n",
      "step 1750: train loss 1.8016, val loss 1.9269\n",
      "step 1775: train loss 1.7963, val loss 1.9252\n",
      "step 1800: train loss 1.7910, val loss 1.9215\n",
      "step 1825: train loss 1.7969, val loss 1.9213\n",
      "step 1850: train loss 1.7902, val loss 1.9142\n",
      "step 1875: train loss 1.7978, val loss 1.9248\n",
      "step 1900: train loss 1.7966, val loss 1.9207\n",
      "step 1925: train loss 1.7945, val loss 1.9182\n",
      "step 1950: train loss 1.7945, val loss 1.9198\n",
      "step 1975: train loss 1.7926, val loss 1.9203\n",
      "step 2000: train loss 1.7922, val loss 1.9203\n",
      "step 2025: train loss 1.7885, val loss 1.9229\n",
      "step 2050: train loss 1.7936, val loss 1.9252\n",
      "step 2075: train loss 1.7841, val loss 1.9170\n",
      "step 2100: train loss 1.7904, val loss 1.9175\n",
      "step 2125: train loss 1.7901, val loss 1.9194\n",
      "step 2150: train loss 1.7904, val loss 1.9251\n",
      "step 2175: train loss 1.7814, val loss 1.9227\n",
      "step 2200: train loss 1.7838, val loss 1.9208\n",
      "step 2225: train loss 1.7854, val loss 1.9170\n",
      "step 2250: train loss 1.7955, val loss 1.9177\n",
      "step 2275: train loss 1.7876, val loss 1.9109\n",
      "step 2300: train loss 1.7855, val loss 1.9179\n",
      "step 2325: train loss 1.7904, val loss 1.9169\n",
      "step 2350: train loss 1.7878, val loss 1.9163\n",
      "step 2375: train loss 1.7857, val loss 1.9225\n",
      "step 2400: train loss 1.7819, val loss 1.9131\n",
      "step 2425: train loss 1.7816, val loss 1.9188\n",
      "step 2450: train loss 1.7852, val loss 1.9154\n",
      "step 2475: train loss 1.7838, val loss 1.9129\n",
      "step 2500: train loss 1.7811, val loss 1.9158\n",
      "step 2525: train loss 1.7809, val loss 1.9155\n",
      "step 2550: train loss 1.7795, val loss 1.9117\n",
      "step 2575: train loss 1.7820, val loss 1.9136\n",
      "step 2600: train loss 1.7790, val loss 1.9198\n",
      "step 2625: train loss 1.7772, val loss 1.9182\n",
      "step 2650: train loss 1.7845, val loss 1.9186\n",
      "step 2675: train loss 1.7764, val loss 1.9161\n",
      "step 2700: train loss 1.7855, val loss 1.9102\n",
      "step 2725: train loss 1.7766, val loss 1.9153\n",
      "step 2750: train loss 1.7775, val loss 1.9190\n",
      "step 2775: train loss 1.7839, val loss 1.9154\n",
      "step 2800: train loss 1.7791, val loss 1.9138\n",
      "step 2825: train loss 1.7748, val loss 1.9105\n",
      "step 2850: train loss 1.7747, val loss 1.9129\n",
      "step 2875: train loss 1.7735, val loss 1.9061\n",
      "step 2900: train loss 1.7725, val loss 1.9114\n",
      "step 2925: train loss 1.7756, val loss 1.9064\n",
      "step 2950: train loss 1.7734, val loss 1.9091\n",
      "step 2975: train loss 1.7745, val loss 1.9136\n",
      "step 3000: train loss 1.7717, val loss 1.9138\n",
      "step 3025: train loss 1.7709, val loss 1.9069\n",
      "step 3050: train loss 1.7698, val loss 1.8976\n",
      "step 3075: train loss 1.7720, val loss 1.9052\n",
      "step 3100: train loss 1.7697, val loss 1.9017\n",
      "step 3125: train loss 1.7687, val loss 1.8996\n",
      "step 3150: train loss 1.7724, val loss 1.9055\n",
      "step 3175: train loss 1.7636, val loss 1.9032\n",
      "step 3200: train loss 1.7702, val loss 1.9072\n",
      "step 3225: train loss 1.7681, val loss 1.9029\n",
      "step 3250: train loss 1.7709, val loss 1.9080\n",
      "step 3275: train loss 1.7746, val loss 1.9030\n",
      "step 3300: train loss 1.7735, val loss 1.9006\n",
      "step 3325: train loss 1.7672, val loss 1.9021\n",
      "step 3350: train loss 1.7715, val loss 1.8998\n",
      "step 3375: train loss 1.7680, val loss 1.9003\n",
      "step 3400: train loss 1.7678, val loss 1.9025\n",
      "step 3425: train loss 1.7669, val loss 1.9030\n",
      "step 3450: train loss 1.7669, val loss 1.8939\n",
      "step 3475: train loss 1.7707, val loss 1.9063\n",
      "step 3500: train loss 1.7681, val loss 1.9043\n",
      "step 3525: train loss 1.7675, val loss 1.9051\n",
      "step 3550: train loss 1.7663, val loss 1.9001\n",
      "step 3575: train loss 1.7667, val loss 1.8983\n",
      "step 3600: train loss 1.7568, val loss 1.8962\n",
      "step 3625: train loss 1.7636, val loss 1.8974\n",
      "step 3650: train loss 1.7656, val loss 1.8974\n",
      "step 3675: train loss 1.7639, val loss 1.8925\n",
      "step 3700: train loss 1.7591, val loss 1.8938\n",
      "step 3725: train loss 1.7627, val loss 1.9018\n",
      "step 3750: train loss 1.7625, val loss 1.9002\n",
      "step 3775: train loss 1.7635, val loss 1.8981\n",
      "step 3800: train loss 1.7604, val loss 1.8948\n",
      "step 3825: train loss 1.7551, val loss 1.8913\n",
      "step 3850: train loss 1.7649, val loss 1.8951\n",
      "step 3875: train loss 1.7566, val loss 1.8916\n",
      "step 3900: train loss 1.7625, val loss 1.8952\n",
      "step 3925: train loss 1.7587, val loss 1.8917\n",
      "step 3950: train loss 1.7602, val loss 1.8986\n",
      "step 3975: train loss 1.7564, val loss 1.8924\n",
      "step 4000: train loss 1.7597, val loss 1.8927\n",
      "step 4025: train loss 1.7542, val loss 1.8910\n",
      "step 4050: train loss 1.7593, val loss 1.8930\n",
      "step 4075: train loss 1.7576, val loss 1.8983\n",
      "step 4100: train loss 1.7548, val loss 1.8925\n",
      "step 4125: train loss 1.7575, val loss 1.8956\n",
      "step 4150: train loss 1.7558, val loss 1.9000\n",
      "step 4175: train loss 1.7538, val loss 1.8888\n",
      "step 4200: train loss 1.7519, val loss 1.8865\n",
      "step 4225: train loss 1.7546, val loss 1.8884\n",
      "step 4250: train loss 1.7504, val loss 1.8895\n",
      "step 4275: train loss 1.7514, val loss 1.8893\n",
      "step 4300: train loss 1.7578, val loss 1.8930\n",
      "step 4325: train loss 1.7636, val loss 1.8907\n",
      "step 4350: train loss 1.7606, val loss 1.8890\n",
      "step 4375: train loss 1.7552, val loss 1.8910\n",
      "step 4400: train loss 1.7514, val loss 1.8891\n",
      "step 4425: train loss 1.7503, val loss 1.8877\n",
      "step 4450: train loss 1.7512, val loss 1.8931\n",
      "step 4475: train loss 1.7509, val loss 1.8861\n",
      "step 4500: train loss 1.7560, val loss 1.8906\n",
      "step 4525: train loss 1.7493, val loss 1.8883\n",
      "step 4550: train loss 1.7492, val loss 1.8950\n",
      "step 4575: train loss 1.7493, val loss 1.8915\n",
      "step 4600: train loss 1.7476, val loss 1.8846\n",
      "step 4625: train loss 1.7502, val loss 1.8870\n",
      "step 4650: train loss 1.7489, val loss 1.8937\n",
      "step 4675: train loss 1.7484, val loss 1.8942\n",
      "step 4700: train loss 1.7472, val loss 1.8863\n",
      "step 4725: train loss 1.7538, val loss 1.8943\n",
      "step 4750: train loss 1.7483, val loss 1.8919\n",
      "step 4775: train loss 1.7510, val loss 1.8938\n",
      "step 4800: train loss 1.7537, val loss 1.8950\n",
      "step 4825: train loss 1.7466, val loss 1.8840\n",
      "step 4850: train loss 1.7453, val loss 1.8873\n",
      "step 4875: train loss 1.7448, val loss 1.8896\n",
      "step 4900: train loss 1.7386, val loss 1.8861\n",
      "step 4925: train loss 1.7447, val loss 1.8859\n",
      "step 4950: train loss 1.7450, val loss 1.8850\n",
      "step 4975: train loss 1.7427, val loss 1.8844\n",
      "step 4999: train loss 1.7473, val loss 1.8875\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    # periodically evaluate loss\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f'step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n",
    "        lossi.append(losses['val'])\n",
    "\n",
    "    #sample data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef71f24a-e22e-4912-97d1-cf9b05bbf33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1779f52e0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed724f75-d3dd-42b4-85a9-8f7e352349f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MENENIUS:\n",
      "Seas me els, jurye the in iterly behisers\n",
      "Make an.\n",
      "A:\n",
      "What see to, it, this bagment in the I hight\n",
      "His him huse telseines.\n",
      "Where keving sheen whem'douly thee he my cotal?\n",
      "' maf stirkngs-we me pesth;\n",
      "For year weret it the sono, when; for Thoure u!\n",
      "I groom that fedvoy name that pote tony cowgr!\n",
      "My wowif! I love apy Clicive steer and love\n",
      "thougge-o a con then'd I gupherveer turbrieed\n",
      "us would I till teair pacey! wyour misoncie.\n",
      "Why sa hengels, our goe houscts befow: thot,\n",
      "And ithy husself.\n",
      "\n",
      "ARIANNGLIES:\n",
      "As yor fam nother: yepenss, that I\n",
      "Indot touth fath toour bloodks caust gon,\n",
      "If thou rotle plunifes:\n",
      "whou by the pisbjery arre, bruth to caurbly! you.\n",
      "\n",
      "GLOUCES ELOUSRGANC:\n",
      "Swernch mely!\n",
      "\n",
      "GLANES:\n",
      "His hasentain ourad hear?\n",
      "Came, that feace my seorth ep, thee man houd stay\n",
      "tered thy knows were me, hat more fier mefol,\n",
      "Sitsm for Yedlum.\n",
      "Then mauty calsanom.\n",
      "Those not: Whith I lefet lieke\n",
      "Hingentiong storters.\n",
      "\n",
      "ROMERCH:\n",
      "Noth soumeny cere kims mpardy be your thy loave,\n",
      "I sell dut sparless bee ily hevy more liked\n",
      "Of sis thus wandem that may of hou the blacth musts'\n",
      "Havery off drainaty She we mure spay stup this his,\n",
      "armmeate shereal ell that here sweresst,\n",
      "and now, Howll I sings,\n",
      "Romer tegs, for igher eminnuess. Kill aby try oin, and,\n",
      "Ner: I, For welt that to bee\n",
      "Abut be man, I stakes uboansuel, laver?\n",
      "\n",
      "Prets'd instledabes:\n",
      "Lor there chall throk yovers a chee;\n",
      "To ingg Gost toon: go, vitthees to that whath\n",
      "user is browly, fruienct, but well takught\n",
      "Toght, to but up thy's my lave.\n",
      "\n",
      "HORD I seet my lite?\n",
      "\n",
      "POLANUS:\n",
      "If he adourd thy shall I weath us upots on.\n",
      "I rieve tronge a croatge mpooh:\n",
      "Shat good on custsome to hing:\n",
      "Lidsteraiong yous: theere theou will upestres it\n",
      "it asiep; beavesty? him sees'd gurves,\n",
      "Ot s a sot\n",
      "Your in tit well me he thee pail a twimme.\n",
      "\n",
      "LUpENCE:\n",
      "Mailino, your ar bout mathere, Gorfow. Soo herk oithing hands Lod.\n",
      "Goriou--make not lenst shallowe, its withm surmio,\n",
      "Yown benven' teempen of Lingt teself: and betheer;\n",
      "That to dorpes here las!'d with as me fielt;\n",
      "I stay hour her heand lenr honem's talemer mere!\n",
      "Thomefere I't descusge, not her, to'd smeay, lave yourfow\n",
      "With such knry sirrcion.\n",
      "\n",
      "GEONUS:\n",
      "I with bustler, sway to beee proy's how is fixe;\n",
      "Go, his athe but hadst all is kinkews years our\n",
      "Mounor the beatder'd, swedsy spest?\n",
      "For. Kearms! That not mee'ss hus a seell;\n",
      "But I well me coust ally? I would sees!\n",
      "Wels to dume hence hows jeaterst Braniong stungmetch horm and\n",
      "Pires'der ut postile your'm our do sthice, torsts\n",
      "Uncword hear nes\n",
      "Ofore unttrurs to the didge bith, parpiesy thate beace;\n",
      "Come in the dakis sorl; I whatere you so choust-wid.\n",
      "For macke rocklon, sir;\n",
      "Foer their wrol knie; spowerer; Citius steem Womel,\n",
      "prrentay, rurrse he as their sentoly\n",
      "Gorth to dis or or the eile mid themselfus; by and my\n",
      "Rothrerfle they in and fech thoRRk I\n",
      "Hartict doth congly make ton,\n",
      "Thy loukesttn'st khing do the uskincy mine alwere you\n",
      "gar if if foltser of or that ands musty pethek\n",
      "Sher, I serpees, no of me?\n",
      "\n",
      "GESIO:\n",
      "For famed!\n",
      "She KING Edwond seak thre jottscend'\n",
      "Wher foulk live the kill the herfll more up prinep Are;\n",
      "your'low bep heremer prests hear be pain, with thou; it mienters:\n",
      "Methanger wany defullis:\n",
      "If, not I have and ared all is!\n",
      "\n",
      "ISISABETIS:\n",
      "Red tell datesist to daree he best, of fall with,\n",
      "dier it my unbume kning! But shon brefory,\n",
      "By LORonck, Pome to with Cata yen:\n",
      "Mack bee plaings' truse in a me wa, to soulde\n",
      "Boras carre, 'fer Lese will -outher ergly;\n",
      "And to my huse to? Boysels the courts;\n",
      "Where's sont o' prome; come that death acked,\n",
      "This delive a' of hat muck me not nows?\n",
      "\n",
      "ANNISHO:\n",
      "What shom, for eck ween seee-fath hant givey\n",
      "Kit hes mangaioouys yer of hus, were will head,\n",
      "Amtter angoot of ame. well sorrgeeng fall G\n",
      "ren: thy mabhters,\n",
      "Thou rown I tamack make soy.\n",
      "\n",
      "Hif Low yeat sheer,\n",
      "Rich my throws ondwitim my to moon eym.\n",
      "They wifer my teers thy shoster of\n",
      "Of XHORhqureinces not any pof thanlows clame dop;\n",
      "Thy call gid my off his. APHother destey wesmy?\n",
      "Ben fand viol a abstea,\n",
      "For ewn thou dasishst-broy, thes, her ingesoms.\n",
      "Whre prawe make digameds of thy kisbe: theve\n",
      "Repge the wichichm bett galed to in to his's.\n",
      "\n",
      "\n",
      "MAUSTINA:\n",
      "This iSuserad:\n",
      "And to He is\n",
      "our shim, ysend I ent thy leave sloway joy.\n",
      "\n",
      "QicHENEOUS:\n",
      "Plimedangly?\n",
      "\n",
      "WAnt's va--disce; anvermay's in a hupe not anibe,\n",
      "Plicest a to minecing orew and the lay. Sich\n",
      "\n",
      "SICUfFL:\n",
      "Nos he em.\n",
      "\n",
      "NESSOME:\n",
      "Hen shread, get'steing lar yourts:\n",
      "You you, that wy.\n",
      "\n",
      "JULESTESt:\n",
      "Thou lonow thest thregaiath no earf to moreh,\n",
      "And have to for hath morstan ow, and here the mort hease\n",
      "nceiiot axt aut and would allobnes\n",
      "Their thou foluely, sen thy and nale difed.\n",
      "I maguther, I cinble not of oper.\n",
      "\n",
      "AUCAPULA:\n",
      "Bucke 'deper yet of't, here titeents.\n",
      "\n",
      "Secire, towe these, snot fly thee, very shalt,\n",
      "O till thest minesty a moreis't'\n",
      "And a 'ercing and thourt lontiphing ha,\n",
      "Upon; hon friareve this usomputions; and didy.\n",
      "Dill Anchould, now?\n",
      " dant, wererean, do they, and tay!\n",
      "\n",
      "CLUCEO:\n",
      "In un!\n",
      "Twhout tho\n",
      "Un well, his a live notlight neir aith.\n",
      "\n",
      "QUES:\n",
      "Sidver intams: not Well and t\n"
     ]
    }
   ],
   "source": [
    "generate_n_ex(model, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c76b8b-2539-4aa2-9489-4989c448f18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2564d-eacb-45d2-b032-323cdc5eda81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
